{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a language model based on wikipedia in you language \n",
    "\n",
    "The notebooke includes the whole process but you will need to help the process if the following problem happens:  \n",
    "-you internet connection is interrupted (stage 1)\n",
    "\n",
    "-you run out of diskspace\n",
    "\n",
    "-because of the huge memory consumption. creatig the databunch for the training requires lots of memory\n",
    "\n",
    "\n",
    "In order ot handle these problemn the notebook is divided into stages. If the process is failes in a stage then you can resume the processing from the beginning of that stage by:\n",
    "-restarting the kernel \"Kernel\"/Restart & Clear Output\n",
    "\n",
    "-running the cells in stage 0: initialization\n",
    "\n",
    "-running the cells from the start of the stage that failed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 0: Initialisation\n",
    "lang: You must set the parameter \"lang\" to the language you want to build a model for. Fx:\n",
    "\n",
    "fr: for french\n",
    "\n",
    "en: for english\n",
    "\n",
    "de:for german\n",
    "\n",
    "da: for danish \n",
    "\n",
    "etc.\n",
    "\n",
    "pathData: You must se the location where you want your data stored using the parameter pathData. Consider using a ssd-rive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang=\"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import * \n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pathlib import *\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathData       = Path(\"../nlp-data\")\n",
    "path           = pathData / lang\n",
    "pathDump       = path/\"wiki-dump\"\n",
    "pathJson       = path/\"wiki-json\"\n",
    "pathTrainValid = path/\"wiki-train_valid\"\n",
    "spCache        = \"sp-model\"\n",
    "pathSPVocab    = pathTrainValid / spCache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: download the selected language from wikipedia and convert the articles to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = f\"{lang}wiki-latest-pages-articles.xml.bz2\"\n",
    "if len(list(pathDump.glob(\"*.bz2\")))==0:\n",
    "    pathDump.mkdir(parents=True,exist_ok=True)\n",
    "    url  = f\"https://dumps.wikimedia.org/{lang}wiki/latest/{fn}\"\n",
    "    pDist = str(pathData/lang)\n",
    "    !wget -c $url -P $pDist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: Convert wikipedia dump to articles in json \n",
    "minWords: wikipedia sections with fewer than minWords are not converte. If you language does not use space \" \" to \"separate\" words then set this prameter to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json as json\n",
    "pathWikiExtractor = Path(\"../wikiextractor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/attardi/wikiextractor.git $pathWikiExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = f\"cd {str(pathWikiExtractor)} && python setup.py install\"\n",
    "! $cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#extracting the json-files. This takes about 1 hour for french with files read from and saved to a ssd hardrive \n",
    "fn_wikidump = list(pathDump.iterdir())[0]\n",
    "strcmd = f\"cd {str(pathWikiExtractor)} && WikiExtractor.py -o {str(pathJson)} --json -q  {str(fn_wikidump)}\"\n",
    "print(strcmd)\n",
    "!$strcmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3: building a vocabulary using sentencepiece\n",
    "\n",
    "Now we separate the title of the wikipedia section from text section that we keep. \n",
    "\n",
    "In order to makes a first reduction on the number of section we clean the text with the preprocessing rules from fastai and ignore text with less than \"minWords\"\n",
    "\n",
    "You must set the lenght of the shortes sections you want to keep using the parameter \"minWords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "minWords  = 25\n",
    "chunksize = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /anaconda3/envs/fastai/lib/python3.7/site-packages (0.1.6)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path       = Path(\"../../data/nlp-data/fr/\")\n",
    "cache_name = \"sp-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencepieceWikiModel:\n",
    "    def __init__(self, lang:str, pathJson:Path, pathTrainValid:Path, cache_name:str='sp-model', \n",
    "                 vocab_size:int=32000, model_type:str='unigram', \n",
    "                 rules=text.transform.default_pre_rules ):  #should include removal of repetitions\n",
    "        self.lang           = lang\n",
    "        self.pathJson       = pathJson\n",
    "        self.pathTrainValid = pathTrainValid\n",
    "        self.pathVocab      = self.pathTrainValid / cache_name\n",
    "        self.vocab_size     = vocab_size\n",
    "        self.model_type     = model_type\n",
    "        self.rules          = rules\n",
    "        \n",
    "        self.pathVocab.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def wikijson2TrainingData(self):\n",
    "        \n",
    "        def save_sections( f_out, data, fileCount ):\n",
    "            p = pathTxt / f\"{fileCount}.txt\"\n",
    "            with p.open(\"w+\") as fw:\n",
    "                fw.write( \"\\n\".join(data[\"text\"]) )\n",
    "            data.to_csv(f_out, index=False, header=False, mode='a')\n",
    "        \n",
    "        \"generate text files for training af sentencepiece vocabulary \" \\\n",
    "        \"and a csv-file for training a languagemodel with the vocabulary and the wiki-text in the csv-file\"\n",
    "        pathTxt  = self.pathTrainValid/\"txt\"\n",
    "        print(pathTxt)\n",
    "        pathTxt.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        pathParts = len(pathJson.parts)        \n",
    "        pathcsv   = self.pathTrainValid/'wiki.csv'\n",
    "        with pathcsv.open(\"w\") as f_out:\n",
    "            fileCount = 0\n",
    "            fields    = ['text', 'wordcount']\n",
    "            data      = pd.DataFrame(columns=fields)\n",
    "            data.to_csv(f_out, index=False, mode='a')\n",
    "            \n",
    "            sections = []\n",
    "            for fn in self.pathJson.glob(\"**/wiki*\"):\n",
    "                with open(fn, encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        section = json.loads(line)\n",
    "                        \n",
    "                        if section['text'].find(section['title']) >=0 :\n",
    "                            section['text'] = section['text'][len(section['title'])+2:]\n",
    "                \n",
    "                        section['text']      = section['text'].replace('\\n\\n', \"\\n\")\n",
    "                        section['text']      = reduce(lambda t, rule: rule(t), self.rules, section['text'])\n",
    "                        section['wordcount'] = len(re.findall(r'\\w+',section['text']))\n",
    "            \n",
    "                        if section[\"wordcount\"] > minWords:\n",
    "                            sections.append({k:section[k] for k in fields})\n",
    "                \n",
    "                if len(sections) > chunksize: \n",
    "                    data = pd.DataFrame(sections)\n",
    "                    sections = []\n",
    "                    \n",
    "                    save_sections(f_out, data, fileCount)\n",
    "                    fileCount += 1\n",
    "                    data = None\n",
    "                    \n",
    "            if len(sections) > 0: \n",
    "                data = pd.DataFrame(sections)\n",
    "                sections = []\n",
    "                    \n",
    "                save_sections(f_out, data, fileCount)\n",
    "                fileCount += 1\n",
    "                data = None\n",
    "                \n",
    "            \n",
    "    def getUserdefinedSymbols(self): \n",
    "        return [text.transform.BOS,\n",
    "                text.transform.PAD,\n",
    "                text.transform.TK_MAJ,\n",
    "                text.transform.TK_UP,\n",
    "                text.transform.TK_REP,\n",
    "                text.transform.TK_WREP,\n",
    "                text.transform.FLD ] \n",
    "\n",
    "    def trainVocabulary(self): \n",
    "        pathSrcTxt   = self.pathTrainValid / \"txt\"\n",
    "        model_prefix = self.pathVocab / \"m\"\n",
    "    \n",
    "        #Set the following controls to sentencepiece values until there is a release where we can set the token value\n",
    "        #Note taku910 has already made the change but the pip of sentencepiewce version has not been updated \n",
    "        text.transform.UNK = \"<unk>\"\n",
    "        #text.transform.BOS = \"<s>\"\n",
    "        #text.transform.PAD = \"<pad>\"\n",
    "    \n",
    "        #create control ids for the rest of the fastai control tokens in case the user needs them\n",
    "        #it is the responsibility of fastai to generate and use the control tokens them and apply them before decoding\n",
    "        #Fx applying TK_MAJ after tokenization would change She to two token TK_MAJ+she.\n",
    "        #Problem! Sentencepiece would tokenize \"Elle\" as _Elle so our deal_caps would not catch it\n",
    "        str_specialcases = \",\".join(getUserdefinedSymbols()) \n",
    "    \n",
    "        pathSrc_list = [str(s) for s in pathSrcTxt.glob(\"**/*.txt\")]\n",
    "        pathSrc_list= \",\".join(getUserdefinedSymbols)\n",
    "    \n",
    "        sp_params = f\"--input={pathSrc_list} \"  \\\n",
    "                    f\"--bos_id=-1 \" \\\n",
    "                    f\"--eos_id=-1 \" \\\n",
    "                    f\"--pad_id=-1 \" \\\n",
    "                    f\"--user_defined_symbols={str_specialcases} \" \\\n",
    "                    f\"--character_coverage=1.0 \" \\\n",
    "                    f\"--model_prefix={model_prefix} \" \\\n",
    "                    f\"--vocab_size={self.vocab_size} \" \\\n",
    "                    f\"--model_type={self.model_type} \" \n",
    "    \n",
    "        #f\"--split_by_number=1 \" \\\n",
    "        #hard_vocab_limit=False\n",
    "        #use_all_vocab\n",
    "        #print(sp_params)\n",
    "        import sentencepiece as spm\n",
    "        spm.SentencePieceTrainer.Train(sp_params)\n",
    "        \n",
    "        #convert sentencepieces vocabulary to a format fastai can read\n",
    "        with open( self.pathVocab/\"m.vocab\", 'r') as f:\n",
    "            vocab = [line.split('\\t')[0] for line in f.readlines()]\n",
    "        pickle.dump(vocab, open( self.pathVocab / \"itos.pkl\", \"wb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencepieceTokenizer(BaseTokenizer):\n",
    "    def __init__(self, lang:str):\n",
    "        path,cache_name = pathTrainValid, \"sp-model\"\n",
    "    #def __init__(self, path:PathOrStr, cache_name:str='sp-model'):\n",
    "        self.pathVocab = path / cache_name\n",
    "        self.vocab_    = Vocab(pickle.load(open(self.pathVocab/'itos.pkl', 'rb')))\n",
    "        self.tok       = spm.SentencePieceProcessor()\n",
    "        \n",
    "        self.tok.Load(str(Path(path) / cache_name / 'm.model'))\n",
    "        text.transform.UNK = \"<unk>\"\n",
    "\n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        return self.tok.EncodeAsPieces(t)\n",
    "    \n",
    "    def add_special_cases(self, toks:Collection[str]):\n",
    "        #this should have been done when training sentencepiece\n",
    "        pass\n",
    "    \n",
    "    def vocab(self): return self.vocab_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../nlp-data/en/wiki-train_valid/txt\n",
      "CPU times: user 46min 17s, sys: 48.7 s, total: 47min 6s\n",
      "Wall time: 48min 9s\n"
     ]
    }
   ],
   "source": [
    "swm = SentencepieceWikiModel(lang=lang, pathJson=pathJson, pathTrainValid=pathTrainValid)\n",
    "\n",
    "%time swm.wikijson2TrainingData()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "swm = SentencepieceWikiModel(lang=lang, pathJson=pathJson, pathTrainValid=pathTrainValid)\n",
    "%time swm.trainVocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "swm = SentencepieceWikiModel(lang=lang, pathJson=pathJson, pathTrainValid=pathTrainValid)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(str(pathTrainValid / cache_name / \"m.model\"))\n",
    "print(\"1: Size of vocabulary:\",sp.GetPieceSize())\n",
    "sentence = \"She is tall. He is small\"\n",
    "print(\"2:\", sp.EncodeAsPieces(sentence))\n",
    "print(\"3:\", sp.EncodeAsIds(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Control symbol\")\n",
    "for s in [\"<unk>\"]: print(f\"{s}({sp.PieceToId(s)})\")\n",
    "\n",
    "print(f\"\\nuser_defined_symbols\")\n",
    "for s in swm.getUserdefinedSymbols():print(f\"{s}({sp.PieceToId(s)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
