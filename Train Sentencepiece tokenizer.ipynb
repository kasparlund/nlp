{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import * \n",
    "\n",
    "import sentencepiece as spm\n",
    "from pathlib import *\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/kl/anaconda3/envs/fastai/lib/python3.6/site-packages (0.1.6)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path       = Path(\"../../data/nlp-data/fr/\")\n",
    "cache_name = \"sp-model\"\n",
    "special_cases=[ \n",
    "    text.transform.BOS,\n",
    "    text.transform.PAD,\n",
    "    text.transform.TK_MAJ,\n",
    "    text.transform.TK_UP,\n",
    "    text.transform.TK_REP,\n",
    "    text.transform.TK_WREP,\n",
    "    text.transform.FLD\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "class SentencepieceWikiModel:\n",
    "    def __init__(self, path:Path, cache_name:str='sp-model', \n",
    "                 vocab_size:int=32000, model_type:str='unigram', \n",
    "                 rules=text.transform.default_pre_rules ):\n",
    "        self.path           = path\n",
    "        self.pathTrainValid = path/\"wiki-train_valid\"        \n",
    "        self.pathVocab      = self.pathTrainValid / cache_name\n",
    "        self.vocab_size = vocab_size\n",
    "        self.model_type = model_type\n",
    "        self.rules = rules\n",
    "        \n",
    "        self.pathVocab.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def prepareWiki_old():\n",
    "        tmpDir = self.pathTrainValid/\"tmp\"    \n",
    "        if tmpDir.exists(): shutil.rmtree(tmpDir, ignore_errors=True)\n",
    "        tmpDir.mkdir(parents=True,exist_ok=True)\n",
    "    \n",
    "        txt = []\n",
    "        noSrcParts = len(pathJson.parts)\n",
    "        for fn in pathJson.glob(\"**/wiki*\"):\n",
    "            with fn.open(encoding='utf-8') as f:\n",
    "                txt = reduce(lambda t, rule: rule(t), rules, f.read())\n",
    "            \n",
    "                if len(txt)>0:\n",
    "                    p = tmpDir.joinpath(*fn.parts[noSrcParts:])\n",
    "                    p.parent.mkdir(exist_ok=True)\n",
    "                    with p.open(\"w+\") as fw:\n",
    "                        fw.write(txt)\n",
    "                        \n",
    "    def wikidump2TrainingData(self):\n",
    "        \"generate text files for training af sentencepiece vocabulary \" \\\n",
    "        \"and a csv-file for training a languagemodel with the vocabulary and the wiki-text in the csv-file\"\n",
    "        pathJson = self.path/\"wiki-json\"\n",
    "        pathTxt  = self.pathTrainValid/\"txt\"\n",
    "        \n",
    "        txt = []\n",
    "        noSrcParts = len(pathJson.parts)\n",
    "        for fn in pathJson.glob(\"**/wiki*\"):\n",
    "            with open(fn, encoding='utf-8') as f:\n",
    "                sections = []\n",
    "                for line in f:\n",
    "            \n",
    "                    section = json.loads(line)\n",
    "                    if section['text'].find(section['title']) >=0 :\n",
    "                        section['text'] = section['text'][len(section['title'])+2:]\n",
    "                \n",
    "                \n",
    "                    section['text']      = section['text'].replace('\\n\\n', \"\\n\")\n",
    "                    section['text']      = reduce(lambda t, rule: rule(t), self.rules, section['text'])\n",
    "                    section['textWords'] = len(re.findall(r'\\w+',section['text']))\n",
    "            \n",
    "                    if section['textWords'] > 0:\n",
    "                        txt.append(section)\n",
    "                        sections.append(section)\n",
    "                \n",
    "                if len(sections)>0:\n",
    "                    p = pathTxt.joinpath(*fn.parts[noSrcParts:])\n",
    "                    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    with p.open(\"w+\") as fw:\n",
    "                        for s in sections:\n",
    "                            fw.write(s[\"text\"])\n",
    "                \n",
    "        pd.DataFrame(txt).to_csv(self.pathTrainValid/'wiki.csv',index=False)        \n",
    "\n",
    "    def trainVocabulary(self): \n",
    "        pathSrcTxt   = self.pathTrainValid / \"txt\"\n",
    "        model_prefix = self.pathVocab / \"m\"\n",
    "    \n",
    "        #Set the following controls to sentencepiece values until there is a release where we can set the token value\n",
    "        #Note taku910 has already made the change but the pip of sentencepiewce version has not been updated \n",
    "        text.transform.UNK = \"<unk>\"\n",
    "        #text.transform.BOS = \"<s>\"\n",
    "        #text.transform.PAD = \"<pad>\"\n",
    "    \n",
    "        #create control ids for the rest of the fastai control tokens in case the user needs them\n",
    "        #it is the responsibility of fastai to generate and use the control tokens them and apply them before decoding\n",
    "        #Fx applying TK_MAJ after tokenization would change She to two token TK_MAJ+she.\n",
    "        #Problem! Sentencepiece would tokenize \"Elle\" as _Elle so our deal_caps would not catch it\n",
    "        special_cases=[ \n",
    "                        text.transform.BOS,\n",
    "                        text.transform.PAD,\n",
    "                        text.transform.TK_MAJ,\n",
    "                        text.transform.TK_UP,\n",
    "                        text.transform.TK_REP,\n",
    "                        text.transform.TK_WREP,\n",
    "                        text.transform.FLD ] \n",
    "        str_specialcases = \",\".join(special_cases) \n",
    "    \n",
    "        pathSrc_list = [str(s) for s in pathSrcTxt.glob(\"**/wiki*\")]\n",
    "        pathSrc_list= \",\".join(pathSrc_list)\n",
    "    \n",
    "        sp_params = f\"--input={pathSrc_list} \"  \\\n",
    "                    f\"--bos_id=-1 \" \\\n",
    "                    f\"--eos_id=-1 \" \\\n",
    "                    f\"--pad_id=-1 \" \\\n",
    "                    f\"--user_defined_symbols={str_specialcases} \" \\\n",
    "                    f\"--character_coverage=1.0 \" \\\n",
    "                    f\"--model_prefix={model_prefix} \" \\\n",
    "                    f\"--vocab_size={self.vocab_size} \" \\\n",
    "                    f\"--model_type={self.model_type} \" \n",
    "    \n",
    "        #f\"--split_by_number=1 \" \\\n",
    "        #hard_vocab_limit=False\n",
    "        #use_all_vocab\n",
    "        #print(sp_params)\n",
    "        spm.SentencePieceTrainer.Train(sp_params)\n",
    "        \n",
    "        #convert sentencepieces vocabulary to a format fastai can read\n",
    "        with open( self.pathVocab/\"m.vocab\", 'r') as f:\n",
    "            vocab = [line.split('\\t')[0] for line in f.readlines()]\n",
    "        pickle.dump(vocab, open( self.pathVocab / \"itos.pkl\", \"wb\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencepieceTokenizer(BaseTokenizer):\n",
    "    def __init__(self, lang:str):\n",
    "        path,cache_name = pathTrainValid, \"sp-model\"\n",
    "    #def __init__(self, path:PathOrStr, cache_name:str='sp-model'):\n",
    "        self.pathVocab = path / cache_name\n",
    "        self.vocab_    = Vocab(pickle.load(open(self.pathVocab/'itos.pkl', 'rb')))\n",
    "        self.tok       = spm.SentencePieceProcessor()\n",
    "        \n",
    "        self.tok.Load(str(Path(path) / cache_name / 'm.model'))\n",
    "        text.transform.UNK = \"<unk>\"\n",
    "\n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        return self.tok.EncodeAsPieces(t)\n",
    "    \n",
    "    def add_special_cases(self, toks:Collection[str]):\n",
    "        #this should have been done when training sentencepiece\n",
    "        pass\n",
    "    \n",
    "    def vocab(self): return self.vocab_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spwm = SentencepieceWikiModel(path=path)\n",
    "%time spwm.wikidump2TrainingData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46min 13s, sys: 16.8 s, total: 46min 30s\n",
      "Wall time: 12min 59s\n"
     ]
    }
   ],
   "source": [
    "spwm = SentencepieceWikiModel(path=path)\n",
    "\n",
    "%time spwm.trainVocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/nlp-data/fr/wiki-train_valid/sp-model/m.model\n",
      "1: Size of vocabulary: 32000\n",
      "2: ['▁Elle', '▁est', '▁grande', '.', '▁Il', '▁est', '▁petit']\n",
      "3: [89, 23, 254, 11, 43, 23, 560]\n",
      "4: Elle est grande. Il est petit\n",
      "5: Elle est grande. Il est petit\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "p = path /\"wiki-train_valid\"/ cache_name / \"m.model\"\n",
    "print(p)\n",
    "sp.Load(str(p))\n",
    "print(\"1: Size of vocabulary:\",sp.GetPieceSize())\n",
    "sentence = \"Elle est grande. Il est petit\"\n",
    "print(\"2:\", sp.EncodeAsPieces(sentence))\n",
    "print(\"3:\", sp.EncodeAsIds(sentence))\n",
    "print(\"4:\", sp.DecodePieces(sp.EncodeAsPieces(sentence)))\n",
    "print(\"5:\", sp.DecodeIds(sp.EncodeAsIds(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unk is a reserved control token in sentence that we cannot change - not even the symbol\n",
      "<unk>(0)\n",
      "<s>(0)\n",
      "<pad>(0)\n",
      "\n",
      "Our special cases and ids registrered as control token\n",
      "xxbos(1)\n",
      "xxpad(2)\n",
      "xxmaj(3)\n",
      "xxup(4)\n",
      "xxrep(5)\n",
      "xxwrep(6)\n",
      "xxfld(7)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unk is a reserved control token in sentence that we cannot change - not even the symbol\")\n",
    "reserved_cases = [\"<unk>\",\"<s>\",\"<pad>\"]\n",
    "for s in reserved_cases:\n",
    "    print(f\"{s}({sp.PieceToId(s)})\")\n",
    "\n",
    "print(f\"\\nOur special cases and ids registrered as control token\")\n",
    "for s in special_cases:\n",
    "    print(f\"{s}({sp.PieceToId(s)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: xxbos\n",
      "2: xxbos\n",
      "3: xxwrep\n",
      "4: xxwrep\n",
      "5: False\n"
     ]
    }
   ],
   "source": [
    "print(\"1:\",sp.IdToPiece(1))\n",
    "print(\"2:\",sp.DecodeIds([1]))\n",
    "print(\"3:\",sp.IdToPiece(6))\n",
    "print(\"4:\",sp.DecodeIds([6]))\n",
    "print(\"5:\",sp.is_control(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathSP=Path(\"../nlp-data/fr/wiki-train_valid\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
