{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from fastai import * \n",
    "from fastai.text import * \n",
    "import sentencepiece as spm\n",
    "import psutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang            = \"en\"\n",
    "pathData       = Path(\"../nlp-data\")\n",
    "path           = pathData / lang\n",
    "pathTrainValid = path/\"wiki-train_valid\"\n",
    "spCache        = \"sp-model\"\n",
    "pathSPVocab    = pathTrainValid / spCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def usedGB_RAM(): return round((psutil.virtual_memory().total-psutil.virtual_memory().free)/1e9,2)\n",
    "def usedGB_RAM(): return round((psutil.virtual_memory().used + psutil.swap_memory().used)/1e9,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.67"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psutil.virtual_memory().total\n",
    "usedGB_RAM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training and validation data we prepared in wiki_preparation.ipynb. In total 100 million articles with a split of 80% / 20% for training/validation:\n",
    "- First column: text content to train the model. \n",
    "- Second column: Boolean representing if the data is for training or validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tokenizer\n",
    "The sentencepiece vocabulary was trained in Train Sentencepiece tokenizer.ipynb.\n",
    "\n",
    "Here we will make a BasicTokenizer from Sentencepiece so that fastai can use it instead of spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencepieceTokenizer(BaseTokenizer):\n",
    "    def __init__(self, lang:str):\n",
    "        path,cache_name = pathTrainValid, \"sp-model\"\n",
    "    #def __init__(self, path:PathOrStr, cache_name:str='sp-model'):\n",
    "        self.pathVocab = path / cache_name\n",
    "        self.vocab_    = Vocab(pickle.load(open(self.pathVocab/'itos.pkl', 'rb')))\n",
    "        self.tok       = spm.SentencePieceProcessor()\n",
    "        \n",
    "        self.tok.Load(str(Path(path) / cache_name / 'm.model'))\n",
    "        text.transform.UNK = \"<unk>\"\n",
    "\n",
    "    #def __call__(self, language:str): return self    \n",
    "        \n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        return self.tok.EncodeAsPieces(t)\n",
    "    \n",
    "    def add_special_cases(self, toks:Collection[str]):\n",
    "        #this should have been done when training sentencepiece\n",
    "        pass\n",
    "    \n",
    "    def vocab(self): return self.vocab_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer SentencepieceTokenizer in en with the following rules:\n",
      " - fix_html\n",
      " - replace_rep\n",
      " - replace_wrep\n",
      " - spec_add_spaces\n",
      " - rm_useless_spaces\n",
      " - replace_all_caps\n",
      " - deal_caps\n",
      "\n",
      "size og vocabulary: 32000\n",
      "pad_idx: 2\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['▁she', '▁is', '▁tall', '.'], ['▁he', '▁is', '▁small']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spt       = SentencepieceTokenizer(lang=\"en\")\n",
    "tokenizer = Tokenizer(SentencepieceTokenizer,\"en\")\n",
    "pad_idx   = spt.vocab().numericalize([text.transform.PAD])[0]\n",
    "vocab,max_vocab  = spt.vocab(), len(spt.vocab().itos)\n",
    "\n",
    "print(tokenizer)\n",
    "print(\"size og vocabulary:\", max_vocab)\n",
    "print(\"pad_idx:\",pad_idx)\n",
    "\n",
    "print(spt.vocab().numericalize( [\"<unk>\" ,\"xxbos\" ,\"xxpad\" ,\"xxmaj\" ,\"xxup\" ,\"xxrep\" ,\"xxwrep\", \"xxfld\"]  ))\n",
    "sentence = [\"She is tall.\", \"He is small\"]\n",
    "tokenizer._process_all_1(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train two LM: one with a 60k vocabulary and one with a 30k vocabulary. The two models have different performance and computation needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discard section with kess than \"minTok\" tokens\n",
    "minToks=10"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#from ..torch_core import *\n",
    "\n",
    "def my_join_texts(texts:Collection[str], mark_fields:bool=False):\n",
    "    if not isinstance(texts, np.ndarray): texts = np.array(texts)\n",
    "    if is1d(texts): texts = texts[:,None]\n",
    "    df = pd.DataFrame({i:texts[:,i] for i in range(texts.shape[1])})\n",
    "    #text_col = f'{BOS} {FLD} {1} ' + df[0] if mark_fields else  f'{BOS} ' + df[0]\n",
    "    text_col = df[0]\n",
    "    print(df.shape)\n",
    "    for i in range(1,len(df.columns)):\n",
    "        #text_col += (f' {FLD} {i+1} ' if mark_fields else ' ') + df[i]\n",
    "        text_col += df[i]\n",
    "    return text_col.values\n",
    "\n",
    "#text.data._join_texts = my_join_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0.  1.  2.  3.1]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= [0,1,2,3.1]\n",
    "json.dumps(a)\n",
    "str(np.asarray(a))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import io\n",
    "a=np.arange(int(1e9))\n",
    "#f = io.StringIO()\n",
    "f = (pathTrainValid/\"dummy.txt\").open(\"w\")\n",
    "%timeit f.write(str(a))\n",
    "f.close()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from multiprocessing.dummy import Pool as ThreadPool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileTokenizer():\n",
    "    \"Put together rules and a tokenizer function to tokenize text with multiprocessing.\"\n",
    "    def __init__(self, tokPath:Path, tok_func:Callable, lang:str, vocab:Vocab=vocab, pre_rules:ListRules=None,\n",
    "                 post_rules:ListRules=None, special_cases:Collection[str]=None, n_cpus:int=None):\n",
    "        self.tok_func,self.lang,self.special_cases = tok_func,lang,special_cases\n",
    "        self.pre_rules  = ifnone(pre_rules,  defaults.text_pre_rules )\n",
    "        self.post_rules = ifnone(post_rules, defaults.text_post_rules )\n",
    "        self.special_cases = special_cases if special_cases else defaults.text_spec_tok\n",
    "        self.n_cpus = ifnone(n_cpus, defaults.cpus)\n",
    "        self.vocab  = vocab\n",
    "        self.tokPath = tokPath\n",
    "        \n",
    "        self.count=0\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        res = f'Tokenizer {self.tok_func.__name__} in {self.lang} with the following rules:\\n'\n",
    "        for rule in self.pre_rules: res += f' - {rule.__name__}\\n'\n",
    "        for rule in self.post_rules: res += f' - {rule.__name__}\\n'\n",
    "        return res\n",
    "\n",
    "    def process_text(self, t:str, tok:BaseTokenizer) -> List[str]:\n",
    "        \"Process one text `t` with tokenizer `tok`.\"\n",
    "        inPath = Path(t)\n",
    "        if not inPath.exists(): \n",
    "            print(f\"file does not exist{str(inPath)}\")\n",
    "            return \"\"\n",
    "        \n",
    "        pathIds = self.tokPath/(inPath.stem+\"-ids.npy\")\n",
    "        pathIds.parent.mkdir(parents=True,exist_ok=True)\n",
    "        \n",
    "        arrays = []\n",
    "        with inPath.open(\"r\") as f:\n",
    "            for line in f:\n",
    "                for rule in self.pre_rules: line = rule(line)\n",
    "                toks = tok.tokenizer(line)\n",
    "                for rule in self.post_rules: toks = rule(toks)\n",
    "                ids = vocab.numericalize(toks) \n",
    "                \n",
    "                if len(toks) < minToks: continue\n",
    "                arrays.append( np.asarray(ids, dtype=np.int16) )\n",
    "        \n",
    "        if len(arrays)>0:\n",
    "            with pathIds.open(\"wb\") as f:\n",
    "                np.save(f, arrays, allow_pickle=True, fix_imports=False)\n",
    "                \n",
    "        return t\n",
    "\n",
    "    def _process_all_1(self, texts:Collection[str]) -> List[List[str]]:\n",
    "        \"Process a list of `texts` in one process.\"\n",
    "        tok = self.tok_func(self.lang)\n",
    "        if self.special_cases: tok.add_special_cases(self.special_cases)\n",
    "        return [self.process_text(t, tok) for t in texts]\n",
    "\n",
    "    def process_all(self, texts:Collection[str]) -> List[List[str]]:\n",
    "        \"Process a list of `texts`.\"\n",
    "        print(\"FileTokenizer process_all\")\n",
    "        \n",
    "        if self.n_cpus <= 1: return self._process_all_1(texts)\n",
    "        print(f\"cpus: {self.n_cpus} number of files:{len(texts)}\")\n",
    "        with ProcessPoolExecutor(self.n_cpus) as e:\n",
    "            #return e.map(self._process_all_1, partition_by_cores(texts, self.n_cpus))\n",
    "            return sum(e.map(self._process_all_1, partition_by_cores(texts, self.n_cpus)), [])\n",
    "        \n",
    "    @staticmethod\n",
    "    def getIds_from_file(files):\n",
    "        idArrays=[]\n",
    "        for fp in files:\n",
    "            with fp.open(\"rb\") as f:\n",
    "                a = np.load(f)\n",
    "                if len(a) > 0: idArrays.extend( a )\n",
    "        return idArrays \n",
    "    \n",
    "    def getIds(self, n_cpus=defaults.cpus):\n",
    "        #threading does not help on speed in this case :(\n",
    "        files = list(self.tokPath.glob(\"*-ids.npy\"))\n",
    "        \n",
    "        #3use_cores = max(1,defaults.cpus)\n",
    "        print(f\"threading with on {n_cpus} cores\")\n",
    "        pool = ThreadPool(n_cpus) \n",
    "\n",
    "        results = pool.map(FileTokenizer.getIds_from_file, partition_by_cores(files, n_cpus))\n",
    "        pool.close() \n",
    "        pool.join()\n",
    "    \n",
    "    \n",
    "        idArrays=[]\n",
    "        for a in results:idArrays.extend(a)\n",
    "        idArrays = np.asarray(idArrays,dtype=object)    \n",
    "        return idArrays \n",
    "    \n",
    "    \"\"\"\n",
    "    def getIds(self):\n",
    "        pool = ThreadPool() \n",
    "\n",
    "        files = list(self.tokPath.glob(\"*-ids.npy\"))\n",
    "        idArrays=[]\n",
    "        for p in files:\n",
    "            with p.open(\"rb\") as f:\n",
    "                a = np.load(f)\n",
    "                if len(a) > 0: idArrays.extend( a )\n",
    "                    \n",
    "        idArrays = np.asarray(idArrays,dtype=object)    \n",
    "        return idArrays \n",
    "    \"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileTokenizeProcessor(PreProcessor):\n",
    "    \"`PreProcessor` that tokenizes the texts in `ds`.\"\n",
    "    def __init__(self, ds:ItemList=None, tokenizer:Tokenizer=None, chunksize:int=10000, mark_fields:bool=False):\n",
    "        self.tokenizer,self.chunksize,self.mark_fields = ifnone(tokenizer, Tokenizer()),chunksize,mark_fields\n",
    "\n",
    "    def process_one(self, item):  return self.tokenizer._process_all_1([item])[0]\n",
    "    def process(self, ds):\n",
    "        print(\"FileTokenizeProcessor process\")\n",
    "        #ds.items = _join_texts(ds.items, self.mark_fields)\n",
    "        self.tokenizer.process_all(ds.items)\n",
    "        #ds.items = self.tokenizer.process_all(ds.items)\n",
    "        #ds.items = tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer SentencepieceTokenizer in en with the following rules:\n",
      " - fix_html\n",
      " - replace_rep\n",
      " - replace_wrep\n",
      " - spec_add_spaces\n",
      " - rm_useless_spaces\n",
      " - replace_all_caps\n",
      " - deal_caps\n",
      "\n",
      "size og vocabulary: 32000\n",
      "pad_idx: 2\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "pad_idx   = spt.vocab().numericalize([text.transform.PAD])[0]\n",
    "vocab,max_vocab = spt.vocab(), len(spt.vocab().itos)\n",
    "trainTokenizer = FileTokenizer(pathTrainValid/\"toks/train\", SentencepieceTokenizer,\"en\",vocab)\n",
    "validTokenizer = FileTokenizer(pathTrainValid/\"toks/valid\", SentencepieceTokenizer,\"en\",vocab)\n",
    "\n",
    "print(trainTokenizer)\n",
    "print(\"size og vocabulary:\", max_vocab)\n",
    "print(\"pad_idx:\",pad_idx)\n",
    "\n",
    "print(spt.vocab().numericalize( [\"<unk>\" ,\"xxbos\" ,\"xxpad\" ,\"xxmaj\" ,\"xxup\" ,\"xxrep\" ,\"xxwrep\", \"xxfld\"]  ))\n",
    "#sentence = [\"She is tall.\", \"He is small\"]\n",
    "#tokenizer._process_all_1(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import timeit start = timeit.default_timer()\n",
    "pathTxt = pathTrainValid/\"txt\"\n",
    "files   = np.asarray( list(pathTxt.glob(\"*.txt\")) )\n",
    "nrows   = len(files)\n",
    "split   = 0.2\n",
    "splitindex, index = int(nrows*split+.5), np.random.permutation(np.arange(nrows)) \n",
    "\n",
    "chunksize=0\n",
    "\n",
    "trainList = TextList( files[:-splitindex], vocab=vocab, pad_idx=pad_idx, \n",
    "                     processor=[FileTokenizeProcessor(tokenizer=trainTokenizer, chunksize=chunksize, mark_fields=False)] )\n",
    "validList = TextList( files[-splitindex:], vocab=vocab, pad_idx=pad_idx, \n",
    "                     processor=[FileTokenizeProcessor(tokenizer=validTokenizer, chunksize=chunksize, mark_fields=False)] )\n",
    "#print(timeit.default_timer()-start )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%time p = trainList.process()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%time p = validList.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 nb: used GB memory: 4.63\n",
      "threading with on 1 cores\n",
      "CPU times: user 1min 49s, sys: 9.74 s, total: 1min 59s\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "print(\"1 nb: used GB memory:\", usedGB_RAM()) \n",
    "%time trainIDS = trainTokenizer.getIds(n_cpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threading with on 1 cores\n",
      "CPU times: user 35.6 s, sys: 5.16 s, total: 40.8 s\n",
      "Wall time: 41.7 s\n",
      "2 nb: used GB memory: 8.51\n"
     ]
    }
   ],
   "source": [
    "%time validIDS = validTokenizer.getIds(n_cpus=1)\n",
    "gc.collect()\n",
    "print(\"2 nb: used GB memory:\", usedGB_RAM()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning:    number of arrays:25527730 - number of ids:2046492393\n",
      "Validation: number of arrays:8504038 - number of ids:733295647\n"
     ]
    }
   ],
   "source": [
    "print(f\"Traning:    number of arrays:{len(trainIDS)} - number of ids:{np.sum([len(ids) for ids in trainIDS])}\")\n",
    "print(f\"Validation: number of arrays:{len(validIDS)} - number of ids:{np.sum([len(ids) for ids in validIDS])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min:1 max:12985 - median:63.0\n",
      "1521907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([134,  71,  70,  16,  35,  39, 269, 261, 146,  89,  39,  43,  42,  20,  33, 132,  87,  67,  12,  36], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sectionlengths = np.asarray([len(s) for s in trainIDS],dtype=np.int32)\n",
    "n, bins, patches = plt.hist(sectionlengths[sectionlengths<1000], 100, density=True, facecolor='g', alpha=0.75)\n",
    "np.histogram(sectionlengths[sectionlengths<1000],50)\n",
    "\n",
    "print(f\"min:{min(sectionlengths)} max:{np.max(sectionlengths)} - median:{np.median(sectionlengths)}\")\n",
    "print(np.sum(sectionlengths<10))\n",
    "sectionlengths[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import traceback\n",
    "def usedGB_RAM(): \n",
    "    import psutil\n",
    "    return round((psutil.virtual_memory().used + psutil.swap_memory().used)/1e9,2)\n",
    "\n",
    "#missing backwards\n",
    "class MyLanguageModelLoader():\n",
    "    \"Create a dataloader with bptt slightly changing.\"\n",
    "    \n",
    "    class CircularIndex():\n",
    "        def __init__(self, length:int ): self.idx = np.arange(length)\n",
    "        #if the index exceed length then getitem wrap the key and starrt return key-len(self.idx)\n",
    "        def __getitem__(self, key): return self.idx[key%len(self.idx)]\n",
    "        def __len__(self) -> int: return len(self.idx)\n",
    "        def shuffle(self): \n",
    "            print(\"shuffle\")\n",
    "            np.random.shuffle(self.idx)\n",
    "                \n",
    "    def __init__(self, dataset:LabelList, bs:int=64, bptt:int=70, backwards:bool=False, shuffle:bool=False,\n",
    "                 max_len:int=25, p_bptt:int=0.95):\n",
    "        self.dataset,self.bs,self.bptt,self.backwards,self.shuffle,self.p_bptt = dataset,bs,bptt,backwards,shuffle,p_bptt\n",
    "        self.idx        = MyLanguageModelLoader.CircularIndex(len(self.dataset))\n",
    "        #self.idx        = np.arange(len(self.dataset))\n",
    "        self.ragLengths = np.asarray([len(s) for s in dataset.x.items],dtype=np.int) if len(dataset.x.items) > 0 else []\n",
    "        self.nToks      = np.sum(self.ragLengths)\n",
    "        \n",
    "        self.min_seq,self.max_seq = 5,max_len\n",
    "        self.num_workers = 0\n",
    "        self.init_kwargs = dict(bs=bs, bptt=bptt, backwards=backwards, shuffle=shuffle, max_seq=max_len)\n",
    "\n",
    "        print(f\"LanguageModelLoader.__init__ - used GB memory: {usedGB_RAM()} length of dataset:{len(self.ragLengths)} \"+\\\n",
    "              f\"\\nnToks:{self.nToks} bptt:{self.bptt} max_seq:{self.max_seq} shuffle:{self.shuffle}\" )  \n",
    "        \n",
    "    def __iter__(self):\n",
    "        if getattr(self.dataset, 'item', None) is not None: \n",
    "            yield LongTensor(getattr(self.dataset, 'item'))[None],LongTensor([0])\n",
    "        if self.shuffle: \n",
    "            self.idx.shuffle()\n",
    "            #np.random.shuffle(self.idx)\n",
    "\n",
    "        #print(f\"LanguageModelLoader.__iter__ - used GB memory: {usedGB_RAM()} length of dataset:{len(self.ragLengths)} \"+\\\n",
    "        #      f\"nToks:{self.nToks} bptt:{self.bptt} max_seq:{self.max_seq}\" )  \n",
    "        \n",
    "        self.offset,self.ei,self.eo  = 0,0,1\n",
    "        while self.offset + self.min_seq*self.bs < self.nToks-1:\n",
    "            if self.offset==0: \n",
    "                seq_len = self.bptt + self.max_seq\n",
    "            else:\n",
    "                bptt    = self.bptt if np.random.random() < self.p_bptt else self.bptt / 2.\n",
    "                seq_len = max(self.min_seq, int(np.random.normal(bptt, 5)))\n",
    "                seq_len = min(seq_len,      self.bptt + self.max_seq)\n",
    "                \n",
    "            res, seq_len = self.get_batch(self.offset, seq_len)\n",
    "            #if self.offset==0 : print(res)\n",
    "            self.offset += self.bs*seq_len\n",
    "            yield res\n",
    "            \n",
    "    def get_batch(self, offset:int, seq_len:int) -> Tuple[LongTensor, LongTensor]:\n",
    "        \"Create a batch at `i` of a given `seq_len`.\"\n",
    "        # create a batch from the ragged token array\n",
    "        \n",
    "        #offset: the number of processed tokens in the batch    \n",
    "        #seq_len: the number of tokens that the caller wants in the batch. We do not wrap around \n",
    "        #         to pick from the start so the returned seq_len in the batch may be smaller \n",
    "        #bi: index of the first rag to be extract\n",
    "        #bo: index where the extract should be in the first rag\n",
    "        #ei: index of the last rag to be extract\n",
    "        #bo: index (not inclusive) where the extract stops in the last rag\n",
    "        \n",
    "        seq_max = (self.nToks-offset)//self.bs\n",
    "        seq_len = min( seq_len, seq_max)\n",
    "        nToks   = self.bs*(seq_len)\n",
    "        \n",
    "        #find the last rag indexes for the batch\n",
    "        bi,bo,cSum = self.ei, self.eo-1, 0\n",
    "        for j in range(bi,len(self.ragLengths)):\n",
    "            rl_i    = self.ragLengths[self.idx[j]]\n",
    "            rl     = (rl_i-bo) if j==bi else rl_i \n",
    "            cSum += rl\n",
    "            #if seq_len==seq_max: print(f\"j:{j} raglength:{self.ragLengths[self.idx[j]} rl:{rl} nToks:{nToks} cSum:{cSum}\")\n",
    "            if nToks <= cSum: \n",
    "                self.ei = j\n",
    "                self.eo = nToks-(cSum-rl) if j>bi else nToks-(cSum-rl) + bo\n",
    "                break\n",
    "            \n",
    "        #Extract the batch\n",
    "        tok_seq = []\n",
    "        for j in range(bi,self.ei+1):\n",
    "            #notice the dobbeltindexing that uses the randomization of the rags\n",
    "            rag = self.dataset.x.items[self.idx[j]]\n",
    "            if   bi==self.ei: toks = rag[bo:self.eo]\n",
    "            elif  j==bi     : toks = rag[bo:] \n",
    "            elif  j==self.ei: toks = rag[:self.eo]\n",
    "            else:             toks = rag    \n",
    "            tok_seq.extend(toks)\n",
    "        #print(f\"rag:{j} rl:{len(rag)} toks:{len(toks)}\")\n",
    "\n",
    "        #print( f\"offset:{offset} seq_len:{seq_len} seq_max:{seq_max} nToks:{nToks} nBToks:{len(tok_seq)} \"+\\\n",
    "        #       f\"bi:{bi} bo:{bo} ei:{self.ei} eo:{self.eo}\" )\n",
    "        \n",
    "        tok_seq = np.asarray(tok_seq).reshape(self.bs,-1)\n",
    "        data    = LongTensor(tok_seq)\n",
    "        res     = data[:,0:seq_len-1], data[:,1:seq_len]\n",
    "        return res, seq_len\n",
    "\n",
    "    def __len__(self) -> int: \n",
    "        #stack = ''.join(traceback.format_stack()[-levels=5:])\n",
    "        #print(f\"__len__: {stack}\")\n",
    "        return int(math.ceil((self.nToks-1) / self.bptt))\n",
    "    def __getattr__(self,k:str)->Any: return getattr(self.dataset, k)\n",
    "\n",
    "    @property\n",
    "    def batch_size(self): return self.bs\n",
    "    @batch_size.setter\n",
    "    def batch_size(self, v): self.bs = v\n",
    "\n",
    "    def batchify(self, data:np.ndarray) -> LongTensor: pass\n",
    "        #\"Split the corpus `data` in batches.\"\n",
    "        #nb = data.shape[0] // self.bs\n",
    "        #data = np.array(data[:nb*self.bs]).reshape(self.bs, -1)\n",
    "        #if self.backwards: data=data[:,::-1].copy()\n",
    "        #return LongTensor(data)\n",
    "\n",
    "class MyTextLMDataBunch(TextLMDataBunch):\n",
    "    \"Create a `TextDataBunch` suitable for training a language model.\"\n",
    "    @classmethod\n",
    "    def from_ids(cls, path:PathOrStr, vocab:Vocab, \n",
    "                 train_ids:Collection[Collection[int]],        valid_ids:Collection[Collection[int]],\n",
    "                 test_ids:Collection[Collection[int]]=None, \n",
    "                 train_lbls:Collection[Union[int,float]]=None, valid_lbls:Collection[Union[int,float]]=None, \n",
    "                 classes:Collection[Any]=None, processor:PreProcessor=None, **kwargs) -> DataBunch:\n",
    "        \"Create a `TextDataBunch` from ids, labels and a `vocab`.\"\n",
    "        src = LabelLists(path, TextList(train_ids, vocab, path=path, processor=[]),\n",
    "                               TextList(valid_ids, vocab, path=path, processor=[]))\n",
    "        #src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_lists(train_lbls, valid_lbls, classes=classes, processor=[]) \n",
    "        src.train = src.train.label_for_lm()\n",
    "        src.valid = src.valid.label_for_lm()\n",
    "\n",
    "        #if test_ids is not None: src.add_test(TextList(test_ids, vocab, path=path), label=train_lbls[0])\n",
    "        #src.valid.x.processor = ifnone(processor, [TokenizeProcessor(), NumericalizeProcessor(vocab=vocab)])\n",
    "       \n",
    "        src.train.x._bunch = MyTextLMDataBunch\n",
    "        src.valid.x._bunch = MyTextLMDataBunch\n",
    "        return src.databunch(**kwargs)\n",
    "    \n",
    "    #need customized version of this in order to set MyLanguageModelLoader\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', no_check:bool=False, **kwargs) -> DataBunch:\n",
    "        \"Create a `TextDataBunch` in `path` from the `datasets` for language modelling.\"\n",
    "        print(\"MyTextLMDataBunch def create\")\n",
    "        datasets    = cls._init_ds(train_ds, valid_ds, test_ds)\n",
    "        dataloaders = [MyLanguageModelLoader(ds, shuffle=(i==0), **kwargs) for i,ds in enumerate(datasets)]\n",
    "        return cls(*dataloaders, path=path, no_check=no_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyTextLMDataBunch def create\n",
      "LanguageModelLoader.__init__ - used GB memory: 10.4 length of dataset:4000 \n",
      "nToks:252772 bptt:70 max_seq:25 shuffle:True\n",
      "LanguageModelLoader.__init__ - used GB memory: 10.4 length of dataset:800 \n",
      "nToks:73497 bptt:70 max_seq:25 shuffle:False\n",
      "LanguageModelLoader.__init__ - used GB memory: 10.4 length of dataset:4000 \n",
      "nToks:252772 bptt:70 max_seq:25 shuffle:False\n"
     ]
    }
   ],
   "source": [
    "#%%debug\n",
    "#i have an issue with passing pad_idx\n",
    "dblm = MyTextLMDataBunch.from_ids( pathTrainValid, vocab, trainIDS[0:4000], validIDS[0:800], bs=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dblm.train_ds.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 nb: used GB memory: 10.4\n",
      "CPU times: user 474 ms, sys: 77.5 ms, total: 552 ms\n",
      "Wall time: 635 ms\n",
      "2 nb: used GB memory: 10.55\n"
     ]
    }
   ],
   "source": [
    "print(\"1 nb: used GB memory:\", usedGB_RAM()) \n",
    "%time learn = language_model_learner(dblm, drop_mult=0, qrnn=False, pad_token=-1, callback_fns=ShowGraph)\n",
    "print(\"2 nb: used GB memory:\", usedGB_RAM()) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%time learn.lr_find()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learn.recorder.plot(skip_start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1 nb: used GB memory: 10.55\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 1:17:34 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>9.728927</th>\n",
       "    <th>8.954528</th>\n",
       "    <th>0.054251</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>8.784595</th>\n",
       "    <th>7.969549</th>\n",
       "    <th>0.054087</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>8.104761</th>\n",
       "    <th>7.505545</th>\n",
       "    <th>0.054339</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>7.676877</th>\n",
       "    <th>7.319628</th>\n",
       "    <th>0.053750</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>7.390817</th>\n",
       "    <th>7.192531</th>\n",
       "    <th>0.054520</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>6</th>\n",
       "    <th>7.191383</th>\n",
       "    <th>7.124517</th>\n",
       "    <th>0.052948</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>7</th>\n",
       "    <th>7.068231</th>\n",
       "    <th>7.070170</th>\n",
       "    <th>0.054729</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>8</th>\n",
       "    <th>6.983055</th>\n",
       "    <th>7.058527</th>\n",
       "    <th>0.054970</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>9</th>\n",
       "    <th>6.924092</th>\n",
       "    <th>7.030709</th>\n",
       "    <th>0.062767</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>10</th>\n",
       "    <th>6.874950</th>\n",
       "    <th>7.022233</th>\n",
       "    <th>0.059933</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle\n",
      "shuffle\n",
      "shuffle\n",
      "shuffle\n",
      "shuffle\n",
      "shuffle\n",
      "shuffle\n",
      "shuffle\n",
      "shuffle\n",
      "CPU times: user 4h 3min 49s, sys: 12min 28s, total: 4h 16min 18s\n",
      "Wall time: 1h 17min 34s\n",
      "2 nb: used GB memory: 10.25\n",
      "1339\n"
     ]
    }
   ],
   "source": [
    "print(gc.collect())\n",
    "print(\"1 nb: used GB memory:\", usedGB_RAM()) \n",
    "%time learn.fit_one_cycle(10, 2e-3, moms=(0.8,0.7))\n",
    "print(\"2 nb: used GB memory:\", usedGB_RAM()) \n",
    "print(gc.collect())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learn.fit_one_cycle(5, 2e-3, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learn.fit_one_cycle(1, 2e-4, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learn.save('model-32k-sentencepiece-vocab')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(text.transform.BOS)\n",
    "print(text.transform.FLD)\n",
    "print(text.transform.PAD)\n",
    "type(data_lm_full.train_ds.y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.exp(4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.exp(3.239415)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_lm_full.train_ds.x.items"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dblm.save('full_lm')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dblm = TextLMDataBunch.load(pathTrainValid, 'full_lm', bs=64)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#%%debug\n",
    "#import pdb; .set_trace() \n",
    "\n",
    "\n",
    "data_lm_full = TextLMDataBunch.from_csv( pathTrainValid, csv_name='deepfrance.csv', text_cols=0, label_cols=1,\n",
    "                                         tokenizer=tokenizer, vocab=vocab,\n",
    "                                         max_vocab=max_vocab,\n",
    "                                         min_freq=0,\n",
    "                                         chunksize=100\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dblm.show_batch()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learn = language_model_learner(dblm, drop_mult=0, qrnn=False, pad_token=-1, callback_fns=ShowGraph)\n",
    "learn.lr_find()\n",
    "learn.recorder.plot(skip_start=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "while training a very large language model i noticed that \"fix_ds\" in basic_data.py => def _init_ds create an extra dataset that will lead to an instance of LanguageModelLoader.\n",
    "Why would we need an extra dataset and LanguageModelLoader. The downside is loss of memory\n",
    "\n",
    "I can see and measure that it is a copy of the training data and that it uses some memory - which is bad.\n",
    "\n",
    "Why do we need it ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
