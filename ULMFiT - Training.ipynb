{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from fastai import *\n",
    "from fastai.text import * \n",
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=Path(\"../nlp-data/fr/wiki-train_valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training and validation data we prepared in wiki_preparation.ipynb. In total 100 million articles with a split of 80% / 20% for training/validation:\n",
    "- First column: text content to train the model. \n",
    "- Second column: Boolean representing if the data is for training or validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>textWords</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>184201</th>\n",
       "      <td>10468104</td>\n",
       "      <td>Références.\\n \\n</td>\n",
       "      <td>1</td>\n",
       "      <td>Nord and Bert Couldn't Make Head or Tail of It</td>\n",
       "      <td>https://fr.wikipedia.org/wiki?curid=10468104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264477</th>\n",
       "      <td>7052901</td>\n",
       "      <td>\\nPatronyme.\\n,</td>\n",
       "      <td>1</td>\n",
       "      <td>Contaut</td>\n",
       "      <td>https://fr.wikipedia.org/wiki?curid=7052901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851122</th>\n",
       "      <td>5923581</td>\n",
       "      <td>Eichner:\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>Eichner</td>\n",
       "      <td>https://fr.wikipedia.org/wiki?curid=5923581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22453</th>\n",
       "      <td>4094043</td>\n",
       "      <td>Balleny:\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>Balleny</td>\n",
       "      <td>https://fr.wikipedia.org/wiki?curid=4094043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728494</th>\n",
       "      <td>6124549</td>\n",
       "      <td>Scholtz:</td>\n",
       "      <td>1</td>\n",
       "      <td>Scholtz</td>\n",
       "      <td>https://fr.wikipedia.org/wiki?curid=6124549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id              text  textWords  \\\n",
       "184201   10468104  Références.\\n \\n          1   \n",
       "1264477   7052901   \\nPatronyme.\\n,          1   \n",
       "851122    5923581        Eichner:\\n          1   \n",
       "22453     4094043        Balleny:\\n          1   \n",
       "728494    6124549          Scholtz:          1   \n",
       "\n",
       "                                                  title  \\\n",
       "184201   Nord and Bert Couldn't Make Head or Tail of It   \n",
       "1264477                                         Contaut   \n",
       "851122                                          Eichner   \n",
       "22453                                           Balleny   \n",
       "728494                                          Scholtz   \n",
       "\n",
       "                                                  url  \n",
       "184201   https://fr.wikipedia.org/wiki?curid=10468104  \n",
       "1264477   https://fr.wikipedia.org/wiki?curid=7052901  \n",
       "851122    https://fr.wikipedia.org/wiki?curid=5923581  \n",
       "22453     https://fr.wikipedia.org/wiki?curid=4094043  \n",
       "728494    https://fr.wikipedia.org/wiki?curid=6124549  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfText = pd.read_csv(path/'wiki.csv')\n",
    "#sort increasing and take the last nrows for training and validatiom\n",
    "dfText.sort_values('textWords', inplace=True)\n",
    "display(dfText.head())\n",
    "\n",
    "#select the sections with most words\n",
    "nrows  = int(1e6)\n",
    "dfText = dfText[-nrows:]\n",
    "\n",
    "#split the data into train and validation\n",
    "split, index = 0.2, np.random.permutation(np.arange(nrows))\n",
    "splitindex   = int(nrows*split+.5)\n",
    "\n",
    "dfText.drop(labels=[\"title\",\"url\",\"textWords\"],axis=1,inplace=True)\n",
    "\n",
    "dfText[\"is_valid\"] = False\n",
    "dfText.iloc[index[:splitindex], dfText.columns.get_loc(\"is_valid\")] = True\n",
    "dfText.to_csv(path/\"train_and_valid.csv\", header=None, index=None)\n",
    "\n",
    "train_and_valid = pd.read_csv(path/'train_and_valid.csv', header=None, names=['label', 'content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7840298</th>\n",
       "      <td>Georges Turines, né le à Lautignac et mort le ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3343103</th>\n",
       "      <td>Le Lac Travis est un réservoir situé sur le Co...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593728</th>\n",
       "      <td>Vraptchichté (en , en ) est une municipalité d...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4139306</th>\n",
       "      <td>Tigran Levonovitch Petrossian (en ) est un gra...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5636896</th>\n",
       "      <td>L'Oberlin noir (N595) est un cépage.\\nDescript...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     label  content\n",
       "7840298  Georges Turines, né le à Lautignac et mort le ...    False\n",
       "3343103  Le Lac Travis est un réservoir situé sur le Co...    False\n",
       "1593728  Vraptchichté (en , en ) est une municipalité d...    False\n",
       "4139306  Tigran Levonovitch Petrossian (en ) est un gra...    False\n",
       "5636896  L'Oberlin noir (N595) est un cépage.\\nDescript...     True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tokenizer\n",
    "The sentencepiece vocabulary was trained in Train Sentencepiece tokenizer.ipynb.\n",
    "\n",
    "Here we will make a BasicTokenizer from Sentencepiece so that fastai can use it instead of spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<s>', '</s>', '▁de', '.', ',', \"'\", 's', '▁la', '▁et']\n"
     ]
    }
   ],
   "source": [
    "class SentencepieceTokenizer(BaseTokenizer):\n",
    "    def __init__(self, path:PathOrStr, cache_name:str='tmp'):\n",
    "        text.transform.UNK = \"<unk>\"\n",
    "        text.transform.BOS = \"<s>\"\n",
    "        text.transform.PAD = \"<pad>\"\n",
    "        \n",
    "        self.tok = spm.SentencePieceProcessor()\n",
    "        self.tok.Load(str(Path(path) / cache_name / 'm.model'))\n",
    "        \n",
    "        self.vocab_ = SentencepieceTokenizer.loadvocab_(path, cache_name)\n",
    "        \n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        #get the tokens and replace unk from sentencepiece with unk from fastai\n",
    "        return self.tok.EncodeAsPieces(t)\n",
    "        #return [text.transform.UNK if t==\"<unk>\" else t for t in self.tok.EncodeAsPieces(t)]\n",
    "    \n",
    "    def add_special_cases(self, toks:Collection[str]):\n",
    "        #this should have been done when training sentencepiece\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def vocab(self): return self.vocab_\n",
    "    @staticmethod\n",
    "    def loadvocab_(path:PathOrStr, cache_name):\n",
    "        p_vocab = Path(path) / cache_name / \"m.vocab\"\n",
    "        with open(str(p_vocab), 'r') as f:\n",
    "            vocab = [line.split('\\t')[0] for line in f.readlines()]\n",
    "        \n",
    "        p_itos = Path(path) / cache_name / 'itos.pkl'\n",
    "        pickle.dump(vocab, open( p_itos, 'wb'))\n",
    "        vocab_ = Vocab(pickle.load(open(p_itos, 'rb')))\n",
    "        return vocab_\n",
    "\n",
    "spt       = SentencepieceTokenizer(path, cache_name=\"sp-model\")\n",
    "tokenizer = Tokenizer(tok_func=spt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fastai.text.transform.Vocab'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 7, 8, 9]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = pt.vocab()\n",
    "print(type(vocab))\n",
    "vocab.textify(np.arange(10))\n",
    "vocab.numericalize([\"<unk>\", \"<s>\", \"</s>\", \"▁de\", \".\" ,\"s\", \"▁la\", \"▁et\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "text.transform.BOS = \"<s>\"\n",
    "#fastai.text.transform.FLD,\n",
    "text.transform.UNK = \"<unk>\" \n",
    "#text.transform.PAD = 'xxbos','xxfld','xxunk','xxpad'\n",
    "\n",
    "#def loadVocab(self, path:PathOrStr, cache_name:str='tmp'):\n",
    "p = Path(path) / \"sp-model\" / \"m.vocab\"\n",
    "#p = Path(path) / cache_name / \"m.vocab\"\n",
    "with open(str(p), 'r') as f:\n",
    "    vocab = [line.split('\\t')[0] for line in f.readlines()]\n",
    "    #vocab[0] = UNK\n",
    "    #vocab[1] = BOS\n",
    "    \n",
    "    #vocab[pad_idx] = PAD\n",
    "#vocab[\"<s>\"]  \n",
    "#        pickle.dump(vocab, open(path / 'itos.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<s>', '</s>', '▁de', '.', ',', \"'\", 's', '▁la', '▁et']\n",
      "<s>\n",
      "<unk>\n",
      "xxfld\n",
      "<pad>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fastai.text.transform.Vocab at 0x1a804a42b0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = SentencepieceTokenizer(path,\"sp-model\")\n",
    "print(text.transform.BOS)\n",
    "print(text.transform.UNK)\n",
    "print(text.transform.FLD)\n",
    "print(text.transform.PAD)\n",
    "tokenizer.vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train two LM: one with a 60k vocabulary and one with a 30k vocabulary. The two models have different performance and computation needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm_full = (TextList.from_csv(path, csv_name=\"train_and_valid.csv\", tokenizer=tokenizer, vocab=tokenizer.vocab())\n",
    "                #Inputs: all the text files in path\n",
    "                #.split_from_df(col=1)\n",
    "                #We may have other temp folders that contain text files so we only keep what's in train and test\n",
    "                #.label_for_lm()           \n",
    "                #We want to do a language model so we label accordingly\n",
    "                .databunch(bs=32)\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm_full.save('full_lm_60k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm_full = TextLMDataBunch.load(PATH, 'full_lm_60k', bs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_lm_full.train_ds.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm_full.show_batch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm_full, drop_mult=0, qrnn=True, callback_fns=ShowGraph)\n",
    "learn.lr_find()\n",
    "learn.recorder.plot(skip_start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm_full, drop_mult=0, qrnn=True, callback_fns=ShowGraph)\n",
    "learn.fit_one_cycle(10, 2e-3, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, 2e-3, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 2e-4, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('model-60k-vocab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train a model with a 30k vocabulary.\n",
    "Because of this, batch size can be higher and training is quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm_full = (TextList.from_csv(PATH, csv_name='fulltrain.csv', cols=0, processor=[TokenizeProcessor(tokenizer=tokenizer), NumericalizeProcessor(max_vocab=30000)])\n",
    "           #Inputs: all the text files in path\n",
    "            .split_from_df(col=1)\n",
    "           #We may have other temp folders that contain text files so we only keep what's in train and test\n",
    "            .label_for_lm()           \n",
    "           #We want to do a language model so we label accordingly\n",
    "            .databunch(bs=64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm_full.save('full_lm_30k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm_full = TextLMDataBunch.load(PATH, 'full_lm_30k', bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_lm_full.train_ds.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm_full, drop_mult=0, qrnn=False, callback_fns=ShowGraph)\n",
    "learn.lr_find()\n",
    "learn.recorder.plot(skip_start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm_full, drop_mult=0, qrnn=False, callback_fns=ShowGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model perplexity is exp(validation loss):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(3.377596)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('model-30k-vocab')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
