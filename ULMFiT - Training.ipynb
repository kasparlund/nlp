{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from fastai import *\n",
    "from fastai.text import * \n",
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathTrainValid=Path(\"../../data/nlp-data/fr/wiki-train_valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training and validation data we prepared in wiki_preparation.ipynb. In total 100 million articles with a split of 80% / 20% for training/validation:\n",
    "- First column: text content to train the model. \n",
    "- Second column: Boolean representing if the data is for training or validation."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfData = pd.read_csv(pathTrainValid/'wiki.csv')\n",
    "#sort increasing and take the last nrows for training and validatiom\n",
    "dfData.sort_values('textWords', inplace=True)\n",
    "display(dfData.head())\n",
    "\n",
    "#select the sections with most words\n",
    "nrows  = int(1e6)\n",
    "dfData = dfData[-nrows:]\n",
    "\n",
    "#split the data into train and validation\n",
    "dfData[\"is_valid\"] = False\n",
    "split, index       = 0.2, np.random.permutation(np.arange(nrows))\n",
    "splitindex         = int(nrows*split+.5)\n",
    "dfData.iloc[index[:splitindex], dfData.columns.get_loc(\"is_valid\")] = True\n",
    "\n",
    "\n",
    "#organize the columns as expect by fastai\n",
    "dfData.drop(labels=[\"title\",\"url\"],axis=1,inplace=True)\n",
    "\n",
    "column_order=[\"title\",\"is_valid\",\"textwords\"]\n",
    "dfData.reindex(column_order, axis = 1)\n",
    "\n",
    "dfData.to_csv(pathTrainValid/\"train_and_valid.csv\", index=None)\n",
    "\n",
    "print(\"Wordcount in training and validation set\" )\n",
    "display(dfData.groupby([\"is_valid\"])[\"textWords\"].sum())\n",
    "display(dfData.head())\n",
    "\n",
    "dfData=None\n",
    "#dfText = pd.read_csv(path/'train_and_valid.csv')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#create lm trainingfile with only two column\n",
    "dfData = pd.read_csv(pathTrainValid/'train_and_valid.csv')\n",
    "\n",
    "dfData.drop([\"id\",\"textWords\"],axis=1,inplace=True)\n",
    "display(dfData.head())\n",
    "\n",
    "dfData.columns = ['content', 'is_valid']\n",
    "dfData.head()\n",
    "dfData.to_csv(pathTrainValid/\"deepfrance.csv\", index=None, header=None)\n",
    "dfData=None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tokenizer\n",
    "The sentencepiece vocabulary was trained in Train Sentencepiece tokenizer.ipynb.\n",
    "\n",
    "Here we will make a BasicTokenizer from Sentencepiece so that fastai can use it instead of spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencepieceTokenizer(BaseTokenizer):\n",
    "    def __init__(self, lang:str):\n",
    "        path,cache_name = pathTrainValid, \"sp-model\"\n",
    "    #def __init__(self, path:PathOrStr, cache_name:str='sp-model'):\n",
    "        self.pathVocab = path / cache_name\n",
    "        self.vocab_    = Vocab(pickle.load(open(self.pathVocab/'itos.pkl', 'rb')))\n",
    "        self.tok       = spm.SentencePieceProcessor()\n",
    "        \n",
    "        self.tok.Load(str(Path(path) / cache_name / 'm.model'))\n",
    "        text.transform.UNK = \"<unk>\"\n",
    "\n",
    "    #def __call__(self, language:str): return self    \n",
    "        \n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        return self.tok.EncodeAsPieces(t)\n",
    "    \n",
    "    def add_special_cases(self, toks:Collection[str]):\n",
    "        #this should have been done when training sentencepiece\n",
    "        pass\n",
    "    \n",
    "    def vocab(self): return self.vocab_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spt       = SentencepieceTokenizer(lang=\"fr\")\n",
    "tokenizer = Tokenizer(SentencepieceTokenizer,\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spt.vocab().textify(np.arange(30)))\n",
    "\n",
    "sentence = [\"Elle est grande. Il est petit\", \"Il est petit. Elle est grande.\"]\n",
    "#tokenizer._process_all_1(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spt.vocab().numericalize( [\"<unk>\" ,\"xxbos\" ,\"xxpad\" ,\"xxmaj\" ,\"xxup\" ,\"xxrep\" ,\"xxwrep\", \"xxfld\", \"‚ñÅde\"]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tok = spt.vocab().itos\n",
    "print(len(tok))\n",
    "#[print(t) for t in tok]\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train two LM: one with a 60k vocabulary and one with a 30k vocabulary. The two models have different performance and computation needs."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#from ..torch_core import *\n",
    "\n",
    "def my_join_texts(texts:Collection[str], mark_fields:bool=False):\n",
    "    if not isinstance(texts, np.ndarray): texts = np.array(texts)\n",
    "    if is1d(texts): texts = texts[:,None]\n",
    "    df = pd.DataFrame({i:texts[:,i] for i in range(texts.shape[1])})\n",
    "    #text_col = f'{BOS} {FLD} {1} ' + df[0] if mark_fields else  f'{BOS} ' + df[0]\n",
    "    text_col = df[0]\n",
    "    print(df.shape)\n",
    "    for i in range(1,len(df.columns)):\n",
    "        #text_col += (f' {FLD} {i+1} ' if mark_fields else ' ') + df[i]\n",
    "        text_col += df[i]\n",
    "    return text_col.values\n",
    "\n",
    "#text.data._join_texts = my_join_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import timeit start = timeit.default_timer()\n",
    "\n",
    "vocab,max_vocab  = spt.vocab(), len(spt.vocab().itos)\n",
    "#print(timeit.default_timer()-start )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%debug\n",
    "#import pdb; .set_trace() \n",
    "\n",
    "\n",
    "data_lm_full = TextLMDataBunch.from_csv( pathTrainValid, csv_name='deepfrance.csv', text_cols=0, label_cols=1,\n",
    "                                         tokenizer=tokenizer, vocab=vocab,\n",
    "                                         max_vocab=max_vocab,\n",
    "                                         min_freq=0\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_lm_full.train_ds.x.items"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_lm_full.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm_full.save('full_lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm_full = TextLMDataBunch.load(pathTrainValid, 'full_lm', bs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_lm_full.train_ds.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm_full.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm_full, drop_mult=0, qrnn=False, pad_token=-1, callback_fns=ShowGraph)\n",
    "learn.lr_find()\n",
    "learn.recorder.plot(skip_start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm_full, drop_mult=0, qrnn=False, pad_token=-1, callback_fns=ShowGraph)\n",
    "learn.fit_one_cycle(10, 2e-3, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learn.fit_one_cycle(5, 2e-3, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learn.fit_one_cycle(1, 2e-4, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('model-30k-sentencepiece-vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text.transform.BOS)\n",
    "print(text.transform.FLD)\n",
    "print(text.transform.PAD)\n",
    "type(data_lm_full.train_ds.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(3.239415)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train a model with a 30k vocabulary.\n",
    "Because of this, batch size can be higher and training is quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm_full = (TextList.from_csv(PATH, csv_name='fulltrain.csv', cols=0, processor=[TokenizeProcessor(tokenizer=tokenizer), NumericalizeProcessor(max_vocab=30000)])\n",
    "           #Inputs: all the text files in path\n",
    "            .split_from_df(col=1)\n",
    "           #We may have other temp folders that contain text files so we only keep what's in train and test\n",
    "            .label_for_lm()           \n",
    "           #We want to do a language model so we label accordingly\n",
    "            .databunch(bs=64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm_full.save('full_lm_30k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm_full = TextLMDataBunch.load(PATH, 'full_lm_30k', bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_lm_full.train_ds.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm_full, drop_mult=0, qrnn=False, callback_fns=ShowGraph)\n",
    "learn.lr_find()\n",
    "learn.recorder.plot(skip_start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm_full, drop_mult=0, qrnn=False, callback_fns=ShowGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model perplexity is exp(validation loss):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(3.377596)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('model-30k-vocab')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
