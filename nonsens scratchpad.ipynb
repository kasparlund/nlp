{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import * \n",
    "from fastai.text import * \n",
    "import numpy as np\n",
    "import io\n",
    "import pathlib\n",
    "import asyncio\n",
    "from fastai_sentencepiece import *\n",
    "from filetokenizer import *\n",
    "from languagemodelloader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang=\"en\"\n",
    "minToks = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pathData       = Path(\"../../data/nlp\")\n",
    "pathData       = Path(\"../nlp-data\")\n",
    "path           = pathData / lang\n",
    "pathDump       = path/\"wiki-dump\"\n",
    "pathJson       = path/\"wiki-json\"\n",
    "\n",
    "pathTrainValid = path/\"wiki-train_valid\"\n",
    "pathTxt        = pathTrainValid/\"txt\"\n",
    "pathToks       = pathTrainValid/\"toks\"\n",
    "pathcsv        = pathTrainValid/\"wiki.csv\"\n",
    "\n",
    "cache_name   = \"sp-model\"\n",
    "pathVocab    = pathTrainValid / cache_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.dtype:<class 'numpy.int16'>\n",
      "self.dtype:<class 'numpy.int16'>\n",
      "Tokenizer SentencepieceTokenizer in en with the following rules:\n",
      " - fix_html\n",
      " - replace_rep\n",
      " - replace_wrep\n",
      " - spec_add_spaces\n",
      " - rm_useless_spaces\n",
      " - rm_extra_lineshift\n",
      " - replace_all_caps\n",
      " - deal_caps\n",
      "\n",
      "size og vocabulary: 32000\n",
      "pad_idx: 2\n",
      "[0, 1, 0, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "#Discard section with kess than \"minTok\" tokens\n",
    "spt_func  = partial(SentencepieceTokenizer.create, pathVocab=pathVocab)\n",
    "spt_func.__name__ = SentencepieceTokenizer.__name__\n",
    "spt       = spt_func(lang=\"en\")\n",
    "pad_idx   = spt.vocab().numericalize([text.transform.PAD])[0]\n",
    "vocab,max_vocab = spt.vocab(), len(spt.vocab().itos)\n",
    "trainTokenizer = FileTokenizer(pathToks/\"train\", spt_func,\"en\",vocab,minToks=minToks,n_cpus=max(defaults.cpus-1,1))\n",
    "validTokenizer = FileTokenizer(pathToks/\"valid\", spt_func,\"en\",vocab,minToks=minToks,n_cpus=max(defaults.cpus-1,1))\n",
    "\n",
    "print(trainTokenizer)\n",
    "print(\"size og vocabulary:\", max_vocab)\n",
    "print(\"pad_idx:\",pad_idx)\n",
    "\n",
    "print(spt.vocab().numericalize( [\"xxunk\" ,\"xxbos\",\"xxeos\" ,\"xxpad\" ,\"xxmaj\" ,\"xxup\" ,\"xxrep\" ,\"xxwrep\", \"xxfld\"]  ))\n",
    "#sentence = [\"She is tall.\", \"He is small\"]\n",
    "#tokenizer._process_all_1(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files   = np.asarray( list(pathTxt.glob(\"*.txt\")) )\n",
    "nrows   = len(files)\n",
    "split   = 0.2\n",
    "splitindex, index = int(nrows*split+.5), np.random.permutation(np.arange(nrows)) \n",
    "\n",
    "chunksize=0\n",
    "\n",
    "trainList = TextList( files[:-splitindex], vocab=vocab, pad_idx=pad_idx, \n",
    "                      processor=[FileTokenizeProcessor(tokenizer=trainTokenizer, \n",
    "                                                       chunksize=chunksize, mark_fields=False)])\n",
    "\n",
    "validList = TextList( files[-splitindex:], vocab=vocab, pad_idx=pad_idx, \n",
    "                      processor=[FileTokenizeProcessor(tokenizer=validTokenizer, \n",
    "                                                       chunksize=chunksize, mark_fields=False)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 50s, sys: 7.87 s, total: 1min 58s\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%time trainIDS = trainTokenizer.getIds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.3 s, sys: 3.74 s, total: 39 s\n",
      "Wall time: 39.7 s\n"
     ]
    }
   ],
   "source": [
    "%time validIDS=validTokenizer.getIds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyTextLMDataBunch def create\n",
      "LanguageModelLoader.__init__ Used GB memory:9.35 batches:169556 nToks:406932154 bptt:400 p_bptt:0.0 shuffle:True backwards:False\n",
      "LanguageModelLoader.__init__ Used GB memory:9.35 batches:35863 nToks:86070425 bptt:400 p_bptt:0.0 shuffle:False backwards:False\n",
      "LanguageModelLoader.__init__ Used GB memory:9.38 batches:169556 nToks:406932154 bptt:400 p_bptt:0.0 shuffle:False backwards:False\n"
     ]
    }
   ],
   "source": [
    "from languagemodelloader import *\n",
    "nTrainToks, nValidToks = int(5e6),int(1e6)\n",
    "#nTrainToks, nValidToks = -1,-1\n",
    "if nTrainToks>0 and nValidToks>0:\n",
    "    trainIDS_ = trainIDS[0:nTrainToks]\n",
    "    validIDS_ = validIDS[0:nValidToks]\n",
    "else:\n",
    "    trainIDS_ = trainIDS\n",
    "    validIDS_ = validIDS\n",
    "\n",
    "dblm = MyTextLMDataBunch.from_ids( pathTrainValid, vocab, trainIDS_, validIDS_, bptt=400, p_bptt=0.0, bs=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.871686"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainIDS)/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464 - 4464 - 4464\n",
      "2 - 2 - 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dblm.train_dl.x.items)\n",
    "xc = dblm.train_dl.x.items\n",
    "r = xc[0]\n",
    "print(f\"{xc[0][0]} - {dblm.train_dl.x.items[0][0]} - {r[0]}\")\n",
    "dblm.train_dl.x.items[0][0]=2\n",
    "print(f\"{xc[0][0]} - {dblm.train_dl.x.items[0][0]} - {r[0]}\")\n",
    "xc[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LanguageModelLoader.__init__ Used GB memory:10.67 batches:181667 nToks:406932154 bptt:70 p_bptt:0.95 shuffle:False backwards:False\n",
      "181667\n",
      "LanguageModelLoader.allocate_buffers Used GB memory:10.67 shuffle:False backwards:False\n"
     ]
    }
   ],
   "source": [
    "#%load_ext line_profiler\n",
    "\n",
    "data = MyLanguageModelLoader(dblm.train_dl,bs=32)\n",
    "print(len(data))\n",
    "def getAllBatches(data,epochs=1):\n",
    "    from fastprogress import master_bar, progress_bar\n",
    "    mb = master_bar(range(epochs))\n",
    "    for i in mb:\n",
    "        for xb,yb in progress_bar(data, parent=mb):\n",
    "            continue\n",
    "            \n",
    "def getAllBatches2(data,epochs=1):\n",
    "    for i in range(epochs):\n",
    "        for xb,yb in data:\n",
    "            continue\n",
    "#%time getAllBatches(data)\n",
    "#%lprun -f MyLanguageModelLoader.fill_row getAllBatches2(data)\n",
    "#%lprun -f MyLanguageModelLoader.CircularIndex.__getitem__ getAllBatches2(data)\n",
    "%time getAllBatches2(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks   =  np.asarray([v for v in spt.vocab().itos])\n",
    "tokslen = np.asarray([len(v) for v in spt.vocab().itos])\n",
    "ix_sort = np.argsort(tokslen)\n",
    "#tokslen = tokslen[ix_sort]\n",
    "#toks    = toks[ix_sort]\n",
    "print(\"number og len(toks)=)1\", np.sum(tokslen==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charaters = \",\".join(toks[tokslen==1])\n",
    "#np.sum(toks=='a')\n",
    "charaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spt.vocab().numericalize([\"a\",\"b\",\"ab\",\"xxbos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spt.vocab().textify([56,299])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.asarray([65536],dtype=np.int16)\n",
    "np.asarray([65536/2-1],dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Oak Creek, Colorado\\n\\n\\nOak Creek is a Statutory Town in Routt County, Colorado, United States. The population was 849 at the 2000 census. It was incorporated in 1907 as a coal mining town. The community was named for scrub oak near the original town site.\\nOak Creek is located at (40.275049, -106.957607).\"\n",
    "text=\"[[Russian Revolution (1917)|Russian Revolution|Russian Revolution]]]\"\n",
    "\n",
    "text=\"in the historical canon), are the anarchist territories during the [[Spanish Revolution of 1936|Spanish Revolution]] and \"+\\\n",
    "     \"the [[Free Territory]] during the [[Russian Revolution (1917)|Russian Revolution]].Through the efforts \"+\\\n",
    "     \"and influence of the [[Anarchism in Spain|Spanish anarchists]] during the Spanish ,詹,誠,词,遇,邱,郵,釋,鎭,閑,\"\n",
    "text=text+text\n",
    "#text=[\"\\nin the historical canon),\", \"\\nare the anarchist territories during the \\n[[Spanish Revolution of 1936|Spanish Revolution]] and \",\n",
    "#      \"the [[Free Territory]] during the [[Russian Revolution (1917)|Russian Revolution]].\\nThrough the efforts \",\"and influence of the [[Anarchism in Spain|Spanish anarchists]] during the Spanish\\n\\n\\n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import fastai_sentencepiece\n",
    "\n",
    "def extract_link_title(t:str) -> str:\n",
    "    return re.sub('\\[\\[([^\\]^|\\[:]+)|([^\\]\\[:]+)\\]\\]', '\\g<1>', t).replace(']]','')\n",
    "    #return re.sub('\\[\\[(?:([^\\]\\[:]+))|([^\\]\\[:]+)\\]\\]', '\\g<1>', t).replace('[','')\n",
    "    #return re.sub('\\[\\[(?:([^\\]\\[:]+))|([^\\]\\[:]+)\\]\\]', '\\g<1>', t)\n",
    "def remove_first_empty_lines(t:str) -> str:\n",
    "    return re.sub('^\\n', '', t)\n",
    "\n",
    "def keep_western(t:str) -> str:\n",
    "    return ''.join(re.findall('([A-Za-z\\d_ -/:-@\\[-`{-~])',t))\n",
    "    #return  re.sub('\\', '', t)\n",
    "                   \n",
    "#\\p{IsLatin}|[! -/:-@\\[-`{-~]|[0-9]\n",
    "                         \n",
    "print(text)\n",
    "print(\"------------\") \n",
    "#for i,t in enumerate(text):\n",
    "#    print(extract_link_title(t))\n",
    "#    print(remove_first_empty_lines(t))\n",
    "#%timeit re.sub('\\[\\[([^\\]^|\\[:]+)|([^\\]\\[:]+)\\]\\]', '\\g<1>', text)\n",
    "print(keep_western(text))\n",
    "print(\"------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults.cpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang     = \"en\"\n",
    "pathData = Path(\"../nlp-data\")\n",
    "path     = pathData / lang\n",
    "pToks    = path/\"wiki-train_valid\"/\"toks\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#p = re.compile('([\\r\\n]+.?)+', r'\\r\\n')\n",
    "p = re.compile(\"[\\r\\n]+.?\")\n",
    "txt = \" 1\\n\\n\\n\\n\\n\"\n",
    "p.sub(\"\\n\",txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imax = int(1e8)\n",
    "def tuppel():\n",
    "    for i in range(imax): \n",
    "        r = (i,i+1)\n",
    "        l = r[1]-r[0]\n",
    "%timeit tuppel()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleParams():\n",
    "    for i in range(imax): \n",
    "        r0 = i\n",
    "        r1 = i+1\n",
    "        l  = r1-r0\n",
    "%timeit simpleParams()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang     = \"en\"\n",
    "pathData = Path(\"../nlp-data\")\n",
    "path     = pathData / lang\n",
    "pTrain   = path/\"wiki-train_valid\"/\"toks\"/\"train\"\n",
    "\n",
    "def loadAll(pTrain):\n",
    "    for p in pTrain.glob(\"*.npy\"):\n",
    "        with p.open(\"rb\") as f:\n",
    "            np.load(f)\n",
    "%time loadAll(pTrain)\n",
    "#s=None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "lang     = \"en\"\n",
    "pathData = Path(\"../nlp-data\")\n",
    "path     = pathData / lang\n",
    "p        = path/\"wiki-train_valid\"/\"toks\"/\"train\"/\"0-ids.npy\"\n",
    "    \n",
    "a=None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            import tracemalloc\n",
    "            tracemalloc.start()\n",
    "            snapshot1 = tracemalloc.take_snapshot()\n",
    "\n",
    "            -----your interesting code\n",
    "            \n",
    "            snapshot2 = tracemalloc.take_snapshot()\n",
    "            top_stats = snapshot2.compare_to(snapshot1, 'lineno')\n",
    "            print(f\"Top 10 of {len(top_stats)}\")\n",
    "            for stat in top_stats[:10]: print(stat)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = buffer[:bs*3].reshape(bs,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-1%9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=9\n",
    "for i in range(2*l):print(f\"i:{i} i-backwards:{l-1 - i%l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len=3\n",
    "a = np.arange(bs*seq_len,-1,-1)\n",
    "ix = np.arange(len(a))\n",
    "print(ix)\n",
    "print(a)\n",
    "ts[ix] = a[ix]\n",
    "print(t.shape)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.numel()/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=4\n",
    "sl=3\n",
    "bs_dim=t.numel()/sl\n",
    "t.view(-1,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "async def myfun():\n",
    "    await asyncio.sleep(1)\n",
    "    return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await myfun()\n",
    "await myfun()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def hello():\n",
    "    await asyncio.sleep(1)\n",
    "    return \"hello\"\n",
    "async def test():\n",
    "    loop = asyncio.get_event_loop()\n",
    "    tasks = []\n",
    "    for i in range(10):\n",
    "        tasks.append(loop.create_task(hello()))\n",
    "    # all the tasks will automatically run\n",
    "    asyncio.set_event_loop(loop)\n",
    "    rest = asyncio.gather(*tasks)\n",
    "    await rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#await test()\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "tasks = []\n",
    "for i in range(10):\n",
    "    tasks.append(loop.create_task(hello()))\n",
    "# all the tasks will automatically run\n",
    "asyncio.set_event_loop(loop)\n",
    "rest = asyncio.gather(*tasks)\n",
    "await rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "\n",
    "pool = ThreadPool(1) \n",
    "results = pool.map(test,None)\n",
    "pool.close() \n",
    "pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \"\"\"\n",
    "        async def hello():\n",
    "            await asyncio.sleep(1)\n",
    "            return \"hello\"\n",
    "    \n",
    "        fileblocks = partition_by_cores(files, n_cpus)\n",
    "        print(f\"fileblocks:{len(fileblocks)}\")\n",
    "        tasks = []\n",
    "        i=0\n",
    "        loop = asyncio.get_event_loop()\n",
    "        for fb in fileblocks:\n",
    "            print(f\"task:{i}\")\n",
    "            i += 1\n",
    "            results = tasks.append( loop.create_task( FileTokenizer.getIds_from_file(fb) ) )\n",
    "        asyncio.set_event_loop(loop)\n",
    "        results = asyncio.gather(*tasks)\n",
    "        await results\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from fastai.text import * \n",
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang            = \"en\"\n",
    "pathData       = Path(\"../nlp-data\")\n",
    "path           = pathData / lang\n",
    "pathTrainValid = path/\"wiki-train_valid\"\n",
    "pathNPY        = pathTrainValid/\"dummy.npy\"\n",
    "pathNPZ        = pathTrainValid/\"dummy.npz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays = np.empty(10, dtype=object)\n",
    "arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "a=np.arange(3,dtype=np.int32).astype(np.int32)\n",
    "b=np.arange(5,dtype=np.int32).astype(np.int32)\n",
    "c=np.arange(10,dtype=np.int32).astype(np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pathNPY.open(\"wb\") as f:\n",
    "    np.save(f,a, allow_pickle=False, fix_imports=False)\n",
    "    np.save(f,b, allow_pickle=False, fix_imports=False)\n",
    "    np.save(f,c, allow_pickle=False, fix_imports=False)\n",
    "\n",
    "with pathNPY.open(\"rb\") as f:\n",
    "    f.seek(0, 2); file_size = f.tell(); f.seek(0)\n",
    "    while f.tell() != file_size:\n",
    "        print(np.load(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays = np.empty(0, dtype=object)\n",
    "arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays = [] #np.empty(3, dtype=object)\n",
    "for i in range(3):\n",
    "    #for i,a in enumerate(arrays):\n",
    "    arrays.append( np.arange(i+1) )\n",
    "\n",
    "#arrays2 = np.empty(3, dtype=object)\n",
    "#arrays2[:3] = arrays[:3]\n",
    "#arrays[1][0] = 10\n",
    "print(arrays)\n",
    "\n",
    "def save(arrays):\n",
    "    with pathNPY.open(\"wb\") as f:\n",
    "        if isinstance(arrays,list): \n",
    "            arrays = np.asarray(arrays,dtype=object)\n",
    "        np.save(f,arrays, allow_pickle=True, fix_imports=False)\n",
    "        \n",
    "def load():\n",
    "    with pathNPY.open(\"rb\") as f:\n",
    "        arrays = np.load(f)\n",
    "    return arrays\n",
    "\n",
    "save(arrays)\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit save(arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit load()\n",
    "print(load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays = [] #np.empty(0, dtype=object)\n",
    "for i in range(3):\n",
    "    a = load()\n",
    "    if len(a) > 0: arrays.extend( a.tolist() )\n",
    "arrays = np.asarray(arrays,dtype=object)    \n",
    "print(len(arrays))\n",
    "print(arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
