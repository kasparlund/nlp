{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import * \n",
    "from fastai.text import * \n",
    "import numpy as np\n",
    "import io\n",
    "import pathlib\n",
    "import asyncio\n",
    "from fastai_sentencepiece import *\n",
    "from filetokenizer import *\n",
    "from languagemodelloader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang=\"en\"\n",
    "minToks = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pathData       = Path(\"../../data/nlp\")\n",
    "pathData       = Path(\"../nlp-data\")\n",
    "path           = pathData / lang\n",
    "pathDump       = path/\"wiki-dump\"\n",
    "pathJson       = path/\"wiki-json\"\n",
    "\n",
    "pathTrainValid = path/\"wiki-train_valid\"\n",
    "pathTxt        = pathTrainValid/\"txt\"\n",
    "pathToks       = pathTrainValid/\"toks\"\n",
    "pathcsv        = pathTrainValid/\"wiki.csv\"\n",
    "\n",
    "cache_name   = \"sp-model\"\n",
    "pathVocab    = pathTrainValid / cache_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.dtype:<class 'numpy.int16'>\n",
      "self.dtype:<class 'numpy.int16'>\n",
      "Tokenizer SentencepieceTokenizer in en with the following rules:\n",
      " - fix_html\n",
      " - replace_rep\n",
      " - replace_wrep\n",
      " - spec_add_spaces\n",
      " - rm_useless_spaces\n",
      " - rm_extra_lineshift\n",
      " - replace_all_caps\n",
      " - deal_caps\n",
      "\n",
      "size og vocabulary: 32000\n",
      "pad_idx: 2\n",
      "[0, 1, 0, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "#Discard section with kess than \"minTok\" tokens\n",
    "spt_func  = partial(SentencepieceTokenizer.create, pathVocab=pathVocab)\n",
    "spt_func.__name__ = SentencepieceTokenizer.__name__\n",
    "spt       = spt_func(lang=\"en\")\n",
    "pad_idx   = spt.vocab().numericalize([text.transform.PAD])[0]\n",
    "vocab,max_vocab = spt.vocab(), len(spt.vocab().itos)\n",
    "trainTokenizer = FileTokenizer(pathToks/\"train\", spt_func,\"en\",vocab,minToks=minToks,n_cpus=max(defaults.cpus-1,1))\n",
    "validTokenizer = FileTokenizer(pathToks/\"valid\", spt_func,\"en\",vocab,minToks=minToks,n_cpus=max(defaults.cpus-1,1))\n",
    "\n",
    "print(trainTokenizer)\n",
    "print(\"size og vocabulary:\", max_vocab)\n",
    "print(\"pad_idx:\",pad_idx)\n",
    "\n",
    "print(spt.vocab().numericalize( [\"xxunk\" ,\"xxbos\",\"xxeos\" ,\"xxpad\" ,\"xxmaj\" ,\"xxup\" ,\"xxrep\" ,\"xxwrep\", \"xxfld\"]  ))\n",
    "#sentence = [\"She is tall.\", \"He is small\"]\n",
    "#tokenizer._process_all_1(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files   = np.asarray( list(pathTxt.glob(\"*.txt\")) )\n",
    "nrows   = len(files)\n",
    "split   = 0.2\n",
    "splitindex, index = int(nrows*split+.5), np.random.permutation(np.arange(nrows)) \n",
    "\n",
    "chunksize=0\n",
    "\n",
    "trainList = TextList( files[:-splitindex], vocab=vocab, pad_idx=pad_idx, \n",
    "                      processor=[FileTokenizeProcessor(tokenizer=trainTokenizer, \n",
    "                                                       chunksize=chunksize, mark_fields=False)])\n",
    "\n",
    "validList = TextList( files[-splitindex:], vocab=vocab, pad_idx=pad_idx, \n",
    "                      processor=[FileTokenizeProcessor(tokenizer=validTokenizer, \n",
    "                                                       chunksize=chunksize, mark_fields=False)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 52s, sys: 8.81 s, total: 2min 1s\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%time trainIDS = trainTokenizer.getIds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.4 s, sys: 4.08 s, total: 39.4 s\n",
      "Wall time: 40.8 s\n"
     ]
    }
   ],
   "source": [
    "%time validIDS=validTokenizer.getIds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyTextLMDataBunch def create\n",
      "LanguageModelLoader.__init__ batches:17 nToks:1130 bptt:70 p_bptt:0.0 shuffle:True backwards:False\n",
      "LanguageModelLoader.__init__ batches:32 nToks:2187 bptt:70 p_bptt:0.0 shuffle:False backwards:False\n",
      "LanguageModelLoader.__init__ batches:17 nToks:1130 bptt:70 p_bptt:0.0 shuffle:False backwards:False\n"
     ]
    }
   ],
   "source": [
    "from languagemodelloader import *\n",
    "nTrainToks, nValidToks = int(1e1),int(1e1)\n",
    "#nTrainToks, nValidToks = -1,-1\n",
    "if nTrainToks>0 and nValidToks>0:\n",
    "    trainIDS_ = trainIDS[0:nTrainToks]\n",
    "    validIDS_ = validIDS[0:nValidToks]\n",
    "else:\n",
    "    trainIDS_ = trainIDS\n",
    "    validIDS_ = validIDS\n",
    "\n",
    "db = MyTextLMDataBunch.from_ids( pathTrainValid, vocab, trainIDS_, validIDS_, bptt=70, p_bptt=0.0, bs=1)\n",
    "#print(f\"len(data.train_dl):{len(data.train_dl)} len(data.train_dl.x.items):{len(data.train_dl.x.items)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128-49\n",
    "134-128+1+42\n",
    "71-42+1+50\n",
    "70-50+1+30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext line_profiler\n",
    "data = MyLanguageModelLoader(dataset=db.train_dl,bptt=70, p_bptt=0.0, bs=2)\n",
    "print(data)\n",
    "            \n",
    "def getAllBatches(data,epochs=1):\n",
    "    for i in range(epochs):\n",
    "        for xb,yb in data:\n",
    "            continue\n",
    "%time getAllBatches(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            import tracemalloc\n",
    "            tracemalloc.start()\n",
    "            snapshot1 = tracemalloc.take_snapshot()\n",
    "\n",
    "            -----your interesting code\n",
    "            \n",
    "            snapshot2 = tracemalloc.take_snapshot()\n",
    "            top_stats = snapshot2.compare_to(snapshot1, 'lineno')\n",
    "            print(f\"Top 10 of {len(top_stats)}\")\n",
    "            for stat in top_stats[:10]: print(stat)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totalTokens:396\n",
      "countTokens:396\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "[15 16 17 18]\n",
      "[19 20 21 22]\n",
      "[23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40]\n",
      "[41 42]\n",
      "[43 44 45 46]\n",
      "[47 48 49 50 51 52 53 54 55 56 57 58 59 60 61]\n",
      "[62 63 64 65 66 67 68 69 70 71 72]\n",
      "[73 74 75 76 77 78 79]\n",
      "[80 81 82 83 84 85 86 87 88 89 90 91 92 93 94]\n",
      "[ 95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111]\n",
      "[112 113 114 115 116 117 118 119 120]\n",
      "[121 122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "[134 135 136 137 138 139 140 141 142 143]\n",
      "[144 145 146 147 148 149 150 151 152 153 154 155]\n",
      "[156 157 158 159 160 161 162 163 164 165 166 167 168 169]\n",
      "[170 171 172 173 174 175 176 177]\n",
      "[178 179 180 181 182 183 184 185 186 187 188 189 190]\n",
      "[191 192 193 194 195 196 197 198 199 200 201 202]\n",
      "[203 204 205 206 207 208]\n",
      "[209 210 211 212 213 214 215 216 217 218 219 220]\n",
      "[221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239]\n",
      "[240 241 242 243 244 245 246 247 248 249 250 251 252]\n",
      "[253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270]\n",
      "[271 272 273]\n",
      "[274 275 276 277 278 279 280 281 282 283]\n",
      "[284 285 286 287]\n",
      "[288 289 290 291 292 293 294 295 296 297 298 299]\n",
      "[300 301 302 303 304 305 306]\n",
      "[307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324]\n",
      "[325 326 327 328 329 330 331 332 333 334]\n",
      "[335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353]\n",
      "[354 355 356 357 358 359 360 361 362 363 364]\n",
      "[365 366 367 368 369 370 371 372 373 374 375 376 377 378]\n",
      "[379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396]\n"
     ]
    }
   ],
   "source": [
    "#create test data sao that we can contro whether LanguageModelLoader returns contigous tokens \n",
    "jagged = []\n",
    "seq_len = 9\n",
    "bs = 4\n",
    "iterations=11\n",
    "\n",
    "countTokens = 0\n",
    "totalTokens = bs*seq_len*iterations\n",
    "print(f\"totalTokens:{totalTokens}\")\n",
    "rand_interval=20\n",
    "while countTokens < totalTokens:\n",
    "    nb = totalTokens-countTokens if   totalTokens-countTokens <rand_interval\\\n",
    "                                 else 1+int(np.random.random() * rand_interval)\n",
    "    #print(f\"nb:{nb}\")\n",
    "    jagged.append(np.arange(countTokens+1,countTokens+1+nb))\n",
    "    countTokens = jagged[-1][-1]\n",
    "jagged = np.asarray(jagged)    \n",
    "print(f\"countTokens:{countTokens}\")\n",
    "for j in jagged: print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyTextLMDataBunch def create\n",
      "LanguageModelLoader.__init__ iterations:11 rags:35 nToks:396 bptt:9 p_bptt:0.0 shuffle:True backwards:False\n",
      "LanguageModelLoader.__init__ iterations:11 rags:35 nToks:396 bptt:9 p_bptt:0.0 shuffle:False backwards:False\n",
      "LanguageModelLoader.__init__ iterations:11 rags:35 nToks:396 bptt:9 p_bptt:0.0 shuffle:False backwards:False\n",
      "LanguageModelLoader.__init__ iterations:11 rags:35 nToks:396 bptt:9 p_bptt:0.0 shuffle:False backwards:False\n",
      "LanguageModelLoader.allocate_buffers shuffle:False backwards:False self.ite_len:11\n",
      "   ei  eo  length\n",
      "0   0   0      14\n",
      "1  10  12      17\n",
      "2  18   4      12\n",
      "3  27   2      12\n",
      "len(self):11 Number of iteration:11\n",
      "\n",
      "\n",
      "self.ite_len:11 Number of iterations:11 countToks:440 self.totalToks:396 countToks < self.totalToks:False\n",
      "   ei  eo  length\n",
      "0  10   5      17\n",
      "1  19   3       6\n",
      "2  27   6      12\n",
      "3  34  10      18\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 7.76 ms, sys: 2.54 ms, total: 10.3 ms\n",
      "Wall time: 8.22 ms\n"
     ]
    }
   ],
   "source": [
    "# %load_ext line_profiler\n",
    "trainIDS_ = jagged\n",
    "validIDS_ = jagged\n",
    "db   = MyTextLMDataBunch.from_ids( pathTrainValid, vocab, trainIDS_, validIDS_, bptt=seq_len, p_bptt=0.0, bs=bs)\n",
    "data = MyLanguageModelLoader(dataset=db.train_dl, bptt=seq_len, p_bptt=0.0, bs=bs, shuffle=False)\n",
    "#print(data)\n",
    "            \n",
    "def getAllBatches(data,epochs=1):\n",
    "    x=None\n",
    "    for i in range(epochs):\n",
    "        for xb,yb in data:\n",
    "            if x is None: x = xb[:,0:seq_len].data.numpy()\n",
    "            else:         x = np.concatenate((x, xb[:,0:seq_len].data.numpy()),axis=1)\n",
    "            continue\n",
    "    return x            \n",
    "%time batches = getAllBatches(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The input was size to an batches that do not wrap aoround. Also the input contains continuously \n",
    "#increasing numbers. Therefore the diff below from one to the next column must be 1 for a properly aligned batches\n",
    "b_diff = batches[:,1:] - batches[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_diff.flatten().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
