{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "#from fastai import * \n",
    "from fastai.text import * \n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang            = \"en\"\n",
    "pathData       = Path(\"../nlp-data\")\n",
    "path           = pathData / lang\n",
    "pathTrainValid = path/\"wiki-train_valid\"\n",
    "spCache        = \"sp-model\"\n",
    "pathSPVocab    = pathTrainValid / spCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usedGB_RAM(): \n",
    "    import psutil\n",
    "    return round((psutil.virtual_memory().used + psutil.swap_memory().used)/1e9,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training and validation data we prepared in wiki_preparation.ipynb. In total 100 million articles with a split of 80% / 20% for training/validation:\n",
    "- First column: text content to train the model. \n",
    "- Second column: Boolean representing if the data is for training or validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tokenizer\n",
    "The sentencepiece vocabulary was trained in Train Sentencepiece tokenizer.ipynb.\n",
    "\n",
    "Here we will make a BasicTokenizer from Sentencepiece so that fastai can use it instead of spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencepieceTokenizer(BaseTokenizer):\n",
    "    def __init__(self, lang:str):\n",
    "        path,cache_name = pathTrainValid, \"sp-model\"\n",
    "    #def __init__(self, path:PathOrStr, cache_name:str='sp-model'):\n",
    "        self.pathVocab = path / cache_name\n",
    "        self.vocab_    = Vocab(pickle.load(open(self.pathVocab/'itos.pkl', 'rb')))\n",
    "        self.tok       = spm.SentencePieceProcessor()\n",
    "        \n",
    "        self.tok.Load(str(Path(path) / cache_name / 'm.model'))\n",
    "        text.transform.UNK = \"<unk>\"\n",
    "\n",
    "    #def __call__(self, language:str): return self    \n",
    "        \n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        return self.tok.EncodeAsPieces(t)\n",
    "    \n",
    "    def add_special_cases(self, toks:Collection[str]):\n",
    "        #this should have been done when training sentencepiece\n",
    "        pass\n",
    "    \n",
    "    def vocab(self): return self.vocab_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer SentencepieceTokenizer in en with the following rules:\n",
      " - fix_html\n",
      " - replace_rep\n",
      " - replace_wrep\n",
      " - spec_add_spaces\n",
      " - rm_useless_spaces\n",
      " - replace_all_caps\n",
      " - deal_caps\n",
      "\n",
      "size og vocabulary: 32000\n",
      "pad_idx: 2\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['▁she', '▁is', '▁tall', '.'], ['▁he', '▁is', '▁small']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spt       = SentencepieceTokenizer(lang=\"en\")\n",
    "tokenizer = Tokenizer(SentencepieceTokenizer,\"en\")\n",
    "pad_idx   = spt.vocab().numericalize([text.transform.PAD])[0]\n",
    "vocab,max_vocab  = spt.vocab(), len(spt.vocab().itos)\n",
    "\n",
    "print(tokenizer)\n",
    "print(\"size og vocabulary:\", max_vocab)\n",
    "print(\"pad_idx:\",pad_idx)\n",
    "\n",
    "print(spt.vocab().numericalize( [\"<unk>\" ,\"xxbos\" ,\"xxpad\" ,\"xxmaj\" ,\"xxup\" ,\"xxrep\" ,\"xxwrep\", \"xxfld\"]  ))\n",
    "sentence = [\"She is tall.\", \"He is small\"]\n",
    "tokenizer._process_all_1(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train two LM: one with a 60k vocabulary and one with a 30k vocabulary. The two models have different performance and computation needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discard section with kess than \"minTok\" tokens\n",
    "minToks=10"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#from ..torch_core import *\n",
    "\n",
    "def my_join_texts(texts:Collection[str], mark_fields:bool=False):\n",
    "    if not isinstance(texts, np.ndarray): texts = np.array(texts)\n",
    "    if is1d(texts): texts = texts[:,None]\n",
    "    df = pd.DataFrame({i:texts[:,i] for i in range(texts.shape[1])})\n",
    "    #text_col = f'{BOS} {FLD} {1} ' + df[0] if mark_fields else  f'{BOS} ' + df[0]\n",
    "    text_col = df[0]\n",
    "    print(df.shape)\n",
    "    for i in range(1,len(df.columns)):\n",
    "        #text_col += (f' {FLD} {i+1} ' if mark_fields else ' ') + df[i]\n",
    "        text_col += df[i]\n",
    "    return text_col.values\n",
    "\n",
    "#text.data._join_texts = my_join_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0.  1.  2.  3.1]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= [0,1,2,3.1]\n",
    "json.dumps(a)\n",
    "str(np.asarray(a))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import io\n",
    "a=np.arange(int(1e9))\n",
    "#f = io.StringIO()\n",
    "f = (pathTrainValid/\"dummy.txt\").open(\"w\")\n",
    "%timeit f.write(str(a))\n",
    "f.close()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from multiprocessing.dummy import Pool as ThreadPool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import asyncio\n",
    "class FileTokenizer():\n",
    "    \"Put together rules and a tokenizer function to tokenize text with multiprocessing.\"\n",
    "    def __init__(self, tokPath:Path, tok_func:Callable, lang:str, vocab:Vocab=vocab, pre_rules:ListRules=None,\n",
    "                 post_rules:ListRules=None, special_cases:Collection[str]=None, n_cpus:int=None):\n",
    "        self.tok_func,self.lang,self.special_cases = tok_func,lang,special_cases\n",
    "        self.pre_rules  = ifnone(pre_rules,  defaults.text_pre_rules )\n",
    "        self.post_rules = ifnone(post_rules, defaults.text_post_rules )\n",
    "        self.special_cases = special_cases if special_cases else defaults.text_spec_tok\n",
    "        self.n_cpus = ifnone(n_cpus, defaults.cpus)\n",
    "        self.vocab  = vocab\n",
    "        self.tokPath = tokPath\n",
    "        \n",
    "        self.count=0\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        res = f'Tokenizer {self.tok_func.__name__} in {self.lang} with the following rules:\\n'\n",
    "        for rule in self.pre_rules: res += f' - {rule.__name__}\\n'\n",
    "        for rule in self.post_rules: res += f' - {rule.__name__}\\n'\n",
    "        return res\n",
    "\n",
    "    def process_text(self, t:str, tok:BaseTokenizer) -> List[str]:\n",
    "        \"Process one text `t` with tokenizer `tok`.\"\n",
    "        inPath = Path(t)\n",
    "        if not inPath.exists(): \n",
    "            print(f\"file does not exist{str(inPath)}\")\n",
    "            return \"\"\n",
    "        \n",
    "        pathIds = self.tokPath/(inPath.stem+\"-ids.npy\")\n",
    "        pathIds.parent.mkdir(parents=True,exist_ok=True)\n",
    "        \n",
    "        arrays = []\n",
    "        with inPath.open(\"r\") as f:\n",
    "            for line in f:\n",
    "                for rule in self.pre_rules: line = rule(line)\n",
    "                toks = tok.tokenizer(line)\n",
    "                for rule in self.post_rules: toks = rule(toks)\n",
    "                ids = vocab.numericalize(toks) \n",
    "                \n",
    "                if len(toks) < minToks: continue\n",
    "                arrays.append( np.asarray(ids, dtype=np.int16) )\n",
    "        \n",
    "        if len(arrays)>0:\n",
    "            with pathIds.open(\"wb\") as f:\n",
    "                np.save(f, arrays, allow_pickle=True, fix_imports=False)\n",
    "                \n",
    "        return t\n",
    "\n",
    "    def _process_all_1(self, texts:Collection[str]) -> List[List[str]]:\n",
    "        \"Process a list of `texts` in one process.\"\n",
    "        tok = self.tok_func(self.lang)\n",
    "        if self.special_cases: tok.add_special_cases(self.special_cases)\n",
    "        return [self.process_text(t, tok) for t in texts]\n",
    "\n",
    "    def process_all(self, texts:Collection[str]) -> List[List[str]]:\n",
    "        \"Process a list of `texts`.\"\n",
    "        print(\"FileTokenizer process_all\")\n",
    "        \n",
    "        if self.n_cpus <= 1: return self._process_all_1(texts)\n",
    "        print(f\"cpus: {self.n_cpus} number of files:{len(texts)}\")\n",
    "        with ProcessPoolExecutor(self.n_cpus) as e:\n",
    "            #return e.map(self._process_all_1, partition_by_cores(texts, self.n_cpus))\n",
    "            return sum(e.map(self._process_all_1, partition_by_cores(texts, self.n_cpus)), [])\n",
    "        \n",
    "    @staticmethod\n",
    "    def getIds_from_file(files):\n",
    "        idArrays=[]\n",
    "        for fp in files:\n",
    "            with fp.open(\"rb\") as f:\n",
    "                a = np.load(f)\n",
    "                if len(a) > 0: idArrays.extend( a )\n",
    "        return idArrays \n",
    "    \n",
    "    def getIds(self, n_cpus=defaults.cpus):\n",
    "        #threading does not help on speed in this case :(\n",
    "        files = list(self.tokPath.glob(\"*-ids.npy\"))\n",
    "        \n",
    "        #3use_cores = max(1,defaults.cpus)\n",
    "        print(f\"threading with on {n_cpus} cores\")\n",
    "        \n",
    "        pool = ThreadPool(n_cpus) \n",
    "        results = pool.map(FileTokenizer.getIds_from_file, partition_by_cores(files, n_cpus))\n",
    "        pool.close() \n",
    "        pool.join()\n",
    "        \n",
    "        idArrays=[]\n",
    "        for a in results:idArrays.extend(a)\n",
    "        idArrays = np.asarray(idArrays,dtype=object)    \n",
    "        return idArrays \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileTokenizeProcessor(PreProcessor):\n",
    "    \"`PreProcessor` that tokenizes the texts in `ds`.\"\n",
    "    def __init__(self, ds:ItemList=None, tokenizer:Tokenizer=None, chunksize:int=10000, mark_fields:bool=False):\n",
    "        self.tokenizer,self.chunksize,self.mark_fields = ifnone(tokenizer, Tokenizer()),chunksize,mark_fields\n",
    "\n",
    "    def process_one(self, item):  return self.tokenizer._process_all_1([item])[0]\n",
    "    def process(self, ds):\n",
    "        print(\"FileTokenizeProcessor process\")\n",
    "        #ds.items = _join_texts(ds.items, self.mark_fields)\n",
    "        self.tokenizer.process_all(ds.items)\n",
    "        #ds.items = self.tokenizer.process_all(ds.items)\n",
    "        #ds.items = tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer SentencepieceTokenizer in en with the following rules:\n",
      " - fix_html\n",
      " - replace_rep\n",
      " - replace_wrep\n",
      " - spec_add_spaces\n",
      " - rm_useless_spaces\n",
      " - replace_all_caps\n",
      " - deal_caps\n",
      "\n",
      "size og vocabulary: 32000\n",
      "pad_idx: 2\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "pad_idx   = spt.vocab().numericalize([text.transform.PAD])[0]\n",
    "vocab,max_vocab = spt.vocab(), len(spt.vocab().itos)\n",
    "trainTokenizer = FileTokenizer(pathTrainValid/\"toks/train\", SentencepieceTokenizer,\"en\",vocab)\n",
    "validTokenizer = FileTokenizer(pathTrainValid/\"toks/valid\", SentencepieceTokenizer,\"en\",vocab)\n",
    "\n",
    "print(trainTokenizer)\n",
    "print(\"size og vocabulary:\", max_vocab)\n",
    "print(\"pad_idx:\",pad_idx)\n",
    "\n",
    "print(spt.vocab().numericalize( [\"<unk>\" ,\"xxbos\" ,\"xxpad\" ,\"xxmaj\" ,\"xxup\" ,\"xxrep\" ,\"xxwrep\", \"xxfld\"]  ))\n",
    "#sentence = [\"She is tall.\", \"He is small\"]\n",
    "#tokenizer._process_all_1(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import timeit start = timeit.default_timer()\n",
    "pathTxt = pathTrainValid/\"txt\"\n",
    "files   = np.asarray( list(pathTxt.glob(\"*.txt\")) )\n",
    "nrows   = len(files)\n",
    "split   = 0.2\n",
    "splitindex, index = int(nrows*split+.5), np.random.permutation(np.arange(nrows)) \n",
    "\n",
    "chunksize=0\n",
    "\n",
    "trainList = TextList( files[:-splitindex], vocab=vocab, pad_idx=pad_idx, \n",
    "                     processor=[FileTokenizeProcessor(tokenizer=trainTokenizer, chunksize=chunksize, mark_fields=False)] )\n",
    "validList = TextList( files[-splitindex:], vocab=vocab, pad_idx=pad_idx, \n",
    "                     processor=[FileTokenizeProcessor(tokenizer=validTokenizer, chunksize=chunksize, mark_fields=False)] )\n",
    "#print(timeit.default_timer()-start )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%time p = trainList.process()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%time p = validList.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 nb: used GB memory: 3.51\n",
      "threading with on 2 cores\n",
      "CPU times: user 2min 26s, sys: 16.7 s, total: 2min 43s\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "print(\"1 nb: used GB memory:\", usedGB_RAM()) \n",
    "%time trainIDS = trainTokenizer.getIds(n_cpus=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threading with on 1 cores\n",
      "CPU times: user 35.4 s, sys: 3.46 s, total: 38.8 s\n",
      "Wall time: 39.4 s\n",
      "2 nb: used GB memory: 7.96\n"
     ]
    }
   ],
   "source": [
    "%time validIDS = validTokenizer.getIds(n_cpus=1)\n",
    "print(\"2 nb: used GB memory:\", usedGB_RAM()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning:    number of arrays:25527730 - number of ids:2046492393\n",
      "Validation: number of arrays:8504038 - number of ids:733295647\n"
     ]
    }
   ],
   "source": [
    "print(f\"Traning:    number of arrays:{len(trainIDS)} - number of ids:{np.sum([len(ids) for ids in trainIDS])}\")\n",
    "print(f\"Validation: number of arrays:{len(validIDS)} - number of ids:{np.sum([len(ids) for ids in validIDS])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min:1 max:12985 - median:63.0\n",
      "1521907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([134,  71,  70,  16,  35,  39, 269, 261, 146,  89,  39,  43,  42,  20,  33, 132,  87,  67,  12,  36], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"Analyse the distribution of the legnth of tokens sequences in the ragged/jagged array of tokes\n",
    "sectionlengths   = np.asarray([len(s) for s in trainIDS],dtype=np.int32)\n",
    "plt.hist(sectionlengths[sectionlengths<1000], 100, density=True, facecolor='g', alpha=0.75)\n",
    "np.histogram(sectionlengths[sectionlengths<1000],50)\n",
    "\n",
    "print(f\"Lenght of token rags min:{min(sectionlengths)} max:{np.max(sectionlengths)} - median:{np.median(sectionlengths)}\")\n",
    "print(np.sum(sectionlengths<10))\n",
    "sectionlengths[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLanguageModelLoader():\n",
    "    \"Create a dataloader with bptt slightly changing.\"\n",
    "    \n",
    "    class CircularIndex():\n",
    "        def __init__(self, length:int): \n",
    "            self.idx = np.arange(length)\n",
    "            self.forward_ = True\n",
    "        #when the index exceeds the length of self.idx then is is wrap to start indexing at the head\n",
    "        #if the index is backwards then start at the end and wraps if necessary at head to continue\n",
    "        def __getitem__(self, i): \n",
    "            index = i%len(self.idx) if self.forward_ else len(self.idx)-1 - i%len(self.idx)\n",
    "            return self.idx[index]\n",
    "        def __len__(self) -> int: return len(self.idx)\n",
    "        def shuffle(self): np.random.shuffle(self.idx)\n",
    "        def forward(self, forward:bool=True): self.forward_ = forward\n",
    "                \n",
    "    def __init__(self, dataset:LabelList, bs:int=64, bptt:int=70, backwards:bool=False, shuffle:bool=False,\n",
    "                 max_len:int=25, p_bptt:int=0.95):\n",
    "        self.init_kwargs = dict(bs=bs, bptt=bptt, backwards=backwards, shuffle=shuffle, max_seq=max_len)\n",
    "        self.dataset,self.bs,self.bptt,self.backwards,self.shuffle,self.p_bptt = dataset,bs,bptt,backwards,shuffle,p_bptt\n",
    "        \n",
    "        #self.idx simplifies calculation of self.ite_len and the batch loop were seq_len varies uniformly\n",
    "        #idx is used to index the raggged dataset. \n",
    "        #It can be shuffle in-place \n",
    "        #The indexing is circular in order to stay within self.dataset.x.items\n",
    "        self.idx = MyLanguageModelLoader.CircularIndex(len(self.dataset))\n",
    "        self.idx.forward( self.backwards==False ) \n",
    "        \n",
    "        nToks = 0\n",
    "        for s in dataset.x.items: nToks+=len(s)\n",
    "        self.ite_len = math.ceil( nToks / (self.bs*self.bptt) ) #this is returned in def __len__(self) \n",
    "        \n",
    "        #The first batch has length = 2*self.bptt as in the original code\n",
    "        self.first_seq_len = 2*self.bptt\n",
    "        #The following batches vary uniformly around bppt as defined by p_bptt\n",
    "        seq_other = math.ceil( self.bptt*(1+0.5*self.p_bptt) )\n",
    "        \n",
    "        #allocate the required worth-case batchbuffer \n",
    "        max_batch_element = self.bs*( max(self.first_seq_len, seq_other) +1 )\n",
    "        self.buffer       = np.zeros(max_batch_element, dtype=np.long)\n",
    "        \n",
    "        #self.min_seq,self.max_seq = 5,max_len #self.min_seq, self.max_seq is no longer used\n",
    "        self.num_workers = 0\n",
    "\n",
    "        print(f\"LanguageModelLoader.__init__ Used GB memory:{usedGB_RAM()} batches:{len(self)} nToks:{nToks} \"+\\\n",
    "              f\"bptt:{self.bptt} p_bptt:{self.p_bptt} shuffle:{self.shuffle} backwards:{self.backwards}\" )  \n",
    "        \n",
    "    def __iter__(self):\n",
    "        if getattr(self.dataset, 'item', None) is not None: \n",
    "            yield LongTensor(getattr(self.dataset, 'item'))[None],LongTensor([0])\n",
    "        if self.shuffle: self.idx.shuffle()\n",
    "\n",
    "        i,self.ei,self.eo = 0,0,1\n",
    "        while i < self.ite_len:\n",
    "            seq_len = self.first_seq_len if i==0 else int(self.bptt*(1. + self.p_bptt*(np.random.random() - 0.5)))\n",
    "            nToks   = self.bs*(seq_len+1)\n",
    "            \n",
    "            self.fill_buffer(nToks)\n",
    "            data  = torch.from_numpy( self.buffer[:nToks].reshape(self.bs,-1) )\n",
    "            res   = data[:,0:seq_len-1], data[:,1:seq_len]        \n",
    "            i    += 1\n",
    "            #if i==self.ite_len : print(res) # check that y is shift to predict x       \n",
    "            yield res\n",
    "            \n",
    "    \n",
    "    def fill_buffer(self, nToks:int):\n",
    "        \"new the tokens in the buffer with nToks from the ragged array\"\n",
    "        #nToks: number of tokens to be extract and inserted starting at the beginning of the buffer from the \n",
    "        #       last saved indices in the ragged array and forward - possibly wrapping to the head of the dataset\n",
    "        #bi: index of the first rag to be extract\n",
    "        #bo: index where the extraction starts in the first rag\n",
    "        #ei: index of the last rag to be extract\n",
    "        #bo: index (not inclusive) where the extract stops in the last rag\n",
    "        \n",
    "        j, bi, bo, csum, ibuf = self.ei, self.ei, self.eo-1, 0, 0\n",
    "        while nToks > csum:   \n",
    "            rag   = self.dataset.x.items[self.idx[j]]\n",
    "            rl    = (len(rag)-bo) if j==bi else len(rag) \n",
    "            csum += rl        \n",
    "            if nToks <= csum: \n",
    "                self.ei = j\n",
    "                self.eo = ( nToks-(csum-rl) ) if j>bi else (nToks-(csum-rl) + bo)\n",
    "                r       = (bo,self.eo) if bi==self.ei else (0,self.eo)\n",
    "            else: \n",
    "                r       = (bo,len(rag)) if j==self.ei else (0,len(rag))            \n",
    "            self.buffer[ibuf:ibuf+r[1]-r[0]] = rag[r[0]:r[1]]\n",
    "            ibuf += r[1]-r[0]\n",
    "            j    += 1      \n",
    "        #print( f\"nToks:{nToks} nBToks:{ibuf} bi:{bi} bo:{bo} ei:{self.ei} eo:{self.eo}\" )        \n",
    "\n",
    "    def __len__(self) -> int: return self.ite_len\n",
    "    def __getattr__(self,k:str)->Any: return getattr(self.dataset, k)\n",
    "\n",
    "    @property\n",
    "    def batch_size(self): return self.bs\n",
    "    @batch_size.setter\n",
    "    def batch_size(self, v): self.bs = v\n",
    "\n",
    "    def batchify(self, data:np.ndarray) -> LongTensor: pass\n",
    "\n",
    "class MyTextLMDataBunch(TextLMDataBunch):\n",
    "    \"Create a `TextDataBunch` suitable for training a language model.\"\n",
    "    @classmethod\n",
    "    def from_ids(cls, path:PathOrStr, vocab:Vocab, \n",
    "                 train_ids:Collection[Collection[int]],        valid_ids:Collection[Collection[int]],\n",
    "                 test_ids:Collection[Collection[int]]=None, \n",
    "                 train_lbls:Collection[Union[int,float]]=None, valid_lbls:Collection[Union[int,float]]=None, \n",
    "                 classes:Collection[Any]=None, processor:PreProcessor=None, **kwargs) -> DataBunch:\n",
    "        \"Create a `TextDataBunch` from ids, labels and a `vocab`.\"\n",
    "        src = LabelLists(path, TextList(train_ids, vocab, path=path, processor=[]),\n",
    "                               TextList(valid_ids, vocab, path=path, processor=[]))\n",
    "        #src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_lists(train_lbls, valid_lbls, classes=classes, processor=[]) \n",
    "        src.train = src.train.label_for_lm()\n",
    "        src.valid = src.valid.label_for_lm()\n",
    "\n",
    "        #if test_ids is not None: src.add_test(TextList(test_ids, vocab, path=path), label=train_lbls[0])\n",
    "        #src.valid.x.processor = ifnone(processor, [TokenizeProcessor(), NumericalizeProcessor(vocab=vocab)])\n",
    "       \n",
    "        #ensure our create is called\n",
    "        src.train.x._bunch = MyTextLMDataBunch\n",
    "        src.valid.x._bunch = MyTextLMDataBunch\n",
    "        return src.databunch(**kwargs)\n",
    "    \n",
    "    #need customized version of this in order to set MyLanguageModelLoader\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', no_check:bool=False, **kwargs) -> DataBunch:\n",
    "        \"Create a `TextDataBunch` in `path` from the `datasets` for language modelling.\"\n",
    "        print(\"MyTextLMDataBunch def create\")\n",
    "        datasets    = cls._init_ds(train_ds, valid_ds, test_ds)\n",
    "        dataloaders = [MyLanguageModelLoader(ds, shuffle=(i==0), **kwargs) for i,ds in enumerate(datasets)]\n",
    "        return cls(*dataloaders, path=path, no_check=no_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyTextLMDataBunch def create\n",
      "LanguageModelLoader.__init__ Used GB memory:10.67 batches:29 nToks:252772 bptt:70 p_bptt:0.95 shuffle:True backwards:False\n",
      "LanguageModelLoader.__init__ Used GB memory:10.67 batches:9 nToks:73497 bptt:70 p_bptt:0.95 shuffle:False backwards:False\n",
      "LanguageModelLoader.__init__ Used GB memory:10.67 batches:29 nToks:252772 bptt:70 p_bptt:0.95 shuffle:False backwards:False\n"
     ]
    }
   ],
   "source": [
    "#i have an issue with passing pad_idx\n",
    "dblm = MyTextLMDataBunch.from_ids( pathTrainValid, vocab, trainIDS[0:4000], validIDS[0:800], bs=128)\n",
    "#dblm = MyTextLMDataBunch.from_ids( pathTrainValid, vocab, trainIDS[0:1000], validIDS[0:200], bs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dblm.train_ds.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>  <col width='5%'>  <col width='95%'>  <tr>\n",
       "    <th>idx</th>\n",
       "    <th>text</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0</th>\n",
       "    <th><unk> ▁lost ▁both ▁his ▁legs ▁in ▁1971. xxup ▁i ga ▁ ś wi ą t ek ▁won ▁the ▁title , ▁defeating <unk> ie xxup ▁k ün g ▁in ▁the ▁final , ▁6–4 , ▁6–2 . <unk> hard <unk> le ben ▁of ▁the xxup ▁i s k ▁travelled ▁to <unk> ▁in ▁1928, ▁seeking ▁support ▁for ▁the ▁group ' s ▁ideas . ▁he ▁recruited ▁three ▁members : <unk> <unk> , <unk> ▁green</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>▁series ▁within ▁the <unk> ma ▁brand ▁and ▁sold ▁alongside ▁the ▁similarly ▁size d <unk> ma xxup ▁m 6. ▁originally ▁launched ▁as ▁the ▁third ▁generation <unk> ma ▁family ▁se dan , ▁the ▁model ▁was ▁later ▁renamed ▁to <unk> ma xxup ▁f ami lia xxup ▁m 5, ▁replacing ▁the <unk> ma ▁3 ▁or <unk> ma ▁family ▁compact ▁cars . ▁de but ing ▁in ▁late ▁2013 ▁and ▁officially ▁launched ▁in ▁may ▁2014, ▁the</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>▁institution ▁of ▁highway s ▁and ▁transportation . ▁she ▁was ▁director ▁of ▁as set ▁management ▁at ▁transport ▁for <unk> . ▁par don ▁died ▁on ▁12 <unk> ▁1910 ▁aboard ▁the ▁\" o xu s \", ▁on ▁which ▁he ▁had ▁embarked ▁in <unk> . ▁during ▁the <unk> – xxmaj <unk> ▁war ▁in <unk> ▁1901 , <unk> ▁was ▁injured ▁in ▁a ▁battle ▁in ▁the <unk> <unk> ▁municipality . ▁although ▁he ▁got ▁shot ▁through ▁the</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>ification ▁of ▁with dra wal ) ▁act ▁2017 ▁upon ▁the ▁result ▁of ▁the ▁public ▁vote ▁and ▁requires ▁the ▁prime ▁minister ▁to ▁not ify ▁the xxup ▁eu ▁of ▁the xxup <unk> ' s ▁re traction ▁of ▁its ▁article ▁50 ▁notice ▁as ▁follows : ▁on <unk> ▁20, ▁2018, <unk> ▁police ▁raid ed ▁the ▁home ▁of ▁the ▁board ▁members ▁of ▁the <unk> ▁non - profit xxup ▁z wi ebel fre und e ,</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>, ▁the ▁hotel ▁was ▁a ▁three - story ▁brick ▁and ▁s tu cco ▁building ▁in <unk> ▁revival ▁style . ▁an ▁extensive ▁re model ing ▁happened ▁in ▁1928, ▁with <unk> xxup ▁t . ▁no lan ▁supervision , ▁which ▁added ▁a ▁fourth <unk> ▁style d ▁story ▁to ▁the ▁building , ▁including ▁an ▁und ulated ▁roof ▁and ▁a ▁corner ▁ob eli sk . ▁the ▁1928 ▁re model ing ▁also ▁added ▁a ▁three -</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dblm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 nb: used GB memory: 10.66\n",
      "CPU times: user 469 ms, sys: 48.2 ms, total: 517 ms\n",
      "Wall time: 535 ms\n",
      "2 nb: used GB memory: 10.66\n"
     ]
    }
   ],
   "source": [
    "print(\"1 nb: used GB memory:\", usedGB_RAM()) \n",
    "%time learn = language_model_learner(dblm, drop_mult=0, qrnn=False, pad_token=-1, callback_fns=ShowGraph)\n",
    "print(\"2 nb: used GB memory:\", usedGB_RAM()) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%time learn.lr_find()\n",
    "learn.recorder.plot(skip_start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 nb: gc.collect:7435 - used GB memory:10.63\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 1:22:45 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>9.112278</th>\n",
       "    <th>7.478217</th>\n",
       "    <th>0.058400</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>7.717758</th>\n",
       "    <th>7.057650</th>\n",
       "    <th>0.051947</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>7.199347</th>\n",
       "    <th>6.861418</th>\n",
       "    <th>0.084079</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>6.848588</th>\n",
       "    <th>6.628748</th>\n",
       "    <th>0.103896</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>6.544610</th>\n",
       "    <th>6.420011</th>\n",
       "    <th>0.124220</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>6</th>\n",
       "    <th>6.275595</th>\n",
       "    <th>6.329161</th>\n",
       "    <th>0.136577</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>7</th>\n",
       "    <th>6.051308</th>\n",
       "    <th>6.228448</th>\n",
       "    <th>0.142387</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>8</th>\n",
       "    <th>5.878789</th>\n",
       "    <th>6.197314</th>\n",
       "    <th>0.145556</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>9</th>\n",
       "    <th>5.748291</th>\n",
       "    <th>6.213835</th>\n",
       "    <th>0.145112</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>10</th>\n",
       "    <th>5.658380</th>\n",
       "    <th>6.169898</th>\n",
       "    <th>0.146772</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl0XPV99/H3bxbNaLTMSLIka7Et2RjiTTG2cAhQQ0JIAiSQxU/iAA1Nc0LbNCXQNgk9SZ6SnvYk6dMFaBJak9KTtASSmgBNS2gCxRiehyU2izfZ2LJlW5Kt1drXkX7PH3e0GUmWNCONdPV5nTPn3rlzZ+b788if+c3vbsZai4iILHyeZBcgIiKJoUAXEXEJBbqIiEso0EVEXEKBLiLiEgp0ERGXUKCLiLiEAl1ExCUU6CIiLuGbyzfzhsLWF86b+HFj8HkNfq+HoN9LWsBLZtA/hxWKiMw/e/fubbTW5l5ovTkN9LUXr+LR/3qejt4onb3R2HRgeP5cVx/1bb1Ut3RxtK6D3uggeQWZPPCZS7koL30uSxURmTeMMSenst6cBnrA52F9UXhK6/ZGB/jvg3X8xS8OcvvDr/HEH15BXkZwlisUEVm45u0YesDn5aZ3F/Ivv7OF5s4+vvSTNxgY1InEREQmMm8DfciG4jB/9fH1vHaime8/fyzZ5YiIzFsXHHIxxjwMfASot9aujy3LBn4KlABVwKestedmq8hPbCrmxaON3Pfs21yxKofykuzZeisRmWf6+/uprq6mp6cn2aXMumAwSHFxMX7/zHYGMRc6H7oxZivQAfx4VKD/NdBsrf2OMeYeIMta+7ULvVl5ebnds2fPjApt7+nnI//wEtEBy9N3/hbhkPZ+EVkMTpw4QUZGBjk5ORhjkl3OrLHW0tTURHt7O6WlpWMeM8bstdaWX+g1LjjkYq3dDTSft/hm4Eex+R8BH5tayTOXEfTzwPZLqWvr4Z6f70MX5hBZHHp6elwf5gDGGHJycuL6JTLTMfR8a+0ZgNh0wp3LjTF3GGP2GGP2NDQ0zPDtHO9eFuGrH76EXx44y47dx+N6LRFZONwe5kPibeesbxS11u6w1pZba8tzcy+4X/wFfeG3VnJjWQHffeYwL7wd3xeEiIibzDTQ64wxBQCxaX3iSpqcMYb/s62MS5Zm8kc/eZ0TjZ1z9dYisgi1tLTwgx/8YNrPu+GGG2hpaZmFiiY200D/D+D22PztwFOJKWdqQik+dvz2Zrwewxd+vIf2nv65fHsRWUQmCvSBgYFJn/f0008TiURmq6xxXTDQjTGPAi8Dlxhjqo0xnwe+A1xnjDkKXBe7P6eWZYf4/q2bONHYyd0/fYtBHXQkIrPgnnvuobKyko0bN3LZZZfxvve9j1tuuYUNGzYA8LGPfYzNmzezbt06duzYMfy8kpISGhsbqaqqYs2aNXzhC19g3bp1fPCDH6S7u3tWar3gfujW2s9M8NC1Ca5l2q5YtYRv3riGe39xiPueO8ofX3dxsksSkVn0rV8c5FBtW0Jfc21hJn/+0XUTPv6d73yHAwcO8Oabb7Jr1y5uvPFGDhw4MLxr4cMPP0x2djbd3d1cdtllfPKTnyQnJ2fMaxw9epRHH32Uhx56iE996lM8/vjj3HbbbQltB8zxuVxmw+1XlHCwto0HnjvKmqUZXL+hINkliYiLbdmyZcx+4g888ABPPPEEAKdPn+bo0aPvCPTS0lI2btwIwObNm6mqqpqV2hZ8oBtj+MuPr+dofQdf2bmPtYWZrMhJS3ZZIjILJutJz5W0tJF82bVrF88++ywvv/wyoVCIa665Ztz9yAOBwPC81+udtSGXeX8ul6kI+Lx875ZL8Ri489E36IsOJrskEXGJjIwM2tvbx32stbWVrKwsQqEQhw8f5pVXXpnj6sZyRaADFGeF+OttZbxV3crf/upIsssREZfIycnhyiuvZP369XzlK18Z89iHP/xhotEoZWVlfPOb3+Tyyy9PUpWOC57LJZHiOZfLVH39if088uop/u3z7+Gq1Utm9b1EZPZVVFSwZs2aZJcxZ8Zrb8LO5bLQfOPGtazKTeNP//0tWrr6kl2OiMiccV2gp6Z4uX/7pTR29PKNJw/oJF4ismi4LtAB1heFufu6i/nPfWd46s3aZJcjIjInXBnoAL9/9SrKV2TxzScPUH2uK9nliIjMOtcGutdj+PtPb2TQWv74Z2/peqQi4nquDXRwzvfyFzc71yN9cJeuRyoi7ubqQAf4xKYibnp3IX//7FFePzVrlz0VEQEgPT0dgNraWrZt2zbuOtdccw2zsQu36wN96NQABeEgX37sDZ1qV0TmRGFhITt37pzT93R9oANkBv3c9+mN1Jzr5s+fOpjsckRkAfna17425nzo9957L9/61re49tpr2bRpExs2bOCpp955SYiqqirWr18PQHd3N9u3b6esrIxPf/rTyTt9rluUl2Rz57Wrue/Zo2y9OJePXVqU7JJEZLp+eQ+c3Z/Y11y6Aa6f+JIO27dv56677uKLX/wiAD/72c945plnuPvuu8nMzKSxsZHLL7+cm266acJrgj744IOEQiH27dvHvn372LRpU2LbELNoAh3gS++7iJeONvKNJw+waXkWy3NCyS5JROa5Sy+9lPr6empra2loaCArK4uCggLuvvtudu/ejcfjoaamhrq6OpYuXTrua+zevZs777wTgLKyMsrKymal1kUV6D6vh/u2b+T6+1/kyz99g3//vffi8y6KUScRd5ikJz2btm3bxs6dOzl79izbt2/nkUceoaGhgb179+L3+ykpKRn3tLmjTdR7T6RFl2bFWSH+6uMbeONUC9995nCyyxGRBWD79u089thj7Ny5k23bttHa2kpeXh5+v5/nn3+ekydPTvr8rVu38sgjjwBw4MAB9u3bNyt1Lqoe+pCb3l3Ib04089CLJ1hXGNZ4uohMat26dbS3t1NUVERBQQG33norH/3oRykvL2fjxo28613vmvT5f/AHf8DnPvc5ysrK2LhxI1u2bJmVOl13+typ6h8Y5NYfvspbp1vY+ftXsKE4nOySRGQcOn3uIj597lT5vR5+cOsmctJSuONf99DQ3pvskkRE4rJoAx1gSXqAHZ8t51xXH198ZK8uXSciC9qiDnRwTrX73U+W8Zuqc9z7i4M6f7rIPLRY/l/G285FH+gAN28s4vevXsVPXj3Ffc8eTXY5IjJKMBikqanJ9aFuraWpqYlgMDjj11iUe7mM56sfuoTGjl7uf+4o6QEfX9i6MtkliQhQXFxMdXU1DQ0NyS5l1gWDQYqLi2f8fAV6jMdj+O4ny+juG+Cvnq4gLeDjlvcsT3ZZIoue3++ntLQ02WUsCAr0UYYuitHVF+XrT+7H5zV8qnxZsssSEZkSjaGfJ8Xn4cHbNnPVRUv46s59/PDF48kuSURkShTo4wj6vfzw9nKuX7+Uv/yvCv7mv4+4foOMiCx8CvQJBHxevnfLJrZftozvPX+Mbzx5QNclFZF5TWPok/B6DN/+xAYioRT+8YVKWrv7+Zv/9W6Cfm+ySxMReQcF+gUYY7jn+neRFfLz7V8e5mRTF//425spiqQmuzQRkTHiGnIxxtxtjDlojDlgjHnUGDPzPeLnud+7ehUPfbacqsZOPvoPL7H7bffvEysiC8uMA90YUwTcCZRba9cDXmB7ogqbj65bm8+TX7qSJekpfPbh1/j2LyvoH9D5X0Rkfoh3o6gPSDXG+IAQUBt/SfPbqtx0nvrDq/jMluX80wvH2faPL3OqqSvZZYmIzDzQrbU1wN8Ap4AzQKu19leJKmw+S03x8u1PbOD7t2zieEMHH7pvNw/uqtTZGkUkqeIZcskCbgZKgUIgzRhz2zjr3WGM2WOM2eO2czHcWFbAM3dt5arVS/juM4e54YEXebmyKdllicgiFc+QyweAE9baBmttP/Bz4IrzV7LW7rDWlltry3Nzc+N4u/mpKJLKQ58t559vL6enf4DPPPQKd//0TV0wQ0TmXDyBfgq43BgTMs7lrK8FKhJT1sJz7Zp8fn331fzR+y/iP/fV8v6/3cWPX67SwUgiMmfiGUN/FdgJvA7sj73WjgTVtSClpnj5kw9ewjN3baWsOMz/fuogN9z/Ik++UUNUe8OIyCxbtBeJnm3WWp7ef5b7n3ubt+s6KM5K5Y6tK/lU+TIdaSoi0zLVi0Qr0GfZ4KDlfw7X84Ndx3j9VAs5aSn87lWl3Hb5CsKp/mSXJyILgAJ9nrHW8tqJZh58oZJdRxpID/i49T3L+fxVpeRluvYAWxFJAAX6PHaoto0HX6jkv/bV4vN4+MSmIm67fAXri8LJLk1E5iEF+gJwsqmTf9p9nMf3VtMbHWTjsgi3Xb6Cj5QVaJxdRIYp0BeQ1q5+Hn+9mkdePUllQyfhVD83bCjgurV5XLFqicJdZJFToI/n6LPw1k9g5TVQejVkrUheLeOw1vLK8WZ+8top/qeijs6+AVL9Xq5avYQPrMnj/e/KJzcjkOwyRWSOTTXQF9f50DvqoOolOPC4cz+rFFZe7YR76dWQlpPU8owxvHdVDu9dlUNvdIBXjjfzXEUdzx6q49eH6jBmPxuXRfjAmnw+sCafi/PTcY7pEhFZbD10AGuh4Qgc3wUnXnACvrfNeWzphljv/RpY8V5ISUtenaNYa6k4086zFXU8W1HHvupWAJZlp3Ltu/K5bm0+l5Vkk+LTFQVF3EhDLlM1EIXaN+DELjj+Apx+FQb6wOOHZVucnvvKa6BoE3jnx37jdW09PFdRz3MVdbx0rJHe6CAZAR9XX5LLdWvzuebiPMKh+VGriMRPgT5TfV1w6mWn9378BTjzFmAhJR1KrooF/NWQtxbmwXBHV1+Ul442OgF/uI7Gjj68HsOlyyJsKc1mS2k2m1dkkRFUwIssVAr0ROlqhqoXnSGa4y9Ac6WzPC0PSrc6vfeVV0NkeRKLdAwOWt6qbuHZijr+77Em9te0MjBo8RhYVxgeDvjLSrLJTktJdrkiMkUK9NnScnqk9358F3TWO8uzSkfCvWRr0jewAnT2RnnjVAuvnWjitapm3jjVQm/sIhyr89KHA/49pTksDetoVZH5SoE+F6yFhsMjvfeql6CvHTCxDayx8ffl82MDa290gP3Vrbx6opnXTjSz9+Q5OnqjACzPDo0K+GyWZ4e0B43IPKFAT4aBKNS+PtJ7r35t7AbWldc4Y/DzZANrdGCQijPtvHqiid9UOSF/rqsfgPzMAJeVOOG+pTSH1XnpeDwKeJFkUKDPB0MbWId2kTyzD2cDawbkXuKMu0eWOdNwbD68DALpSSl3cNBS2dAx3IN/7UQzZ9t6AIiE/MMBf1lJNusKM/F5tZukyFxQoM9HXc1wYrezkbXpGLScgtZqpxc/Wmr2qLBf4YT8cPAvg9TInJRrreV0c/eYHnxVU5dTot/LusJMNhSHKSsOs6EowsolaerFi8wCBfpCMTjoHMHaetoJ+JZTo+Zj02j32OcEwmMDPjKqdx9ZAaHsWdulsq6tZ3j8fX9NKwdrW+npdza0pqV4WVcUpqwozIbiMBuKwpTkKORF4qVAdwtroasJWk6OBPz5gd/XPvY5/tDEYR9Z5uxy6UnMcEl0YJDKhk72Vbewv6aV/TWtHKptG96bJiPgY/2ogC8rDmuDq8g0KdAXC2uhp2WcsB/V2+8+N/Y53gCEi52Tk2WvgpxVI9PICvDGd4qf/oFBjtZ1cKCmlX01LeyvbqXiTDt9seuqZgZ9sYCPxIZrwhRnpSrkRSagQJcRve3jB/65Kmg+PnIuGwCPzwn1nFWQcxFkrxyZzyyecc++LzrI23Xt7K9pZV91KwdqWjl8to3+AefvLxLyD/fgNxSF2VAcoTAcVMiLoECXqbIWOhudI2CbjkFTZWz+uDPt7xpZ1xuA7NKxQZ8dC/uMpdMet++NDnDkrBPy+6udoH+7rp3ooPM3mZOWMjxU44R9hPzMgEJeFh0FusTPWmg/44R807GxQd98fOzeOf60Ub35UUGfswpCOVMO+57+AQ6fbWd/dQv7qp0x+aP1HQzEQn5JemC4Fz801TVZxe0U6DK7BgecXS6bK2OBXzkyf64K7MDIuoEw5KyM9exXjQ39KeyC2d03wKEzbeyvbmF/TRv7a1o4Vt9BLOPJzwywoSgyHPLri8K6EIi4igJdkmeg3xmjHw75UUM5LaeBUX9zoRznPDhZJc5G2siKkfnM4gk30Hb1RTlU2zbci99f00plQwdDf85L0lO4ZGkGF+dncEl+BhfH5tMDi+uaLuIOCnSZn/p7YhtjK0eGcs5VObtltlbDYHRkXeON7Y1T4gR8Vkks8Eud++cN5XT0RjkYC/e369o5UtfB0bp2uvpGfi0URVJHgn5pOpfkZ7IqL42AT9dtlflLgS4Lz0AU2mqccD9XBedOjoT9uSrobBi7vj9tnLAvGenpp4QYHLTUtHRz5Gw7R+raOXK2nbfr2qls6Bjew8brMaxcksbawkzWFmSyrjDM2sJMnWJY5g0FurhPX6cT8i0n3xn2505Cf+fY9dPyxh/KySqhP62AE809wwFfcaaNQ7Vt1Lb2DD+9IByMBXwmawoyWZ2fQUlOSOewkTmnQJfFZWj3y+GArxob9q3VYzfUenzO0bNDvfvMYghm0kEqp7t8VLYaDrcY9jdaDjVZWm2IPvz4vYaVS9K5KD+di/MyWJ2fzuq8dEqWpOFX0MssUaCLjDYQhbbqcYZyYvNdjRd+CY+fHk86naRybiBI80CQdptKOyE6ScWbGiGUESE9kkNOdg55S3LJy80lJS0CgUwIZDg3j8brZXqmGuja5C+Lg9cXG3IpGf/xaB/0dUBPq3NkbW8b9LSNzPe24e1pI623nbTeNvJ62hjoaaOvs4XBnrN4ettJ6evA2zQITUDlxKUM+NMwwUw8gUwIDgV9bD4Yjt0io+bPW+ZPnRfXs5X5R4EuAuBLAV+2c6bKKfICqaMXWAv9XfR0nKP6bD21Z89S39hIc1Mjra3N9HW0kEYn6dFuMrq7yU3pJdffS5a3jgxzgtTBTvz97Zjzz655Po9/nLAPO/v0j/cFcP68XwdiuZUCXSRRjIGUNILZaVyUXcxFa8c+3D8wyMmmLo7VdzgXEqnv4FhDB5X1HXSO2rUyOwgblhjWRgZZHR6gJC3K8lA/2d5uPL2tzq+I4VuLM22rGVkW7WFS3sDkXwQpac6Xhtcfm/pG3feNWn7+fd80njcHw06Dg2AHnW0ngwPnzVtn3g7G7o+eHxyZNwa8KeALOP9uvhRn6k1J2BlLE0mBLjJH/F4PF+Wlc1He2CtSWWs529ZDZX0nx+rbqWzo5Fh9BztPdtDQ3ovz3zRIKCXC6vyLWZWbxvLsEMsLQ840J0Ru+qhz3PT3xIaMWqG7ZWzwv+PW4pyN81zVyPqD/XPwr2Eu/EWAPS+IJwroCZbPNo8/FvQp501HBf87plNdf9Tjvqkf9RxXoBtjIsAPgfU4h//9rrX25XheU2SxMcZQEE6lIJzKVauXjHmstbufY/XOAVKHzzr70b9S2cQTb9Qwen+GUIqX5dkhVuQ4IV8YSaUwkkFRJI+iwlQiIf/UTmpmrXOk72B/bBqd5H501PLz70/1eZOsh3F68sYLxhOb94yan8pyr9PLntZyj9P7Np7Yv0cfRHtHTXudbS5jpr3jrNfnXIZy4Nw46496HonbMSXeHvr9wDPW2m3GmBQglICaRCQmnOpn84osNq/IGrO8NzpAzbluTjZ3caqpi6qmTk7FhnN2HWkYvsDIkFS/l8JIkKKsEEWRIIXhVAojqRRlpVIUSWVpOOjsdmmM00tEB1XNCWudL7ELfWF868opvdyMA90YkwlsBX7Hqcv2AX2TPUdEEiPg87IyN52Vue+8oLi1lubOPmpbeqhp6aKmpYfalm5qW7qpaenmUG0rjR1j/6t6PYbCSNAZwskOsSw7xIrstOH74ZB/rpq2uJjY0JM3Mf++8fTQVwINwL8YY94N7AW+bK3tnPxpIjKbjDHkpAfISQ+woTg87jo9/QOcaXWCvuZcN6fPdXGyqYtTzV386mAdTZ1jAz8z6GN5bDinOCtEXkaA/MwgeRkBloaD5GcGCfq1f32yzfjAImNMOfAKcKW19lVjzP1Am7X2m+etdwdwB8Dy5cs3nzx5Ms6SRWS2dfRGOd3shPzpZifoTzU789XnuocvJzhadloK+ZlBCmIBXxAOsjQcZOmo+YygevozMetHihpjlgKvWGtLYvd/C7jHWnvjRM/RkaIiC5+1ltbufuraeqlr66GurYezrT2cjU3PtDrLzu/lA6SleJ2QDwdZmplKdpqfzKCfSFoKuekBcjNSyE0PsiQjhVCKdsIbMutHilprzxpjThtjLrHWHgGuBQ7N9PVEZGEwxhAJpRAJOeecn0hP/wD1bb2cbevhTGs3dW1O2A+F//+rbKSlq5/u/vF3MUxL8bIkI0BueoAl6QGWjAr73PTA8GO5GQEN98TE+xX4R8AjsT1cjgOfi78kEXGDoN/rjLvnTL7zW290gJaufhrae2no6KVxeNpHY0cvDe29VDZ08MqJXlq6xt9HPj3gIzcjwJL0lNg0QFYohUjI79xSUwiH/IRT/URSnakbz5oZV6Bba98ELvgzQERkIgGfl/xML/lTuDZsX3SQps6xYd8Qmw7dP3K2nZfaG2nriU76WhkBH5mp/glDPxKbz4x9AQzNZwR88/ZC5RqkEpEFI8XnGT4I60KiA4O09URp7e6npauPlu5+WrtGzQ/djz1+prWN1i5neXRw4m2LHoMT7EEfmcHRUz+ZqT5nOuqx9KCP9IDPmQ/4SQ/6CPm9eDyJ/1JQoIuIK/m8HrLTUmJXnkqb8vOstXT2DTjB39VPW3c/bT2xL4DYrb0nStvQtKefU81dI8t6J/9lAM6XwtAXwFDwh1J8BHwegn4vQb+HgM9LwO/htvesmHqbp7ymiMgiYIwhPeD0qouzLrz++QYGLR29Udp7+mnrjtLZF6WjJ0pHb+wW+xIY+gJoi63X0N5LT/8AvdHBMdPr1xdM+b0V6CIiCeT1mOExd2bwhRAP923mFRFZpBToIiIuoUAXEXEJBbqIiEso0EVEXEKBLiLiEgp0ERGXUKCLiLiEAl1ExCUU6CIiLqFAFxFxCQW6iIhLKNBFRFxCgS4i4hIKdBERl1Cgi4i4hAJdRMQlFOgiIi6hQBcRcQkFuoiISyjQRURcQoEuIuISCnQREZdQoIuIuIQCXUTEJRToIiIuoUAXEXEJBbqIiEso0EVEXCLuQDfGeI0xbxhj/jMRBYmIyMwkoof+ZaAiAa8jIiJxiCvQjTHFwI3ADxNTjoiIzFS8PfT7gK8CgwmoRURE4jDjQDfGfASot9buvcB6dxhj9hhj9jQ0NMz07URE5ALi6aFfCdxkjKkCHgPeb4z5t/NXstbusNaWW2vLc3Nz43g7ERGZzIwD3Vr7Z9baYmttCbAd+B9r7W0Jq0xERKZF+6GLiLiELxEvYq3dBexKxGuJiMjMqIcuIuISCnQREZdQoIuIuIQCXUTEJRToIiIuoUAXEXEJBbqIiEso0EVEXEKBLiLiEgp0ERGXUKCLiLiEAl1ExCUU6CIiLqFAFxFxCQW6iIhLKNBFRFxCgS4i4hIKdBERl1Cgi4i4hAJdRMQlFOgiIi6hQBcRcQkFuoiISyjQRURcQoEuIuISCnQREZdQoIuIuIQCXUTEJRToIiIuoUAXEXEJBbqIiEso0EVEXEKBLiLiEjMOdGPMMmPM88aYCmPMQWPMlxNZmIiITI8vjudGgT+x1r5ujMkA9hpjfm2tPZSg2kREZBpm3EO31p6x1r4em28HKoCiRBUmIiLTk5AxdGNMCXAp8GoiXk9ERKYv7kA3xqQDjwN3WWvbxnn8DmPMHmPMnoaGhnjfTkREJhBXoBtj/Dhh/oi19ufjrWOt3WGtLbfWlufm5sbzdiIiMol49nIxwD8DFdbav0tcSSIiMhPx9NCvBH4beL8x5s3Y7YYE1SUiItM0490WrbUvASaBtYiISBx0pKiIiEso0EVEXEKBLiLiEgp0ERGXUKCLiLiEAl1ExCUU6CIiLqFAFxFxCQW6iIhLKNBFRFxCgS4i4hIKdBERl1Cgi4i4hAJdRMQlFOgiIi6hQBcRcQkFuoiISyjQRURcQoEuIuISCnQREZdQoIuIuIQCXUTEJRToIiIuoUAXEXEJBbqIiEso0EVEXEKBLiLiEgp0ERGXUKCLiLiEAl1ExCUU6CIiLqFAFxFxCQW6iIhLxBXoxpgPG2OOGGOOGWPuSVRRIiIyfTMOdGOMF/g+cD2wFviMMWZtogoTEZHpiaeHvgU4Zq09bq3tAx4Dbk5MWSIiMl3xBHoRcHrU/erYMhERSQJfHM814yyz71jJmDuAO2J3e40xB+J4z4ViCdCY7CJm2WJoI6idbrKQ27hiKivFE+jVwLJR94uB2vNXstbuAHYAGGP2WGvL43jPBWExtHMxtBHUTjdZDG2MZ8jlN8BqY0ypMSYF2A78R2LKEhGR6ZpxD91aGzXGfAn4b8ALPGytPZiwykREZFriGXLBWvs08PQ0nrIjnvdbQBZDOxdDG0HtdBPXt9FY+47tmCIisgDp0H8REZeYk0B38ykCjDFVxpj9xpg3jTF7YsuyjTG/NsYcjU2zkl3ndBljHjbG1I/ezXSidhnHA7HPd58xZlPyKp+eCdp5rzGmJvaZvmmMuWHUY38Wa+cRY8yHklP19BhjlhljnjfGVBhjDhpjvhxb7qrPc5J2uurznJS1dlZvOBtMK4GVQArwFrB2tt93rm5AFbDkvGV/DdwTm78H+G6y65xBu7YCm4ADF2oXcAPwS5xjEy4HXk12/XG2817gT8dZd23s7zcAlMb+rr3JbsMU2lgAbIrNZwBvx9riqs9zkna66vOc7DYXPfTFeIqAm4EfxeZ/BHwsibXMiLV2N9B83uKJ2nUz8GPreAWIGGMK5qbS+EzQzoncDDxmre211p4AjuH8fc9r1toz1trXY/PtQAXOUd2u+jwnaedEFuTnOZm5CHS3nyLAAr8yxuyNHRULkG+tPQPOHxmQl7TqEmuidrnxM/5SbLgcfU8YAAABrklEQVTh4VFDZgu+ncaYEuBS4FVc/Hme105w6ed5vrkI9CmdImABu9JauwnnrJN/aIzZmuyCksBtn/GDwCpgI3AG+NvY8gXdTmNMOvA4cJe1tm2yVcdZtpDb6crPczxzEehTOkXAQmWtrY1N64EncH6y1Q39RI1N65NXYUJN1C5XfcbW2jpr7YC1dhB4iJGf4Qu2ncYYP07IPWKt/Xlsses+z/Ha6cbPcyJzEeiuPUWAMSbNGJMxNA98EDiA077bY6vdDjyVnAoTbqJ2/Qfw2djeEZcDrUM/5Rei88aLP47zmYLTzu3GmIAxphRYDbw21/VNlzHGAP8MVFhr/27UQ676PCdqp9s+z0nN0dbnG3C2OFcCX0/2luAEtmslzlbyt4CDQ20DcoDngKOxaXaya51B2x7F+Xnaj9OT+fxE7cL56fr92Oe7HyhPdv1xtvNfY+3Yh/OfvmDU+l+PtfMIcH2y659iG6/CGUrYB7wZu93gts9zkna66vOc7KYjRUVEXEJHioqIuIQCXUTEJRToIiIuoUAXEXEJBbqIiEso0EVEXEKBLiLiEgp0ERGX+P8dkAPHeJqvBgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4h 20min 54s, sys: 12min 45s, total: 4h 33min 40s\n",
      "Wall time: 1h 22min 45s\n",
      "2 nb: gc.collect:1339 - used GB memory:13.83\n"
     ]
    }
   ],
   "source": [
    "print(f\"1 nb: gc.collect:{gc.collect()} - used GB memory:{usedGB_RAM()}\") \n",
    "%time learn.fit_one_cycle(10, 2e-3, moms=(0.8,0.7))\n",
    "print(f\"2 nb: gc.collect:{gc.collect()} - used GB memory:{usedGB_RAM()}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('model-32k-sentencepiece-vocab')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#load does not work\n",
    "learn.load('model-32k-sentencepiece-vocab')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "while training a very large language model i noticed that \"fix_ds\" in basic_data.py => def _init_ds create an extra dataset that will lead to an instance of LanguageModelLoader.\n",
    "Why would we need an extra dataset and LanguageModelLoader. The downside is loss of memory\n",
    "\n",
    "I can see and measure that it is a copy of the training data and that it uses some memory - which is bad.\n",
    "\n",
    "Why do we need it ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
