{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a language model based on wikipedia in you language \n",
    "\n",
    "The notebooke includes the whole process but you will need to help the process if the following problem happens:  \n",
    "-you internet connection is interrupted (stage 1)\n",
    "\n",
    "-you run out of diskspace\n",
    "\n",
    "-because of the huge memory consumption. creatig the databunch for the training requires lots of memory\n",
    "\n",
    "\n",
    "In order ot handle these problemn the notebook is divided into stages. If the process is failes in a stage then you can resume the processing from the beginning of that stage by:\n",
    "-restarting the kernel \"Kernel\"/Restart & Clear Output\n",
    "\n",
    "-running the cells in stage 0: initialization\n",
    "\n",
    "-running the cells from the start of the stage that failed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 0: Initialisation\n",
    "lang: You must set the parameter \"lang\" to the language you want to build a model for. Fx:\n",
    "\n",
    "fr: for french\n",
    "\n",
    "en: for english\n",
    "\n",
    "de:for german\n",
    "\n",
    "da: for danish \n",
    "\n",
    "etc.\n",
    "\n",
    "pathData: You must se the location where you want your data stored using the parameter pathData. Consider using a ssd-rive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang=\"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import * \n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pathlib import *\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathData       = Path(\"../../data/nlp\")\n",
    "path           = pathData / lang\n",
    "pathDump       = path/\"wiki-dump\"\n",
    "pathJson       = path/\"wiki-json\"\n",
    "\n",
    "pathTrainValid = path/\"wiki-train_valid\"\n",
    "pathTxt        = pathTrainValid/\"txt\"\n",
    "pathToks       = pathTrainValid/\"toks\"\n",
    "pathcsv        = pathTrainValid/\"wiki.csv\"\n",
    "\n",
    "cache_name   = \"sp-model\"\n",
    "pathVocab    = pathTrainValid / cache_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# requried libraries: \n",
    "conda install -c anaconda psutil \n",
    "\n",
    "conda install -c anaconda git \n",
    "\n",
    "#conda install -c menpo wget \n",
    "conda install curl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usedGB_RAM():\n",
    "    import psutil\n",
    "    return round((psutil.virtual_memory().used + psutil.swap_memory().used)/1e9,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: download the selected language from wikipedia and convert the articles to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if len(list(pathDump.glob(\"*.bz2\")))==0:\n",
    "    print(f\"1 nb: gc.collect:{gc.collect()} - used GB memory:{usedGB_RAM()}\") \n",
    "    pathDump.mkdir(parents=True,exist_ok=True)\n",
    "    #fn  = f\"{lang}wiki-latest-pages-articles.xml.bz2\"\n",
    "    fn  = f\"{lang}wiki-latest-pages-articles-multistream.xml.bz2\"\n",
    "    url = f\"https://dumps.wikimedia.org/{lang}wiki/latest/{fn}\"\n",
    "    #cmd = f\"wget -c --no-check-certificate --show-progress {str(url)} -P {str(pathDump)}\"\n",
    "    cmd = f'curl -k -C - -o \"{str(pathDump/fn)}\" \"{str(url)}\"'\n",
    "    print(f\"If the command fails in the notebook then copy the command and run it in the terminal:{cmd}\")\n",
    "    ! $cmd\n",
    "    print(f\"2 nb: gc.collect:{gc.collect()} - used GB memory:{usedGB_RAM()}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: Convert wikipedia dump to articles in json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathWikiExtractor = Path(\"../wikiextractor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pathJson.exists():\n",
    "    !git clone https://github.com/attardi/wikiextractor.git $pathWikiExtractor\n",
    "    cmd = f\"cd {str(pathWikiExtractor)} && python setup.py install\"\n",
    "    ! $cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not pathJson.exists():\n",
    "    print(f\"1 nb: gc.collect:{gc.collect()} - used GB memory:{usedGB_RAM()}\") \n",
    "    #extracting the json-files. This takes about 1 hour for french with files read from and saved to a ssd hardrive \n",
    "    fn_wikidump = list(pathDump.iterdir())[0]\n",
    "    cmd = f\"cd {str(pathWikiExtractor)} && python WikiExtractor.py -o {str(pathJson)} --json --processes {defaults.cpus} -q  {str(fn_wikidump)}\"\n",
    "    print(f\"If WikiExtractor fails in the notebook then copy the command and run it in the terminal:{cmd}\")\n",
    "    ! $cmd\n",
    "    print(f\"2 nb: gc.collect:{gc.collect()} - used GB memory:{usedGB_RAM()}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3: building a vocabulary using sentencepiece\n",
    "\n",
    "Now we separate the title of the wikipedia section from text section that we keep. \n",
    "\n",
    "In order to makes a first reduction on the number of section we clean the text with the preprocessing rules from fastai and ignore text with less than \"minWords\"\n",
    "\n",
    "You must set the lenght of the shortes sections you want to keep using the parameter \"minWords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minWords  = 7\n",
    "chunksize = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\kl\\appdata\\local\\conda\\conda\\envs\\fastai\\lib\\site-packages (0.1.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai_sentencepiece import *\n",
    "from filetokenizer import *\n",
    "from languagemodelloader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 nb: gc.collect:7 - used GB memory:14.68\n"
     ]
    }
   ],
   "source": [
    "if not pathTxt.exists():\n",
    "    print(f\"1 nb: gc.collect:{gc.collect()} - used GB memory:{usedGB_RAM()}\") \n",
    "    swm = SentencepieceWikiModel(lang=lang, pathJson=pathJson, pathcsv=pathcsv, pathTxt=pathTxt, pathVocab=pathVocab, minWords=minWords )\n",
    "    %time swm.wikijson2TrainingData()   \n",
    "    print(f\"2 nb: gc.collect:{gc.collect()} - used GB memory:{usedGB_RAM()}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow(2,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not pathVocab.exists():\n",
    "    print(f\"1 nb: gc.collect:{gc.collect()} - used GB memory:{usedGB_RAM()}\") \n",
    "    swm = SentencepieceWikiModel(lang=lang, pathJson=pathJson, pathcsv=pathcsv, pathTxt=pathTxt, pathVocab=pathVocab, minWords=minWords)\n",
    "    %time swm.trainVocabulary()\n",
    "    print(f\"2 nb: gc.collect:{gc.collect()} - used GB memory:{usedGB_RAM()}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show some examples using the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "swm = SentencepieceWikiModel(lang=lang, pathJson=pathJson, pathcsv=pathcsv, pathTxt=pathTxt, pathVocab=pathVocab, minWords=minWords)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(str(pathVocab / \"m.model\"))\n",
    "print(\"1: Size of vocabulary:\",sp.GetPieceSize())\n",
    "sentence = \"She is tall. He is small\"\n",
    "print(\"2:\", sp.EncodeAsPieces(sentence))\n",
    "print(\"3:\", sp.EncodeAsIds(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Control symbol\")\n",
    "for s in [\"<unk>\"]: print(f\"{s}({sp.PieceToId(s)})\")\n",
    "\n",
    "print(f\"\\nuser_defined_symbols\")\n",
    "for s in swm.getUserdefinedSymbols():print(f\"{s}({sp.PieceToId(s)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training of the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tokenizer\n",
    "The sentencepiece vocabulary was trained in Train Sentencepiece tokenizer.ipynb.\n",
    "\n",
    "Here we will make a BasicTokenizer from Sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spt_func  = partial(SentencepieceTokenizer.create, pathVocab=pathVocab)\n",
    "spt_func.__name__ = SentencepieceTokenizer.__name__\n",
    "spt       = spt_func(lang=\"en\")\n",
    "tokenizer = Tokenizer(spt_func,\"en\")\n",
    "\n",
    "pad_idx   = spt.vocab().numericalize([text.transform.PAD])[0]\n",
    "vocab,max_vocab  = spt.vocab(), len(spt.vocab().itos)\n",
    "\n",
    "print(tokenizer)\n",
    "print(\"size og vocabulary:\", max_vocab)\n",
    "print(\"pad_idx:\",pad_idx)\n",
    "\n",
    "print(spt.vocab().numericalize( [\"<unk>\" ,\"xxbos\" ,\"xxpad\" ,\"xxmaj\" ,\"xxup\" ,\"xxrep\" ,\"xxwrep\", \"xxfld\"]  ))\n",
    "sentence = [\"She is tall.\", \"He is small\"]\n",
    "tokenizer._process_all_1(sentence)\n",
    "\n",
    "print(\"vocab:\",vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM Training\n",
    "Set the minimum number of tokens for the sections that we shall retain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discard section with kess than \"minTok\" tokens\n",
    "minToks = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx   = spt.vocab().numericalize([text.transform.PAD])[0]\n",
    "vocab,max_vocab = spt.vocab(), len(spt.vocab().itos)\n",
    "trainTokenizer = FileTokenizer(pathToks/\"train\", spt_func,\"en\",vocab)\n",
    "validTokenizer = FileTokenizer(pathToks/\"valid\", spt_func,\"en\",vocab)\n",
    "\n",
    "print(trainTokenizer)\n",
    "print(\"size og vocabulary:\", max_vocab)\n",
    "print(\"pad_idx:\",pad_idx)\n",
    "\n",
    "print(spt.vocab().numericalize( [\"<unk>\" ,\"xxbos\" ,\"xxpad\" ,\"xxmaj\" ,\"xxup\" ,\"xxrep\" ,\"xxwrep\", \"xxfld\"]  ))\n",
    "#sentence = [\"She is tall.\", \"He is small\"]\n",
    "#tokenizer._process_all_1(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files   = np.asarray( list(pathTxt.glob(\"*.txt\")) )\n",
    "nrows   = len(files)\n",
    "split   = 0.2\n",
    "splitindex, index = int(nrows*split+.5), np.random.permutation(np.arange(nrows)) \n",
    "\n",
    "chunksize=0\n",
    "\n",
    "trainList = TextList( files[:-splitindex], vocab=vocab, pad_idx=pad_idx, \n",
    "                      processor=[FileTokenizeProcessor(tokenizer=trainTokenizer, \n",
    "                                                       chunksize=chunksize, mark_fields=False)],\n",
    "                      minToks=minToks)\n",
    "\n",
    "validList = TextList( files[-splitindex:], vocab=vocab, pad_idx=pad_idx, \n",
    "                      processor=[FileTokenizeProcessor(tokenizer=validTokenizer, \n",
    "                                                       chunksize=chunksize, mark_fields=False)],\n",
    "                      minToks=minToks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (pathToks/\"train\").exists():\n",
    "    print(f\"1 nb: gc.collect:{gc.collect()} - used GB memory:{usedGB_RAM()}\") \n",
    "    %time trainList.process()\n",
    "    trainList=None\n",
    "    print(f\"2 nb: gc.collect:{gc.collect()} - used GB memory:{usedGB_RAM()}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (pathToks/\"valid\").exists():\n",
    "    print(f\"1 nb: gc.collect:{gc.collect()} - used GB memory:{usedGB_RAM()}\") \n",
    "    %time p = validList.process()\n",
    "    validList=None\n",
    "    print(f\"2 nb: gc.collect:{gc.collect()} - used GB memory:{usedGB_RAM()}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"1 nb: gc.collect:{gc.collect()} - used GB memory:{usedGB_RAM()}\") \n",
    "%time trainIDS = trainTokenizer.getIds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time validIDS=validTokenizer.getIds()\n",
    "print(f\"2 nb: gc.collect:{gc.collect()} - used GB memory:{usedGB_RAM()}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f\"Traning:    number of arrays:{len(trainIDS)} - number of ids:{np.sum([len(ids) for ids in trainIDS])}\")\n",
    "print(f\"Validation: number of arrays:{len(validIDS)} - number of ids:{np.sum([len(ids) for ids in validIDS])}\")\n",
    "\n",
    "#Analyse the distribution of the legnth of tokens sequences in the ragged/jagged array of tokes\n",
    "sectionlengths = np.asarray([len(s) for s in trainIDS],dtype=np.int32)\n",
    "plt.hist(sectionlengths[sectionlengths<1000], 100, density=True, facecolor='g', alpha=0.75)\n",
    "np.histogram(sectionlengths[sectionlengths<1000],50)\n",
    "\n",
    "print(f\"Lenght of token rags min:{min(sectionlengths)} max:{np.max(sectionlengths)} - median:{np.median(sectionlengths)}\")\n",
    "print(f\"rags > 1000 tokes:{np.sum(sectionlengths>1000)}\")\n",
    "sectionlengths[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i have an issue with passing pad_idx\n",
    "\n",
    "#uses nTrainToks, nValidToks to limit the number of tokens used for training/validation\n",
    "nTrainToks, nValidToks = 1000,200\n",
    "#nTrainToks, nValidToks = 3000,600\n",
    "if nTrainToks>0 and nValidToks>0:\n",
    "    trainIDS = trainIDS[0:1000]\n",
    "    validIDS = validIDS[0:1000]\n",
    "dblm = MyTextLMDataBunch.from_ids( pathTrainValid, vocab, trainIDS, validIDS, bptt=70, p_bptt=0.25, bs=32)\n",
    "#dblm = TextLMDataBunch.from_ids( pathTrainValid, vocab, trainIDS, validIDS, bs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dblm.train_ds.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dblm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"1 nb: gc.collect:{gc.collect()} - used GB memory:{usedGB_RAM()}\") \n",
    "%time learn = language_model_learner(dblm, drop_mult=0, qrnn=False, pad_token=-1, callback_fns=ShowGraph)\n",
    "print(f\"2 nb: gc.collect:{gc.collect()} - used GB memory:{usedGB_RAM()}\") "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#problematic at present\n",
    "%time learn.lr_find()\n",
    "learn.recorder.plot(skip_start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai_sentencepiece import *\n",
    "from filetokenizer import *\n",
    "from languagemodelloader import *\n",
    "print(f\"1 nb: gc.collect:{gc.collect()} - used GB memory:{usedGB_RAM()}\") \n",
    "%time learn.fit_one_cycle(10, 2e-3, moms=(0.8,0.7))\n",
    "print(f\"2 nb: gc.collect:{gc.collect()} - used GB memory:{usedGB_RAM()}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('model-32k-sentencepiece-vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "søg på \"Music plays a huge role in this production."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
