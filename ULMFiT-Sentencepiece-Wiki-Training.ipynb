{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a language model based on wikipedia in you language \n",
    "\n",
    "The notebooke includes the whole process but you will need to help the process if the following problem happens:  \n",
    "-you internet connection is interrupted (stage 1)\n",
    "\n",
    "-you run out of diskspace\n",
    "\n",
    "-because of the huge memory consumption. creatig the databunch for the training requires lots of memory\n",
    "\n",
    "\n",
    "In order ot handle these problemn the notebook is divided into stages. If the process is failes in a stage then you can resume the processing from the beginning of that stage by:\n",
    "-restarting the kernel \"Kernel\"/Restart & Clear Output\n",
    "\n",
    "-running the cells in stage 0: initialization\n",
    "\n",
    "-running the cells from the start of the stage that failed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 0: Initialisation\n",
    "lang: You must set the parameter \"lang\" to the language you want to build a model for. Fx:\n",
    "\n",
    "fr: for french\n",
    "\n",
    "en: for english\n",
    "\n",
    "de:for german\n",
    "\n",
    "da: for danish \n",
    "\n",
    "etc.\n",
    "\n",
    "pathData: You must se the location where you want your data stored using the parameter pathData. Consider using a ssd-rive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang=\"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import * \n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pathlib import *\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathData       = Path(\"../../data/nlp\")\n",
    "path           = pathData / lang\n",
    "pathDump       = path/\"wiki-dump\"\n",
    "pathJson       = path/\"wiki-json\"\n",
    "\n",
    "pathTrainValid = path/\"wiki-train_valid\"\n",
    "pathTxt        = pathTrainValid/\"txt\"\n",
    "pathToks       = pathTrainValid/\"toks\"\n",
    "pathcsv        = pathTrainValid/\"wiki.csv\"\n",
    "\n",
    "cache_name   = \"sp-model\"\n",
    "pathVocab    = pathTrainValid / cache_name"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data\n",
    "└── nlp\n",
    "     └── lang\n",
    "        ├── wiki-dump\n",
    "        ├── wiki-json\n",
    "        └── wiki-train_valid\n",
    "            ├── models         <- language model\n",
    "            ├── sp-model       <- sentencepiece model\n",
    "            ├── toks\n",
    "            ├── txt            <- preprocessed text\n",
    "            └── wiki           <- preprocessed txt i csv file\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# requried libraries: \n",
    "conda install -c anaconda psutil \n",
    "\n",
    "conda install -c anaconda git \n",
    "\n",
    "#conda install -c menpo wget \n",
    "conda install curl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: download the selected language from wikipedia and convert the articles to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if len(list(pathDump.glob(\"*.bz2\")))==0:\n",
    "    pathDump.mkdir(parents=True,exist_ok=True)\n",
    "    #fn  = f\"{lang}wiki-latest-pages-articles.xml.bz2\"\n",
    "    fn  = f\"{lang}wiki-latest-pages-articles-multistream.xml.bz2\"\n",
    "    url = f\"https://dumps.wikimedia.org/{lang}wiki/latest/{fn}\"\n",
    "    #cmd = f\"wget -c --no-check-certificate --show-progress {str(url)} -P {str(pathDump)}\"\n",
    "    cmd = f'curl -k -C - -o \"{str(pathDump/fn)}\" \"{str(url)}\"'\n",
    "    print(f\"If the command fails in the notebook then copy the command and run it in the terminal:{cmd}\")\n",
    "    ! $cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: Convert wikipedia dump to articles in json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathWikiExtractor = Path(\"../wikiextractor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pathJson.exists():\n",
    "    !git clone https://github.com/attardi/wikiextractor.git $pathWikiExtractor\n",
    "    cmd = f\"cd {str(pathWikiExtractor)} && python setup.py install\"\n",
    "    ! $cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not pathJson.exists():\n",
    "    #extracting the json-files. This takes about 1 hour for french with files read from and saved to a ssd hardrive \n",
    "    fn_wikidump = list(pathDump.iterdir())[0]\n",
    "    #cmd = f\"cd {str(pathWikiExtractor)} && python WikiExtractor.py -o {str(pathJson)} --json --processes {defaults.cpus} -q --filter_disambig_pages --no-templates --discard_elements gallery,timeline,noinclude {str(fn_wikidump)}\"\n",
    "    #cmd = f\"cd {str(pathWikiExtractor)} && python WikiExtractor.py -o {str(pathJson)} --json --processes {defaults.cpus} -q --filter_disambig_pages --discard_elements gallery,timeline,noinclude {str(fn_wikidump)}\"\n",
    "    cmd = f\"cd {str(pathWikiExtractor)} && python WikiExtractor.py -o {str(pathJson)} --json --processes {defaults.cpus} -q --filter_disambig_pages --no-templates --discard_elements gallery,timeline,noinclude {str(fn_wikidump)}\"\n",
    "    print(f\"If WikiExtractor fails in the notebook then copy the command and run it in the terminal:{cmd}\")\n",
    "    ! $cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3: building a vocabulary using sentencepiece\n",
    "\n",
    "Now we separate the title of the wikipedia section from text section that we keep. \n",
    "\n",
    "In order to makes a first reduction on the number of section we clean the text with the preprocessing rules from fastai and ignore text with less than \"minWords\"\n",
    "\n",
    "You must set the lenght of the shortes sections to keep using the parameter \"minWords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "minWords  = 1\n",
    "chunksize = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\kl\\appdata\\local\\conda\\conda\\envs\\fastai\\lib\\site-packages (0.1.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai_sentencepiece import *\n",
    "from filetokenizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "if not pathTxt.exists():\n",
    "    swm = SentencepieceWikiVocab(lang=lang, pathJson=pathJson, pathcsv=pathcsv, pathTxt=pathTxt, pathVocab=pathVocab)    \n",
    "    %time swm.wikijson2TrainingData()\n",
    "    %time data = pd.read_csv(pathcsv)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if data is not None:\n",
    "    percentiles = 0.01 * np.asarray([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, \n",
    "                                     1,2,3,4,10,20, 50, 75, 90, 95, 96, 97, 98, \n",
    "                                     99, 99.1, 99.2, 99.3, 99.4, 99.5, 99.6, 99.7, 99.8, 99.9,99.99]) \n",
    "    %time display(data.describe(percentiles,include=[np.number]))\n",
    "    print(f\"data:{data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data is not None:\n",
    "    lpl = data.alpha_r3.values\n",
    "    #ix = lpl<8\n",
    "    ix = np.logical_and(lpl>.17,lpl<.18)\n",
    "    data_empty = data[ix]\n",
    "    print(f\"data_empty:{data_empty.shape}\")\n",
    "    \n",
    "    percentiles = 0.01 * np.asarray([1,10,25, 50, 75, 90, 99])     \n",
    "    %time data_empty.describe(percentiles,include=[np.number])\n",
    "    display(data_empty)\n",
    "    print(data_empty.text.head().values)\n",
    "    print(data_empty.text.tail().values)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not (pathVocab/\"m.model\").exists():\n",
    "    swm = SentencepieceWikiVocab(lang=lang, pathJson=pathJson, pathcsv=pathcsv, pathTxt=pathTxt, pathVocab=pathVocab)\n",
    "    %time swm.trainVocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (pathVocab/\"itos.pkl\").exists():\n",
    "    swm = SentencepieceWikiVocab(lang=lang, pathJson=pathJson, pathcsv=pathcsv, pathTxt=pathTxt, pathVocab=pathVocab)\n",
    "    swm.convertSPVocab2FastaiVocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show some examples using the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Size of vocabulary: 4000\n",
      "Control symbol:\n",
      "<unk>(0)\n",
      "xxbos(1)\n",
      "xxeos(2)\n",
      "xxpad(3)\n",
      "\n",
      "User defined symbols:\n",
      "xxfld(4)\n",
      "xxmaj(5)\n",
      "xxup(6)\n",
      "xxrep(7)\n",
      "xxwrep(8)\n",
      "0(9)\n",
      "1(10)\n",
      "2(11)\n",
      "3(12)\n",
      "4(13)\n",
      "5(14)\n",
      "6(15)\n",
      "7(16)\n",
      "8(17)\n",
      "9(18)\n",
      "°(19)\n",
      "%(20)\n",
      "$(21)\n",
      "§(22)\n",
      "£(23)\n",
      "€(24)\n",
      "((25)\n",
      ")(26)\n",
      "<(27)\n",
      ">(28)\n",
      "\"(29)\n",
      "'(30)\n",
      "“(31)\n",
      "”(32)\n",
      "‘(33)\n",
      "’(34)\n",
      "!(35)\n",
      ",(57)\n",
      ";(36)\n",
      ":(37)\n",
      ".(38)\n",
      "•(39)\n",
      "—(40)\n",
      "|(41)\n",
      "\\(42)\n",
      "/(43)\n",
      "*(44)\n",
      "+(45)\n",
      "-(46)\n",
      "=(47)\n",
      "⁄(48)\n",
      "′(49)\n",
      "_(50)\n",
      "#(51)\n",
      "&(52)\n",
      "?(53)\n",
      "Sentence:          she is tall. he is small\n",
      "Sentence as pieces:['▁she', '▁is', '▁tall', '.', '▁he', '▁is', '▁small']\n",
      "Sentence as ids:   [126, 66, 3118, 38, 74, 66, 557]\n",
      "Sentence from ids:   she is tall. he is small\n"
     ]
    }
   ],
   "source": [
    "swm = SentencepieceWikiVocab(lang=lang, pathJson=pathJson, pathcsv=pathcsv, pathTxt=pathTxt, pathVocab=pathVocab)\n",
    "sp  = spm.SentencePieceProcessor()\n",
    "sp.Load(str(pathVocab / \"m.model\"))\n",
    "print(\"\\nSize of vocabulary:\",sp.GetPieceSize())\n",
    "\n",
    "control_symbols = [\"<unk>\",\"xxbos\",\"xxeos\",\"xxpad\"]\n",
    "print(f\"Control symbol:\")\n",
    "for s in control_symbols: print(f\"{s}({sp.PieceToId(s)})\")\n",
    "\n",
    "print(f\"\\nUser defined symbols:\")\n",
    "for s in swm.getUserdefinedSymbols():print(f\"{s}({sp.PieceToId(s)})\")\n",
    "\n",
    "sentence = \"She is tall. He is small\".lower()\n",
    "print(f\"Sentence:          {sentence}\")\n",
    "print(f\"Sentence as pieces:{sp.EncodeAsPieces(sentence)}\")\n",
    "print(f\"Sentence as ids:   {sp.EncodeAsIds(sentence)}\")\n",
    "print(f\"Sentence from ids:   {sp.DecodeIds(sp.EncodeAsIds(sentence))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training of the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM Training\n",
    "Set the minimum number of tokens for the sections that we shall retain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discard section with kess than \"minTok\" tokens\n",
    "minToks = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unk_idx:0 pad_idx:3\n"
     ]
    }
   ],
   "source": [
    "spt_func  = partial(SentencepieceTokenizer.create, pathVocab=pathVocab)\n",
    "spt_func.__name__ = SentencepieceTokenizer.__name__\n",
    "spt       = spt_func(lang=\"en\")\n",
    "\n",
    "unk_idx,pad_idx = spt.vocab().numericalize([text.transform.UNK])[0], spt.vocab().numericalize([text.transform.PAD])[0]\n",
    "print(f\"unk_idx:{unk_idx} pad_idx:{pad_idx}\")\n",
    "vocab,max_vocab = spt.vocab(), len(spt.vocab().itos)\n",
    "trainTokenizer  = SPFileTokenizer(pathToks/\"train\", spt_func,\"en\",vocab,n_cpus=max(defaults.cpus-1,1))\n",
    "validTokenizer  = SPFileTokenizer(pathToks/\"valid\", spt_func,\"en\",vocab,n_cpus=max(defaults.cpus-1,1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pad_idx   = spt.vocab().numericalize([text.transform.PAD])[0]\n",
    "vocab,max_vocab  = spt.vocab(), len(spt.vocab().itos)\n",
    "\n",
    "print(\"size og vocabulary:\", max_vocab)\n",
    "print(\"pad_idx:\",pad_idx)\n",
    "\n",
    "print(spt.vocab().numericalize( [\"<unk>\" ,\"xxbos\" ,\"xxpad\" ,\"xxmaj\" ,\"xxup\" ,\"xxrep\" ,\"xxwrep\", \"xxfld\"]  ))\n",
    "sentence = [\"She is tall.\", \"He is small\"]\n",
    "\n",
    "tokenizer = Tokenizer(spt_func,\"en\")\n",
    "print(tokenizer)\n",
    "tokenizer._process_all_1(sentence)\n",
    "\n",
    "print(\"vocab:\",vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files   = np.asarray( list(pathTxt.glob(\"*.txt\")) )\n",
    "nrows   = len(files)\n",
    "split   = 0.2\n",
    "splitindex, index = int(nrows*split+.5), np.random.permutation(np.arange(nrows)) \n",
    "\n",
    "chunksize=0\n",
    "\n",
    "trainList = TextList( files[:-splitindex], vocab=vocab, pad_idx=pad_idx, \n",
    "                      processor=[SPFileTokenizeProcessor(tokenizer=trainTokenizer, \n",
    "                                                       chunksize=chunksize, mark_fields=False)])\n",
    "\n",
    "validList = TextList( files[-splitindex:], vocab=vocab, pad_idx=pad_idx, \n",
    "                      processor=[SPFileTokenizeProcessor(tokenizer=validTokenizer, \n",
    "                                                       chunksize=chunksize, mark_fields=False)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (pathToks/\"train\").exists():\n",
    "    %time trainList.process()\n",
    "    trainList=None\n",
    "    gc.collect()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (pathToks/\"valid\").exists():\n",
    "    %time p = validList.process()\n",
    "    validList=None\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following cells are used to find macUnk, minToks and maxToks.\n",
    "The 4 cells were deactivate after deciding on these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 40s\n",
      "Wall time: 30 s\n"
     ]
    }
   ],
   "source": [
    "%time trainIDS = trainTokenizer.getIds()\n",
    "%time validIDS = validTokenizer.getIds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unkPrSentence %  percentile \n",
      "0.0  0\n",
      "50.0  0\n",
      "75.0  0\n",
      "90.0  0\n",
      "97.0  0\n",
      "98.5  0\n",
      "99.0  0\n",
      "99.1  1\n",
      "99.2  1\n",
      "99.3  1\n",
      "99.4  1\n",
      "99.5  1\n",
      "99.6  2\n",
      "99.7  2\n",
      "99.8  3\n",
      "99.9  4\n",
      "99.95  6\n",
      "99.99  11\n",
      "100.0  133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how much do we loos by removing sentences with unks\n",
    "unkPrSentence        = np.asarray([np.sum(s==unk_idx) for s in trainIDS],dtype=np.int)\n",
    "percentUnkPrSentence = np.asarray([np.sum(s==unk_idx)/len(s) for s in trainIDS])\n",
    "percent              = np.asarray([0, 50, 75, 90, 97, 98.5, 99, 99.1, 99.2, 99.3, 99.4, 99.5, 99.6, 99.7, 99.8, 99.9, 99.95,  99.99, 100]) \n",
    "percentiles          = np.percentile(unkPrSentence,percent).astype(np.int)\n",
    "print(\"unkPrSentence %  percentile \")\n",
    "for i in range(len(percent)):print(f\"{percent[i]}  {percentiles[i]}\")   \n",
    "\n",
    "unkPrSentence=None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning:    number of arrays:25685835 - number of ids:2980870303\n",
      "Validation: number of arrays:6181342 - number of ids:722508809\n",
      "Lenght of token rags min:7 max:2038 - median:94.0\n",
      "Lenght of token rags mean:116.05113491541155 std:88.30266532778766\n",
      "%  percentile\n",
      "0.0  7\n",
      "0.11  11\n",
      "0.2  12\n",
      "1.0  15\n",
      "1.5  16\n",
      "2.0  18\n",
      "25.0  53\n",
      "50.0  94\n",
      "75.0  154\n",
      "90.0  228\n",
      "97.0  327\n",
      "99.0  423\n",
      "99.5  488\n",
      "99.9  662\n",
      "99.99  975\n",
      "100.0  2038\n",
      "\n",
      "minToks:11 maxToks:975\n",
      "rags <= minToks:36511\n",
      "rags >= maxToks:2589"
     ]
    }
   ],
   "source": [
    "print(f\"Traning:    number of arrays:{len(trainIDS)} - number of ids:{sum(len(ids) for ids in trainIDS)}\")\n",
    "print(f\"Validation: number of arrays:{len(validIDS)} - number of ids:{sum(len(ids) for ids in validIDS)}\")\n",
    "\n",
    "#Analyse the distribution of the legnth of tokens sequences in the ragged/jagged array of tokes\n",
    "sectionlengths = np.asarray([len(s) for s in trainIDS],dtype=np.int32)\n",
    "plt.hist(sectionlengths[sectionlengths<500], 250, density=True, facecolor='g', alpha=0.75)\n",
    "np.histogram(sectionlengths[sectionlengths<1000],50)\n",
    "\n",
    "print(f\"Lenght of token rags min:{min(sectionlengths)} max:{np.max(sectionlengths)} - median:{np.median(sectionlengths)}\")\n",
    "print(f\"Lenght of token rags mean:{np.mean(sectionlengths)} std:{np.std(sectionlengths)}\")\n",
    "\n",
    "percent     = np.asarray([ 0, 0.11, 0.2, 1, 1.5, 2, 25,50,75,90,97,99,99.5,99.9,99.99,100]) # %np.arange(101,dtype=np.int)\n",
    "percentiles = np.percentile(sectionlengths,percent).astype(np.int)\n",
    "print(\"%  percentile\")\n",
    "for i in range(len(percent)):print(f\"{percent[i]}  {percentiles[i]}\")\n",
    "minToks = percentiles[np.where( percent==0.11 )[0][0]]\n",
    "maxToks = percentiles[np.where( percent==99.99 )[0][0]]\n",
    "print(f\"\\nminToks:{minToks} maxToks:{maxToks}\")\n",
    "print(f\"rags <= minToks:{np.sum(sectionlengths <= minToks)}\")\n",
    "print(f\"rags >= maxToks:{np.sum(sectionlengths >= maxToks)}\")\n",
    "\n",
    "sectionlengths = trainIDS = validIDS = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxUnks,minToks,maxToks = 0,14,400\n",
    "\n",
    "%time trainIDS = trainTokenizer.getIds(minToks, maxToks, maxUnks, unk_idx=unk_idx)\n",
    "%time validIDS = validTokenizer.getIds(minToks, maxToks, maxUnks, unk_idx=unk_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Traning:    number of arrays:{len(trainIDS)} - number of ids:{sum(len(ids) for ids in trainIDS)}\")\n",
    "print(f\"Validation: number of arrays:{len(validIDS)} - number of ids:{sum(len(ids) for ids in validIDS)}\")\n",
    "sectionlengths = np.asarray([len(s) for s in trainIDS],dtype=np.int32)\n",
    "plt.hist(sectionlengths[sectionlengths<maxToks], 300, density=True, facecolor='g', alpha=0.75)\n",
    "np.histogram(sectionlengths[sectionlengths<1000],50)\n",
    "print(f\"Lenght of token rags min:{min(sectionlengths)} max:{np.max(sectionlengths)} - median:{np.median(sectionlengths)}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "long_sentences = trainIDS[sectionlengths<950]\n",
    "print(f\"nb long sentences:{len(long_sentences)}\")\n",
    "\n",
    "l_alpha_pr_sentence,l_alpha_pr_word, l_tech_pr_word, l_txt = [], [],[],[]\n",
    "print(f\"nb tokens  :  alpha_pr_sentence  :  alpha_pr_word  :  tech_pr_word\")\n",
    "for s in long_sentences:\n",
    "    txt = sp.DecodeIds(s.tolist())\n",
    "    l_txt.append(txt)  \n",
    "    alpha_pr_sentence, alpha_pr_word, tech_pr_word  = count_alphas(txt)\n",
    "    l_alpha_pr_sentence.append( alpha_pr_sentence )\n",
    "    l_alpha_pr_word.append( alpha_pr_word )\n",
    "    l_tech_pr_word.append( tech_pr_word )\n",
    "    #print(f\"{len(s)}  :  {alpha_pr_sentence:.2f}  :  {alpha_pr_sentence:.2f}  :  {tech_pr_word:.2f}\")\n",
    "percent     = np.asarray([ 0, 10, 25,50,75,80,85,86,87,88,88.5,89,90,95,96,97,98,99,100])\n",
    "percentiles = np.percentile(l_tech_pr_word,percent)\n",
    "print(\"%  percentile\")\n",
    "for i in range(len(percent)):print(f\"{percent[i]}  {percentiles[i]}\")\n",
    "    \n",
    "for s,a1,a2,a3,txt in zip(long_sentences,l_alpha_pr_sentence,l_alpha_pr_word, l_tech_pr_word, l_txt):\n",
    "    #if a3>.6: \n",
    "    if a3>.15 and a3<=.18 : \n",
    "        print(f\"nb tokens:{len(s)} - {a1:.2f} - {a2:.2f} - {a3:.2f} - sentence:   {txt}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the rags so that any subset is composed of similar text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from languagemodelloader import *\n",
    "\n",
    "#nTrainToks, nValidToks = int(5e6),int(2e6)\n",
    "nTrainToks, nValidToks = int(5e5),int(2e5)\n",
    "#nTrainToks, nValidToks = int(5e4),int(1e4)\n",
    "#nTrainToks, nValidToks = int(5e1),int(1e1)\n",
    "if nTrainToks>0 and nValidToks>0:\n",
    "    trainIDS_ = trainIDS[0:nTrainToks]\n",
    "    validIDS_ = validIDS[0:nValidToks]\n",
    "else:\n",
    "    trainIDS_ = trainIDS\n",
    "    validIDS_ = validIDS\n",
    "\n",
    "#dblm = TextLMDataBunch.from_ids( pathTrainValid, vocab, trainIDS_, validIDS_, bptt=70, bs=32, num_workers=0, lengths=lengths)\n",
    "%time dblm = TextLMDataBunch.from_ids( pathTrainValid, vocab, trainIDS_, validIDS_, bptt=90, bs=384, num_workers=0, no_check=True)\n",
    "nTrainToks, nValidToks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dblm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn = language_model_learner(dblm, drop_mult=1, qrnn=False, pad_token=pad_idx,  callback_fns=ShowGraph )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot(skip_start=0, skip_end=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%load_ext line_profiler\n",
    "#%time learn.fit_one_cycle(1,2e-3, moms=(0.8,0.7))\n",
    "#%time learn.fit_one_cycle(1,4e-3, moms=(0.95,0.85))\n",
    "#%time learn.fit_one_cycle(20,4e-3, moms=(0.8,0.7), wd=1e-7)\n",
    "#learn.clip_grad(clip=0.12)\n",
    "#%time learn.fit_one_cycle(1,5e-3, moms=(0.99,0.2))\n",
    "epochs=10\n",
    "#%time learn.fit_one_cycle(cyc_len=epochs, max_lr=5e-3, moms=(0.99,0.7))\n",
    "%time learn.fit_one_cycle(cyc_len=epochs, max_lr=5e-3, moms=(0.8,0.7)) #256,384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remember to update the last validation loss to get your perplexity score\n",
    "print( f\"Perplexity: {exp(3.328701)}\") #best: 3.221317\n",
    "trainTokens={sum(len(s) for s in trainIDS_)}\n",
    "print( f\"Number of tokens in trainingdata:   {sum(len(s) for s in trainIDS_)}\")\n",
    "print( f\"Number of tokens in validationdata: {sum(len(s) for s in validIDS_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'lm-32k-sentencepiece-{epochs}-{len(trainIDS_)}-{len(validIDS_)} dropout{int(round(10*drop_mult)} ')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learn.load('lmmodel-32k-sentencepiece')\n",
    "learn.clip_grad()\n",
    "%time learn.fit_one_cycle(1,slice(1e-6,1e-4), moms=(0.999,0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT = \"if i knew back then what i know now,\" #  i would not agree # seen in polval\n",
    "TEXT = \"i liked this movie because\"\n",
    "#TEXT = \"anarchy is a political philosophy that\"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES))).replace(\"▁\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.25) for _ in range(N_SENTENCES))).replace(\"▁\",\"\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Compare with the current fastai version of LanguageModelPreLoader"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gc.collect()\n",
    "dblm = TextLMDataBunch.from_ids( pathTrainValid, vocab, trainIDS_, validIDS_, bptt=70, bs=32, num_workers=0)\n",
    "nTrainToks, nValidToks"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dblm.show_batch()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%time learn = language_model_learner(dblm, drop_mult=0, qrnn=False, pad_token=pad_idx,  callback_fns=ShowGraph)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%time learn.fit_one_cycle(5,2e-3, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#i have an issue with passing pad_idx"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from fastai.utils.show_install import *\n",
    "show_install()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
