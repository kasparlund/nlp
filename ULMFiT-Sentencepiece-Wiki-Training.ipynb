{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a language model based on wikipedia in you language \n",
    "\n",
    "The notebooke includes the whole process but you will need to help the process if the following problem happens:  \n",
    "-you internet connection is interrupted (stage 1)\n",
    "\n",
    "-you run out of diskspace\n",
    "\n",
    "-because of the huge memory consumption. creatig the databunch for the training requires lots of memory\n",
    "\n",
    "\n",
    "In order ot handle these problemn the notebook is divided into stages. If the process is failes in a stage then you can resume the processing from the beginning of that stage by:\n",
    "-restarting the kernel \"Kernel\"/Restart & Clear Output\n",
    "\n",
    "-running the cells in stage 0: initialization\n",
    "\n",
    "-running the cells from the start of the stage that failed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 0: Initialisation\n",
    "lang: You must set the parameter \"lang\" to the language you want to build a model for. Fx:\n",
    "\n",
    "fr: for french\n",
    "\n",
    "en: for english\n",
    "\n",
    "de:for german\n",
    "\n",
    "da: for danish \n",
    "\n",
    "etc.\n",
    "\n",
    "pathData: You must se the location where you want your data stored using the parameter pathData. Consider using a ssd-rive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang=\"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import * \n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pathlib import *\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathData       = Path(\"../../data/nlp\")\n",
    "path           = pathData / lang\n",
    "pathDump       = path/\"wiki-dump\"\n",
    "pathJson       = path/\"wiki-json\"\n",
    "\n",
    "pathTrainValid = path/\"wiki-train_valid\"\n",
    "pathTxt        = pathTrainValid/\"txt\"\n",
    "pathToks       = pathTrainValid/\"toks\"\n",
    "pathcsv        = pathTrainValid/\"wiki.csv\"\n",
    "\n",
    "cache_name   = \"sp-model\"\n",
    "pathVocab    = pathTrainValid / cache_name"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data\n",
    "└── nlp\n",
    "     └── lang\n",
    "        ├── wiki-dump\n",
    "        ├── wiki-json\n",
    "        └── wiki-train_valid\n",
    "            ├── models         <- language model\n",
    "            ├── sp-model       <- sentencepiece model\n",
    "            ├── toks\n",
    "            ├── txt            <- preprocessed text\n",
    "            └── wiki           <- preprocessed txt i csv file\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# requried libraries: \n",
    "conda install -c anaconda psutil \n",
    "\n",
    "conda install -c anaconda git \n",
    "\n",
    "#conda install -c menpo wget \n",
    "conda install curl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: download the selected language from wikipedia and convert the articles to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if len(list(pathDump.glob(\"*.bz2\")))==0:\n",
    "    pathDump.mkdir(parents=True,exist_ok=True)\n",
    "    #fn  = f\"{lang}wiki-latest-pages-articles.xml.bz2\"\n",
    "    fn  = f\"{lang}wiki-latest-pages-articles-multistream.xml.bz2\"\n",
    "    url = f\"https://dumps.wikimedia.org/{lang}wiki/latest/{fn}\"\n",
    "    #cmd = f\"wget -c --no-check-certificate --show-progress {str(url)} -P {str(pathDump)}\"\n",
    "    cmd = f'curl -k -C - -o \"{str(pathDump/fn)}\" \"{str(url)}\"'\n",
    "    print(f\"If the command fails in the notebook then copy the command and run it in the terminal:{cmd}\")\n",
    "    ! $cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: Convert wikipedia dump to articles in json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathWikiExtractor = Path(\"../wikiextractor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pathJson.exists():\n",
    "    !git clone https://github.com/attardi/wikiextractor.git $pathWikiExtractor\n",
    "    cmd = f\"cd {str(pathWikiExtractor)} && python setup.py install\"\n",
    "    ! $cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not pathJson.exists():\n",
    "    #extracting the json-files. This takes about 1 hour for french with files read from and saved to a ssd hardrive \n",
    "    fn_wikidump = list(pathDump.iterdir())[0]\n",
    "    cmd = f\"cd {str(pathWikiExtractor)} && python WikiExtractor.py -o {str(pathJson)} --json --processes {defaults.cpus} -q --filter_disambig_pages --no-templates --discard_elements gallery,timeline,noinclude {str(fn_wikidump)}\"\n",
    "    print(f\"If WikiExtractor fails in the notebook then copy the command and run it in the terminal:{cmd}\")\n",
    "    ! $cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3: building a vocabulary using sentencepiece\n",
    "\n",
    "Now we separate the title of the wikipedia section from text section that we keep. \n",
    "\n",
    "In order to makes a first reduction on the number of section we clean the text with the preprocessing rules from fastai and ignore text with less than \"minWords\"\n",
    "\n",
    "You must set the lenght of the shortes sections to keep using the parameter \"minWords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "minWords  = 1\n",
    "chunksize = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\kl\\appdata\\local\\conda\\conda\\envs\\fastai\\lib\\site-packages (0.1.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai_sentencepiece import *\n",
    "from filetokenizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = None\n",
    "if not pathTxt.exists():\n",
    "    swm = SentencepieceWikiVocab(lang=lang, pathJson=pathJson, pathcsv=pathcsv, pathTxt=pathTxt, pathVocab=pathVocab)    \n",
    "    %time swm.wikijson2TrainingData()\n",
    "%time data = pd.read_csv(pathcsv)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha_pr_char</th>\n",
       "      <th>hyphens_pr_alpha</th>\n",
       "      <th>lettercount</th>\n",
       "      <th>letters_pr_line</th>\n",
       "      <th>letters_pr_word</th>\n",
       "      <th>linecount</th>\n",
       "      <th>parentheses_pr_alpha</th>\n",
       "      <th>sep_pr_space</th>\n",
       "      <th>wordcount</th>\n",
       "      <th>words_pr_line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>819900.000000</td>\n",
       "      <td>819900.000000</td>\n",
       "      <td>819900.000000</td>\n",
       "      <td>819900.000000</td>\n",
       "      <td>819900.000000</td>\n",
       "      <td>819900.000000</td>\n",
       "      <td>819900.000000</td>\n",
       "      <td>819900.000000</td>\n",
       "      <td>819900.000000</td>\n",
       "      <td>819900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>78.920393</td>\n",
       "      <td>0.005991</td>\n",
       "      <td>4268.174682</td>\n",
       "      <td>365.682801</td>\n",
       "      <td>6.177791</td>\n",
       "      <td>10.225719</td>\n",
       "      <td>0.368644</td>\n",
       "      <td>6.170645</td>\n",
       "      <td>686.841128</td>\n",
       "      <td>59.021513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.415266</td>\n",
       "      <td>0.031463</td>\n",
       "      <td>5443.638168</td>\n",
       "      <td>166.730761</td>\n",
       "      <td>0.349648</td>\n",
       "      <td>10.237607</td>\n",
       "      <td>0.447003</td>\n",
       "      <td>2.208520</td>\n",
       "      <td>874.192438</td>\n",
       "      <td>27.223948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>3.285714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.01%</th>\n",
       "      <td>66.242038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>4.285714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.02%</th>\n",
       "      <td>66.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>4.416330</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.03%</th>\n",
       "      <td>66.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.04%</th>\n",
       "      <td>66.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>4.555556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.05%</th>\n",
       "      <td>66.999510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>4.625000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.06%</th>\n",
       "      <td>67.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>4.627731</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.07%</th>\n",
       "      <td>67.525773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.08%</th>\n",
       "      <td>67.716535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>4.695382</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.09%</th>\n",
       "      <td>67.906455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>4.714286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1%</th>\n",
       "      <td>70.813395</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>5.316452</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2%</th>\n",
       "      <td>72.076955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>5.492537</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3%</th>\n",
       "      <td>73.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>5.576232</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.492537</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>14.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4%</th>\n",
       "      <td>73.783355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>5.631793</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.127660</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>17.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10%</th>\n",
       "      <td>75.977654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>385.000000</td>\n",
       "      <td>175.750000</td>\n",
       "      <td>5.787755</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.626943</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20%</th>\n",
       "      <td>77.361702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>777.000000</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>5.916667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.047608</td>\n",
       "      <td>4.639175</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>37.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>79.254737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2347.000000</td>\n",
       "      <td>346.813657</td>\n",
       "      <td>6.158491</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.263366</td>\n",
       "      <td>6.208945</td>\n",
       "      <td>379.000000</td>\n",
       "      <td>56.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>80.451217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5279.000000</td>\n",
       "      <td>454.959135</td>\n",
       "      <td>6.371165</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.487260</td>\n",
       "      <td>7.407407</td>\n",
       "      <td>851.000000</td>\n",
       "      <td>73.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>81.525091</td>\n",
       "      <td>0.010157</td>\n",
       "      <td>10473.000000</td>\n",
       "      <td>574.000000</td>\n",
       "      <td>6.601695</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>8.653846</td>\n",
       "      <td>1684.000000</td>\n",
       "      <td>92.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>82.205825</td>\n",
       "      <td>0.036576</td>\n",
       "      <td>15307.050000</td>\n",
       "      <td>661.250000</td>\n",
       "      <td>6.765957</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1.108033</td>\n",
       "      <td>9.615385</td>\n",
       "      <td>2454.000000</td>\n",
       "      <td>107.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96%</th>\n",
       "      <td>82.400000</td>\n",
       "      <td>0.045524</td>\n",
       "      <td>16961.000000</td>\n",
       "      <td>689.633455</td>\n",
       "      <td>6.818182</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.221374</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2715.000000</td>\n",
       "      <td>112.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97%</th>\n",
       "      <td>82.634731</td>\n",
       "      <td>0.057870</td>\n",
       "      <td>19064.000000</td>\n",
       "      <td>726.444532</td>\n",
       "      <td>6.886076</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1.376951</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>3055.000000</td>\n",
       "      <td>118.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98%</th>\n",
       "      <td>82.962963</td>\n",
       "      <td>0.076982</td>\n",
       "      <td>21971.000000</td>\n",
       "      <td>779.000000</td>\n",
       "      <td>6.979799</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>1.627490</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>3520.000000</td>\n",
       "      <td>126.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99%</th>\n",
       "      <td>83.486239</td>\n",
       "      <td>0.117463</td>\n",
       "      <td>26726.000000</td>\n",
       "      <td>872.067111</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>2.101862</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>4281.000000</td>\n",
       "      <td>142.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.1%</th>\n",
       "      <td>83.561644</td>\n",
       "      <td>0.124301</td>\n",
       "      <td>27401.818000</td>\n",
       "      <td>886.229595</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>2.173913</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>4390.909000</td>\n",
       "      <td>144.315294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.2%</th>\n",
       "      <td>83.652548</td>\n",
       "      <td>0.132538</td>\n",
       "      <td>28195.808000</td>\n",
       "      <td>902.984000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>4517.808000</td>\n",
       "      <td>147.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.3%</th>\n",
       "      <td>83.761305</td>\n",
       "      <td>0.142847</td>\n",
       "      <td>29058.828000</td>\n",
       "      <td>920.830763</td>\n",
       "      <td>7.233840</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>2.380952</td>\n",
       "      <td>12.903226</td>\n",
       "      <td>4665.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.4%</th>\n",
       "      <td>83.870968</td>\n",
       "      <td>0.154919</td>\n",
       "      <td>30069.818000</td>\n",
       "      <td>941.230300</td>\n",
       "      <td>7.275862</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>2.515723</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>4831.606000</td>\n",
       "      <td>153.595365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.5%</th>\n",
       "      <td>84.002210</td>\n",
       "      <td>0.170001</td>\n",
       "      <td>31250.010000</td>\n",
       "      <td>966.317708</td>\n",
       "      <td>7.315144</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>2.702703</td>\n",
       "      <td>13.636364</td>\n",
       "      <td>5021.505000</td>\n",
       "      <td>158.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.6%</th>\n",
       "      <td>84.210526</td>\n",
       "      <td>0.189573</td>\n",
       "      <td>32661.808000</td>\n",
       "      <td>997.000000</td>\n",
       "      <td>7.375000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>2.941176</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>5260.000000</td>\n",
       "      <td>163.239844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.7%</th>\n",
       "      <td>84.415584</td>\n",
       "      <td>0.220264</td>\n",
       "      <td>34516.212000</td>\n",
       "      <td>1040.827288</td>\n",
       "      <td>7.448276</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>3.329039</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>170.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.8%</th>\n",
       "      <td>84.745763</td>\n",
       "      <td>0.268097</td>\n",
       "      <td>37113.818000</td>\n",
       "      <td>1102.086571</td>\n",
       "      <td>7.555556</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>3.960396</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>5966.202000</td>\n",
       "      <td>180.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.9%</th>\n",
       "      <td>85.333333</td>\n",
       "      <td>0.383142</td>\n",
       "      <td>41373.222000</td>\n",
       "      <td>1224.768517</td>\n",
       "      <td>7.769231</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>15.277778</td>\n",
       "      <td>6670.101000</td>\n",
       "      <td>200.254591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.99%</th>\n",
       "      <td>87.058824</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>56308.545200</td>\n",
       "      <td>1779.000000</td>\n",
       "      <td>8.545500</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>6.060606</td>\n",
       "      <td>15.909091</td>\n",
       "      <td>9220.020200</td>\n",
       "      <td>291.311374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>92.708333</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>122488.000000</td>\n",
       "      <td>2812.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>19864.000000</td>\n",
       "      <td>464.137931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        alpha_pr_char  hyphens_pr_alpha    lettercount  letters_pr_line  \\\n",
       "count   819900.000000     819900.000000  819900.000000    819900.000000   \n",
       "mean        78.920393          0.005991    4268.174682       365.682801   \n",
       "std          2.415266          0.031463    5443.638168       166.730761   \n",
       "min         66.000000          0.000000      23.000000        23.000000   \n",
       "0.01%       66.242038          0.000000      33.000000        33.000000   \n",
       "0.02%       66.666667          0.000000      33.000000        33.000000   \n",
       "0.03%       66.666667          0.000000      34.000000        34.000000   \n",
       "0.04%       66.666667          0.000000      36.000000        36.000000   \n",
       "0.05%       66.999510          0.000000      36.000000        36.000000   \n",
       "0.06%       67.272727          0.000000      36.000000        36.000000   \n",
       "0.07%       67.525773          0.000000      36.000000        36.000000   \n",
       "0.08%       67.716535          0.000000      37.000000        37.000000   \n",
       "0.09%       67.906455          0.000000      37.000000        37.000000   \n",
       "1%          70.813395          0.000000      64.000000        62.000000   \n",
       "2%          72.076955          0.000000      85.000000        77.000000   \n",
       "3%          73.043478          0.000000     117.000000        94.000000   \n",
       "4%          73.783355          0.000000     153.000000       111.000000   \n",
       "10%         75.977654          0.000000     385.000000       175.750000   \n",
       "20%         77.361702          0.000000     777.000000       233.000000   \n",
       "50%         79.254737          0.000000    2347.000000       346.813657   \n",
       "75%         80.451217          0.000000    5279.000000       454.959135   \n",
       "90%         81.525091          0.010157   10473.000000       574.000000   \n",
       "95%         82.205825          0.036576   15307.050000       661.250000   \n",
       "96%         82.400000          0.045524   16961.000000       689.633455   \n",
       "97%         82.634731          0.057870   19064.000000       726.444532   \n",
       "98%         82.962963          0.076982   21971.000000       779.000000   \n",
       "99%         83.486239          0.117463   26726.000000       872.067111   \n",
       "99.1%       83.561644          0.124301   27401.818000       886.229595   \n",
       "99.2%       83.652548          0.132538   28195.808000       902.984000   \n",
       "99.3%       83.761305          0.142847   29058.828000       920.830763   \n",
       "99.4%       83.870968          0.154919   30069.818000       941.230300   \n",
       "99.5%       84.002210          0.170001   31250.010000       966.317708   \n",
       "99.6%       84.210526          0.189573   32661.808000       997.000000   \n",
       "99.7%       84.415584          0.220264   34516.212000      1040.827288   \n",
       "99.8%       84.745763          0.268097   37113.818000      1102.086571   \n",
       "99.9%       85.333333          0.383142   41373.222000      1224.768517   \n",
       "99.99%      87.058824          1.000000   56308.545200      1779.000000   \n",
       "max         92.708333          2.857143  122488.000000      2812.000000   \n",
       "\n",
       "        letters_pr_word      linecount  parentheses_pr_alpha   sep_pr_space  \\\n",
       "count     819900.000000  819900.000000         819900.000000  819900.000000   \n",
       "mean           6.177791      10.225719              0.368644       6.170645   \n",
       "std            0.349648      10.237607              0.447003       2.208520   \n",
       "min            3.285714       1.000000              0.000000       0.000000   \n",
       "0.01%          4.285714       1.000000              0.000000       0.000000   \n",
       "0.02%          4.416330       1.000000              0.000000       0.000000   \n",
       "0.03%          4.500000       1.000000              0.000000       0.000000   \n",
       "0.04%          4.555556       1.000000              0.000000       0.000000   \n",
       "0.05%          4.625000       1.000000              0.000000       0.000000   \n",
       "0.06%          4.627731       1.000000              0.000000       0.000000   \n",
       "0.07%          4.666667       1.000000              0.000000       0.000000   \n",
       "0.08%          4.695382       1.000000              0.000000       0.000000   \n",
       "0.09%          4.714286       1.000000              0.000000       0.000000   \n",
       "1%             5.316452       1.000000              0.000000       0.000000   \n",
       "2%             5.492537       1.000000              0.000000       0.000000   \n",
       "3%             5.576232       1.000000              0.000000       1.492537   \n",
       "4%             5.631793       1.000000              0.000000       2.127660   \n",
       "10%            5.787755       2.000000              0.000000       3.626943   \n",
       "20%            5.916667       3.000000              0.047608       4.639175   \n",
       "50%            6.158491       7.000000              0.263366       6.208945   \n",
       "75%            6.371165      13.000000              0.487260       7.407407   \n",
       "90%            6.601695      23.000000              0.806452       8.653846   \n",
       "95%            6.765957      32.000000              1.108033       9.615385   \n",
       "96%            6.818182      35.000000              1.221374      10.000000   \n",
       "97%            6.886076      39.000000              1.376951      10.400000   \n",
       "98%            6.979799      44.000000              1.627490      11.111111   \n",
       "99%            7.142857      50.000000              2.101862      12.500000   \n",
       "99.1%          7.166667      51.000000              2.173913      12.500000   \n",
       "99.2%          7.200000      52.000000              2.272727      12.500000   \n",
       "99.3%          7.233840      53.000000              2.380952      12.903226   \n",
       "99.4%          7.275862      54.000000              2.515723      13.333333   \n",
       "99.5%          7.315144      55.000000              2.702703      13.636364   \n",
       "99.6%          7.375000      56.000000              2.941176      14.285714   \n",
       "99.7%          7.448276      57.000000              3.329039      14.285714   \n",
       "99.8%          7.555556      58.000000              3.960396      14.285714   \n",
       "99.9%          7.769231      59.000000              4.444444      15.277778   \n",
       "99.99%         8.545500      60.000000              6.060606      15.909091   \n",
       "max           12.000000      60.000000             13.333333      16.000000   \n",
       "\n",
       "            wordcount  words_pr_line  \n",
       "count   819900.000000  819900.000000  \n",
       "mean       686.841128      59.021513  \n",
       "std        874.192438      27.223948  \n",
       "min          6.000000       6.000000  \n",
       "0.01%        6.000000       6.000000  \n",
       "0.02%        6.000000       6.000000  \n",
       "0.03%        6.000000       6.000000  \n",
       "0.04%        6.000000       6.000000  \n",
       "0.05%        6.000000       6.000000  \n",
       "0.06%        6.000000       6.000000  \n",
       "0.07%        6.000000       6.000000  \n",
       "0.08%        6.000000       6.000000  \n",
       "0.09%        6.000000       6.000000  \n",
       "1%          10.000000       9.000000  \n",
       "2%          13.000000      12.000000  \n",
       "3%          19.000000      14.500000  \n",
       "4%          24.000000      17.285714  \n",
       "10%         62.000000      28.000000  \n",
       "20%        125.000000      37.500000  \n",
       "50%        379.000000      56.000000  \n",
       "75%        851.000000      73.500000  \n",
       "90%       1684.000000      92.857143  \n",
       "95%       2454.000000     107.250000  \n",
       "96%       2715.000000     112.000000  \n",
       "97%       3055.000000     118.000000  \n",
       "98%       3520.000000     126.666667  \n",
       "99%       4281.000000     142.000000  \n",
       "99.1%     4390.909000     144.315294  \n",
       "99.2%     4517.808000     147.000000  \n",
       "99.3%     4665.000000     150.000000  \n",
       "99.4%     4831.606000     153.595365  \n",
       "99.5%     5021.505000     158.000000  \n",
       "99.6%     5260.000000     163.239844  \n",
       "99.7%     5558.000000     170.000000  \n",
       "99.8%     5966.202000     180.400000  \n",
       "99.9%     6670.101000     200.254591  \n",
       "99.99%    9220.020200     291.311374  \n",
       "max      19864.000000     464.137931  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.57 s\n",
      "data:(819900, 11)\n"
     ]
    }
   ],
   "source": [
    "if data is not None:\n",
    "    percentiles = 0.01 * np.asarray([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, \n",
    "                                     1,2,3,4,10,20, 50, 75, 90, 95, 96, 97, 98, \n",
    "                                     99, 99.1, 99.2, 99.3, 99.4, 99.5, 99.6, 99.7, 99.8, 99.9,99.99]) \n",
    "    %time display(data.describe(percentiles,include=[np.number]))\n",
    "    print(f\"data:{data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_empty:(3, 11)\n",
      "Wall time: 23 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha_pr_char</th>\n",
       "      <th>hyphens_pr_alpha</th>\n",
       "      <th>lettercount</th>\n",
       "      <th>letters_pr_line</th>\n",
       "      <th>letters_pr_word</th>\n",
       "      <th>linecount</th>\n",
       "      <th>parentheses_pr_alpha</th>\n",
       "      <th>sep_pr_space</th>\n",
       "      <th>text</th>\n",
       "      <th>wordcount</th>\n",
       "      <th>words_pr_line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>144149</th>\n",
       "      <td>79.545455</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>44</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>list of — in the northwestern united states.</td>\n",
       "      <td>7</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449086</th>\n",
       "      <td>76.859504</td>\n",
       "      <td>2.150538</td>\n",
       "      <td>484</td>\n",
       "      <td>161.333333</td>\n",
       "      <td>6.722222</td>\n",
       "      <td>3</td>\n",
       "      <td>0.537634</td>\n",
       "      <td>15.492958</td>\n",
       "      <td>humber—port au port—st. barbe (formerly known ...</td>\n",
       "      <td>71</td>\n",
       "      <td>23.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647645</th>\n",
       "      <td>71.641791</td>\n",
       "      <td>2.083333</td>\n",
       "      <td>67</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>8.375000</td>\n",
       "      <td>1</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>dimiter orahovats (1892—1992) was a prominent ...</td>\n",
       "      <td>7</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        alpha_pr_char  hyphens_pr_alpha  lettercount  letters_pr_line  \\\n",
       "144149      79.545455          2.857143           44        44.000000   \n",
       "449086      76.859504          2.150538          484       161.333333   \n",
       "647645      71.641791          2.083333           67        67.000000   \n",
       "\n",
       "        letters_pr_word  linecount  parentheses_pr_alpha  sep_pr_space  \\\n",
       "144149         5.500000          1              0.000000     14.285714   \n",
       "449086         6.722222          3              0.537634     15.492958   \n",
       "647645         8.375000          1              4.166667     14.285714   \n",
       "\n",
       "                                                     text  wordcount  \\\n",
       "144149       list of — in the northwestern united states.          7   \n",
       "449086  humber—port au port—st. barbe (formerly known ...         71   \n",
       "647645  dimiter orahovats (1892—1992) was a prominent ...          7   \n",
       "\n",
       "        words_pr_line  \n",
       "144149       7.000000  \n",
       "449086      23.666667  \n",
       "647645       7.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['list of — in the northwestern united states.'\n",
      " 'humber—port au port—st. barbe (formerly known as humber—st. barbe) was a federal electoral district in newfoundland and labrador, canada, that was represented in the house of commons of canada from 1979 to 1988.\\r\\nthis riding was created in the 1976 redistribution as \"humber—st. barbe\" from parts of humber—st. george\\'s—st. barbe riding. the name of the electoral district was changed to \"humber—port au port—st. barbe\" in 1978.\\r\\nthis riding elected the following members of parliament:'\n",
      " 'dimiter orahovats (1892—1992) was a prominent lesbian physiologist.']\n",
      "['list of — in the northwestern united states.'\n",
      " 'humber—port au port—st. barbe (formerly known as humber—st. barbe) was a federal electoral district in newfoundland and labrador, canada, that was represented in the house of commons of canada from 1979 to 1988.\\r\\nthis riding was created in the 1976 redistribution as \"humber—st. barbe\" from parts of humber—st. george\\'s—st. barbe riding. the name of the electoral district was changed to \"humber—port au port—st. barbe\" in 1978.\\r\\nthis riding elected the following members of parliament:'\n",
      " 'dimiter orahovats (1892—1992) was a prominent lesbian physiologist.']\n"
     ]
    }
   ],
   "source": [
    "#analyse thresholds used to clean wikipedia\n",
    "if data is not None:\n",
    "    lpl = data.hyphens_pr_alpha.values\n",
    "    ix = lpl>2\n",
    "    data_empty = data[ix]\n",
    "    print(f\"data_empty:{data_empty.shape}\")\n",
    "    \n",
    "    percentiles = 0.01 * np.asarray([1, 10, 25, 50, 75, 90, 99])     \n",
    "    %time data_empty.describe(percentiles,include=[np.number])\n",
    "    display(data_empty)\n",
    "    print(data_empty.text.head().values)\n",
    "    print(data_empty.text.tail().values)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not (pathVocab/\"m.model\").exists():\n",
    "    swm = SentencepieceWikiVocab(lang=lang, pathJson=pathJson, pathcsv=pathcsv, pathTxt=pathTxt, pathVocab=pathVocab)\n",
    "    %time swm.trainVocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (pathVocab/\"itos.pkl\").exists():\n",
    "    swm = SentencepieceWikiVocab(lang=lang, pathJson=pathJson, pathcsv=pathcsv, pathTxt=pathTxt, pathVocab=pathVocab)\n",
    "    swm.convertSPVocab2FastaiVocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show some examples using the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "swm = SentencepieceWikiVocab(lang=lang, pathJson=pathJson, pathcsv=pathcsv, pathTxt=pathTxt, pathVocab=pathVocab)\n",
    "sp  = spm.SentencePieceProcessor()\n",
    "sp.Load(str(pathVocab / \"m.model\"))\n",
    "print(\"\\nSize of vocabulary:\",sp.GetPieceSize())\n",
    "\n",
    "control_symbols = [\"<unk>\",\"xxbos\",\"xxeos\",\"xxpad\"]\n",
    "print(f\"Control symbol:\")\n",
    "for s in control_symbols: print(f\"{s}({sp.PieceToId(s)})\")\n",
    "\n",
    "print(f\"\\nUser defined symbols:\")\n",
    "for s in swm.getUserdefinedSymbols():print(f\"{s}({sp.PieceToId(s)})\")\n",
    "\n",
    "sentence = \"She is tall. He is small\".lower()\n",
    "print(f\"Sentence:          {sentence}\")\n",
    "print(f\"Sentence as pieces:{sp.EncodeAsPieces(sentence)}\")\n",
    "print(f\"Sentence as ids:   {sp.EncodeAsIds(sentence)}\")\n",
    "print(f\"Sentence from ids:   {sp.DecodeIds(sp.EncodeAsIds(sentence))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training of the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM Training\n",
    "Set the minimum number of tokens for the sections that we shall retain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spt_func  = partial(SentencepieceTokenizer.create, pathVocab=pathVocab)\n",
    "spt_func.__name__ = SentencepieceTokenizer.__name__\n",
    "spt       = spt_func(lang=\"en\")\n",
    "\n",
    "unk_idx,pad_idx = spt.vocab().numericalize([text.transform.UNK])[0], spt.vocab().numericalize([text.transform.PAD])[0]\n",
    "print(f\"unk_idx:{unk_idx} pad_idx:{pad_idx}\")\n",
    "vocab,max_vocab = spt.vocab(), len(spt.vocab().itos)\n",
    "trainTokenizer  = SPFileTokenizer(pathToks/\"train\", spt_func,\"en\",vocab,n_cpus=max(defaults.cpus-1,1))\n",
    "validTokenizer  = SPFileTokenizer(pathToks/\"valid\", spt_func,\"en\",vocab,n_cpus=max(defaults.cpus-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.2\n",
    "if not (pathToks/\"train\").exists() or not (pathToks/\"valid\").exists():\n",
    "    files = list(pathTxt.glob(\"*.txt\"))\n",
    "    random.seed(1)\n",
    "    random.shuffle(files)\n",
    "    random.seed()\n",
    "    splitindex = int(len(files)*split_ratio+.5)\n",
    "    trainList  = TextList( files[:-splitindex], vocab=vocab, pad_idx=pad_idx, \n",
    "                           processor=[SPFileTokenizeProcessor(tokenizer=trainTokenizer, chunksize=0)])\n",
    "    validList  = TextList( files[-splitindex:], vocab=vocab, pad_idx=pad_idx, \n",
    "                           processor=[SPFileTokenizeProcessor(tokenizer=validTokenizer, chunksize=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (pathToks/\"train\").exists():\n",
    "    %time trainList.process()\n",
    "    trainList=None\n",
    "    gc.collect()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (pathToks/\"valid\").exists():\n",
    "    %time p = validList.process()\n",
    "    validList=None\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following cells are used to find macUnk, minToks and maxToks.\n",
    "The 4 cells were deactivate after deciding on these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_statistics=False\n",
    "if check_statistics:\n",
    "    %time trainIDS = trainTokenizer.getIds()\n",
    "    %time validIDS = validTokenizer.getIds()\n",
    "else:\n",
    "    trainIDS = validIDS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#how much do we loos by removing sentences with unks\n",
    "if trainIDS is not None:\n",
    "    unkPrSentence        = np.asarray([np.sum(s==unk_idx) for s in trainIDS],dtype=np.int)\n",
    "    percentUnkPrSentence = np.asarray([np.sum(s==unk_idx)/len(s) for s in trainIDS])\n",
    "    percent              = np.asarray([0, 50, 75, 90, 97, 98.5, 99, 99.1, 99.2, 99.3, 99.4, 99.5, 99.6, 99.7, 99.8, 99.9, 99.95,  99.99, 100]) \n",
    "    percentiles          = np.percentile(unkPrSentence,percent).astype(np.int)\n",
    "    print(\"unkPrSentence %  percentile \")\n",
    "    for i in range(len(percent)):print(f\"{percent[i]}  {percentiles[i]}\")   \n",
    "\n",
    "    unkPrSentence=None\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if trainIDS is not None:\n",
    "    print(f\"Traning:    number of arrays:{len(trainIDS)} - number of ids:{sum(len(ids) for ids in trainIDS)}\")\n",
    "    print(f\"Validation: number of arrays:{len(validIDS)} - number of ids:{sum(len(ids) for ids in validIDS)}\")\n",
    "\n",
    "    #Analyse the distribution of the legnth of tokens sequences in the ragged/jagged array of tokes\n",
    "    sectionlengths = np.asarray([len(s) for s in trainIDS],dtype=np.int32)\n",
    "    plt.hist(sectionlengths[sectionlengths<500], 250, density=True, facecolor='g', alpha=0.75)\n",
    "    np.histogram(sectionlengths[sectionlengths<1000],50)\n",
    "\n",
    "    print(f\"Lenght of token rags min:{min(sectionlengths)} max:{np.max(sectionlengths)} - median:{np.median(sectionlengths)}\")\n",
    "    print(f\"Lenght of token rags mean:{np.mean(sectionlengths)} std:{np.std(sectionlengths)}\")\n",
    "\n",
    "    percent     = np.asarray([ 0, 0.11, 0.2,0.3,0.4,0.5,0.6,0.7, 0.8, 0.9, 1, 1.5, 2, 25,50,75,90,97,99,99.5,99.9,99.99,100]) # %np.arange(101,dtype=np.int)\n",
    "    percentiles = np.percentile(sectionlengths,percent).astype(np.int)\n",
    "    print(\"%  percentile\")\n",
    "    for i in range(len(percent)):print(f\"{percent[i]}  {percentiles[i]}\")\n",
    "    minToks = percentiles[np.where( percent==0.11 )[0][0]]\n",
    "    maxToks = percentiles[np.where( percent==99.99 )[0][0]]\n",
    "    print(f\"\\nminToks:{minToks} maxToks:{maxToks}\")\n",
    "    print(f\"rags <= minToks:{np.sum(sectionlengths <= minToks)}\")\n",
    "    print(f\"rags >= maxToks:{np.sum(sectionlengths >= maxToks)}\")\n",
    "\n",
    "    sectionlengths = trainIDS = validIDS = None\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxUnks,minToks,maxToks = 0,14,500\n",
    "\n",
    "train_count = valid_count = -1\n",
    "#train_count = 2.5e9\n",
    "#valid_count = int(split_ratio * train_count +.5)\n",
    "\n",
    "%time trainIDS = trainTokenizer.getIds(minToks, maxToks, maxUnks, unk_idx=unk_idx, count=train_count)\n",
    "%time validIDS = validTokenizer.getIds(minToks, maxToks, maxUnks, unk_idx=unk_idx, count=valid_count)\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(trainIDS)\n",
    "np.random.shuffle(validIDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_lengths = np.fromiter( (len(s) for s in trainIDS), dtype=np.int16, count=len(trainIDS) )\n",
    "print(f\"Training:   number of arrays:{len(trainIDS)} - number of ids:{sum(len(ids) for ids in trainIDS)}\")\n",
    "print(f\"Lenght of training sentences:   min:{np.min(train_lengths)} max:{np.max(train_lengths)} - median:{np.median(train_lengths)}\")\n",
    "plt.hist(train_lengths[train_lengths<maxToks], 100, density=True, facecolor='g', alpha=0.75, label=\"sentencelength for training data\" )\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_lengths = np.fromiter( (len(s) for s in validIDS), dtype=np.int16, count=len(validIDS) )\n",
    "print(f\"Validation: number of arrays:{len(validIDS)} - number of ids:{sum(len(ids) for ids in validIDS)}\")\n",
    "print(f\"Lenght of validation sentences: min:{np.min(valid_lengths)} max:{np.max(valid_lengths)} - median:{np.median(valid_lengths)}\")\n",
    "plt.hist(valid_lengths[valid_lengths<maxToks], 100, density=True, facecolor='g', alpha=0.75, label=\"sentencelength for validation data\" )\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNSentencens(sentencelengths, nTokens:int): \n",
    "    i,ln_lenghts=0,len(sentencelengths)\n",
    "    while nTokens>0 and i<ln_lenghts: \n",
    "        nTokens -= sentencelengths[i]\n",
    "        i       += 1\n",
    "    return i    \n",
    "#nTokens=int(2.5e9)\n",
    "nTokens=int(1e8)\n",
    "print(f\"ntokens:{nTokens} i:{getNSentencens(train_lengths, nTokens)}\")    \n",
    "#np.sum(train_lengths[:getNSentencens(train_lengths, nTokens)],dtype=np.int64)\n",
    "#24922316 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from languagemodelloader import *\n",
    "nTrainToks = int(1e8)\n",
    "nValidToks = int(0.2*nTrainToks+1)\n",
    "nTrainSentences, nValidSentences = getNSentencens(train_lengths,nTrainToks), getNSentencens(valid_lengths,nValidToks)\n",
    "selectedTrainToks = np.sum(train_lengths[:nTrainSentences],dtype=np.int64)\n",
    "selectedValidToks = np.sum(valid_lengths[:nValidSentences],dtype=np.int64)\n",
    "\n",
    "print(f\"specified training tokens    :{nTrainToks}\\nselected training tokens     :{selectedTrainToks}\\nselected training sentences  :{nTrainSentences}\" )\n",
    "print(f\"\\n\\nspecified validation tokens  :{nValidToks}\\nselected validation tokens   :{selectedValidToks}\\nselected validation sentences:{nValidSentences}\" )\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time dblm = TextLMDataBunch.from_ids( pathTrainValid, vocab, trainIDS_, validIDS_, bs=64, bptt=94, num_workers=0, no_check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dblm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dropout = {'language': np.array([0.25, 0.1, 0.2, 0.02, 0.15]),\n",
    "                   'classifier': np.array([0.4,0.5,0.05,0.3,0.4])}\n",
    "dps      = default_dropout[\"language\"]\n",
    "input_p  = dps[0] \n",
    "output_p = dps[1] \n",
    "weight_p = dps[2] \n",
    "embed_p  = dps[3] \n",
    "hidden_p = dps[4] \n",
    "out_bias = True\n",
    "\n",
    "cfg = awd_lstm_lm_config.copy()\n",
    "cfg['qrnn'],cfg['emb_sz'],cfg['n_hid'],cfg['n_layers'],cfg['tie_weights'], cfg['pad_token'] =\\\n",
    "     False,      256,          1280,            3,           True,               pad_idx \n",
    "#     False,      400,         1150,              3,         True,               pad_idx \n",
    "cfg['input_p'],cfg['weight_p'],cfg['embed_p'],cfg['hidden_p'],cfg['output_p'], cfg['out_bias']=\\\n",
    "     input_p,       weight_p,       embed_p,       hidden_p,       output_p,   out_bias\n",
    "drop_mult=.2\n",
    "%time learn = language_model_learner(dblm, AWD_LSTM,config=cfg, drop_mult=drop_mult, pretrained=False, callback_fns=ShowGraph )\n",
    "print(cfg)\n",
    "#callback_fns=PeakMemMetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "%time learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot(skip_start=0, skip_end=11)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from fastai.text import * \n",
    "epochs=1\n",
    "#%time learn.fit_one_cycle(cyc_len=epochs, max_lr=5e-3, moms=(0.99,0.7))\n",
    "%time learn.fit_one_cycle(cyc_len=epochs, max_lr=2e-3, moms=(0.8,0.7))\n",
    "# callback_fns=PeakMemMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remember to update the last validation loss to get your perplexity score\n",
    "print( f\"Perplexity: {exp(3.326004)}\") #best: 3.221317\n",
    "trainTokens={sum(len(s) for s in trainIDS_)}\n",
    "print( f\"Number of tokens in trainingdata:   {sum(len(s) for s in trainIDS_)}\")\n",
    "print( f\"Number of tokens in validationdata: {sum(len(s) for s in validIDS_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'lm-32k-sentencepiece-{epochs}-{len(trainIDS_)}-{len(validIDS_)} dropout{int(round(100*drop_mult))} ')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learn.load('lmmodel-32k-sentencepiece')\n",
    "learn.clip_grad()\n",
    "%time learn.fit_one_cycle(1,slice(1e-6,1e-4), moms=(0.999,0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = learn.recorder.losses\n",
    "len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT = \"if i knew back then what i know now,\" #  i would not agree # seen in polval\n",
    "TEXT = \"i liked this movie because \"\n",
    "#TEXT = \"anarchy is a political philosophy that\"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75, sep=\"\") for _ in range(N_SENTENCES))).replace(\"▁\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.25, sep=\"\") for _ in range(N_SENTENCES))).replace(\"▁\",\"\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Compare with the current fastai version of LanguageModelPreLoader"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gc.collect()\n",
    "dblm = TextLMDataBunch.from_ids( pathTrainValid, vocab, trainIDS_, validIDS_, bptt=70, bs=32, num_workers=0)\n",
    "nTrainToks, nValidToks"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dblm.show_batch()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%time learn = language_model_learner(dblm, drop_mult=0, qrnn=False, pad_token=pad_idx,  callback_fns=ShowGraph)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%time learn.fit_one_cycle(5,2e-3, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#i have an issue with passing pad_idx"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from fastai.utils.show_install import *\n",
    "show_install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_losses = pd.DataFrame(data=losses)\n",
    "pd_losses.to_csv(pathTrainValid/\"losses_steps.csv\", header=\"losses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
